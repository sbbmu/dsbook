<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 33 Large datasets | Introduction to Data Science</title>
  <meta name="description" content="This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 33 Large datasets | Introduction to Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 33 Large datasets | Introduction to Data Science" />
  
  <meta name="twitter:description" content="This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  

<meta name="author" content="Rafael A. Irizarry" />


<meta name="date" content="2019-11-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="machine-learning-in-practice.html">
<link rel="next" href="clustering.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Data Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#case-studies"><i class="fa fa-check"></i>Case studies</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#who-will-find-this-book-useful"><i class="fa fa-check"></i>Who will find this book useful?</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#what-does-this-book-cover"><i class="fa fa-check"></i>What does this book cover?</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#what-is-not-covered-by-this-book"><i class="fa fa-check"></i>What is not covered by this book?</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>1</b> Getting started with R and RStudio</a><ul>
<li class="chapter" data-level="1.1" data-path="getting-started.html"><a href="getting-started.html#why-r"><i class="fa fa-check"></i><b>1.1</b> Why R?</a></li>
<li class="chapter" data-level="1.2" data-path="getting-started.html"><a href="getting-started.html#the-r-console"><i class="fa fa-check"></i><b>1.2</b> The R console</a></li>
<li class="chapter" data-level="1.3" data-path="getting-started.html"><a href="getting-started.html#scripts"><i class="fa fa-check"></i><b>1.3</b> Scripts</a></li>
<li class="chapter" data-level="1.4" data-path="getting-started.html"><a href="getting-started.html#rstudio"><i class="fa fa-check"></i><b>1.4</b> RStudio</a><ul>
<li class="chapter" data-level="1.4.1" data-path="getting-started.html"><a href="getting-started.html#the-panes"><i class="fa fa-check"></i><b>1.4.1</b> The panes</a></li>
<li class="chapter" data-level="1.4.2" data-path="getting-started.html"><a href="getting-started.html#key-bindings"><i class="fa fa-check"></i><b>1.4.2</b> Key bindings</a></li>
<li class="chapter" data-level="1.4.3" data-path="getting-started.html"><a href="getting-started.html#running-commands-while-editing-scripts"><i class="fa fa-check"></i><b>1.4.3</b> Running commands while editing scripts</a></li>
<li class="chapter" data-level="1.4.4" data-path="getting-started.html"><a href="getting-started.html#changing-global-options"><i class="fa fa-check"></i><b>1.4.4</b> Changing global options</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="getting-started.html"><a href="getting-started.html#installing-r-packages"><i class="fa fa-check"></i><b>1.5</b> Installing R packages</a></li>
</ul></li>
<li class="part"><span><b>I R</b></span></li>
<li class="chapter" data-level="2" data-path="r-basics.html"><a href="r-basics.html"><i class="fa fa-check"></i><b>2</b> R basics</a><ul>
<li class="chapter" data-level="2.1" data-path="r-basics.html"><a href="r-basics.html#case-study-us-gun-murders"><i class="fa fa-check"></i><b>2.1</b> Case study: US Gun Murders</a></li>
<li class="chapter" data-level="2.2" data-path="r-basics.html"><a href="r-basics.html#the-very-basics"><i class="fa fa-check"></i><b>2.2</b> The very basics</a><ul>
<li class="chapter" data-level="2.2.1" data-path="r-basics.html"><a href="r-basics.html#objects"><i class="fa fa-check"></i><b>2.2.1</b> Objects</a></li>
<li class="chapter" data-level="2.2.2" data-path="r-basics.html"><a href="r-basics.html#the-workspace"><i class="fa fa-check"></i><b>2.2.2</b> The workspace</a></li>
<li class="chapter" data-level="2.2.3" data-path="r-basics.html"><a href="r-basics.html#functions"><i class="fa fa-check"></i><b>2.2.3</b> Functions</a></li>
<li class="chapter" data-level="2.2.4" data-path="r-basics.html"><a href="r-basics.html#other-prebuilt-objects"><i class="fa fa-check"></i><b>2.2.4</b> Other prebuilt objects</a></li>
<li class="chapter" data-level="2.2.5" data-path="r-basics.html"><a href="r-basics.html#variable-names"><i class="fa fa-check"></i><b>2.2.5</b> Variable names</a></li>
<li class="chapter" data-level="2.2.6" data-path="r-basics.html"><a href="r-basics.html#saving-your-workspace"><i class="fa fa-check"></i><b>2.2.6</b> Saving your workspace</a></li>
<li class="chapter" data-level="2.2.7" data-path="r-basics.html"><a href="r-basics.html#motivating-scripts"><i class="fa fa-check"></i><b>2.2.7</b> Motivating scripts</a></li>
<li class="chapter" data-level="2.2.8" data-path="r-basics.html"><a href="r-basics.html#commenting-your-code"><i class="fa fa-check"></i><b>2.2.8</b> Commenting your code</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="r-basics.html"><a href="r-basics.html#exercises"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
<li class="chapter" data-level="2.4" data-path="r-basics.html"><a href="r-basics.html#data-types"><i class="fa fa-check"></i><b>2.4</b> Data types</a><ul>
<li class="chapter" data-level="2.4.1" data-path="r-basics.html"><a href="r-basics.html#data-frames"><i class="fa fa-check"></i><b>2.4.1</b> Data frames</a></li>
<li class="chapter" data-level="2.4.2" data-path="r-basics.html"><a href="r-basics.html#examining-an-object"><i class="fa fa-check"></i><b>2.4.2</b> Examining an object</a></li>
<li class="chapter" data-level="2.4.3" data-path="r-basics.html"><a href="r-basics.html#the-accessor"><i class="fa fa-check"></i><b>2.4.3</b> The accessor: <code>$</code></a></li>
<li class="chapter" data-level="2.4.4" data-path="r-basics.html"><a href="r-basics.html#vectors-numerics-characters-and-logical"><i class="fa fa-check"></i><b>2.4.4</b> Vectors: numerics, characters, and logical</a></li>
<li class="chapter" data-level="2.4.5" data-path="r-basics.html"><a href="r-basics.html#factors"><i class="fa fa-check"></i><b>2.4.5</b> Factors</a></li>
<li class="chapter" data-level="2.4.6" data-path="r-basics.html"><a href="r-basics.html#lists"><i class="fa fa-check"></i><b>2.4.6</b> Lists</a></li>
<li class="chapter" data-level="2.4.7" data-path="r-basics.html"><a href="r-basics.html#matrices"><i class="fa fa-check"></i><b>2.4.7</b> Matrices</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="r-basics.html"><a href="r-basics.html#exercises-1"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
<li class="chapter" data-level="2.6" data-path="r-basics.html"><a href="r-basics.html#vectors"><i class="fa fa-check"></i><b>2.6</b> Vectors</a><ul>
<li class="chapter" data-level="2.6.1" data-path="r-basics.html"><a href="r-basics.html#creating-vectors"><i class="fa fa-check"></i><b>2.6.1</b> Creating vectors</a></li>
<li class="chapter" data-level="2.6.2" data-path="r-basics.html"><a href="r-basics.html#names"><i class="fa fa-check"></i><b>2.6.2</b> Names</a></li>
<li class="chapter" data-level="2.6.3" data-path="r-basics.html"><a href="r-basics.html#sequences"><i class="fa fa-check"></i><b>2.6.3</b> Sequences</a></li>
<li class="chapter" data-level="2.6.4" data-path="r-basics.html"><a href="r-basics.html#subsetting"><i class="fa fa-check"></i><b>2.6.4</b> Subsetting</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="r-basics.html"><a href="r-basics.html#coercion"><i class="fa fa-check"></i><b>2.7</b> Coercion</a><ul>
<li class="chapter" data-level="2.7.1" data-path="r-basics.html"><a href="r-basics.html#not-availables-na"><i class="fa fa-check"></i><b>2.7.1</b> Not availables (NA)</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="r-basics.html"><a href="r-basics.html#exercises-2"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
<li class="chapter" data-level="2.9" data-path="r-basics.html"><a href="r-basics.html#sorting"><i class="fa fa-check"></i><b>2.9</b> Sorting</a><ul>
<li class="chapter" data-level="2.9.1" data-path="r-basics.html"><a href="r-basics.html#sort"><i class="fa fa-check"></i><b>2.9.1</b> <code>sort</code></a></li>
<li class="chapter" data-level="2.9.2" data-path="r-basics.html"><a href="r-basics.html#order"><i class="fa fa-check"></i><b>2.9.2</b> <code>order</code></a></li>
<li class="chapter" data-level="2.9.3" data-path="r-basics.html"><a href="r-basics.html#max-and-which.max"><i class="fa fa-check"></i><b>2.9.3</b> <code>max</code> and <code>which.max</code></a></li>
<li class="chapter" data-level="2.9.4" data-path="r-basics.html"><a href="r-basics.html#rank"><i class="fa fa-check"></i><b>2.9.4</b> <code>rank</code></a></li>
<li class="chapter" data-level="2.9.5" data-path="r-basics.html"><a href="r-basics.html#beware-of-recycling"><i class="fa fa-check"></i><b>2.9.5</b> Beware of recycling</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="r-basics.html"><a href="r-basics.html#exercises-3"><i class="fa fa-check"></i><b>2.10</b> Exercises</a></li>
<li class="chapter" data-level="2.11" data-path="r-basics.html"><a href="r-basics.html#vector-arithmetics"><i class="fa fa-check"></i><b>2.11</b> Vector arithmetics</a><ul>
<li class="chapter" data-level="2.11.1" data-path="r-basics.html"><a href="r-basics.html#rescaling-a-vector"><i class="fa fa-check"></i><b>2.11.1</b> Rescaling a vector</a></li>
<li class="chapter" data-level="2.11.2" data-path="r-basics.html"><a href="r-basics.html#two-vectors"><i class="fa fa-check"></i><b>2.11.2</b> Two vectors</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="r-basics.html"><a href="r-basics.html#exercises-4"><i class="fa fa-check"></i><b>2.12</b> Exercises</a></li>
<li class="chapter" data-level="2.13" data-path="r-basics.html"><a href="r-basics.html#indexing"><i class="fa fa-check"></i><b>2.13</b> Indexing</a><ul>
<li class="chapter" data-level="2.13.1" data-path="r-basics.html"><a href="r-basics.html#subsetting-with-logicals"><i class="fa fa-check"></i><b>2.13.1</b> Subsetting with logicals</a></li>
<li class="chapter" data-level="2.13.2" data-path="r-basics.html"><a href="r-basics.html#logical-operators"><i class="fa fa-check"></i><b>2.13.2</b> Logical operators</a></li>
<li class="chapter" data-level="2.13.3" data-path="r-basics.html"><a href="r-basics.html#which"><i class="fa fa-check"></i><b>2.13.3</b> <code>which</code></a></li>
<li class="chapter" data-level="2.13.4" data-path="r-basics.html"><a href="r-basics.html#match"><i class="fa fa-check"></i><b>2.13.4</b> <code>match</code></a></li>
<li class="chapter" data-level="2.13.5" data-path="r-basics.html"><a href="r-basics.html#in"><i class="fa fa-check"></i><b>2.13.5</b> <code>%in%</code></a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="r-basics.html"><a href="r-basics.html#exercises-5"><i class="fa fa-check"></i><b>2.14</b> Exercises</a></li>
<li class="chapter" data-level="2.15" data-path="r-basics.html"><a href="r-basics.html#basic-plots"><i class="fa fa-check"></i><b>2.15</b> Basic plots</a><ul>
<li class="chapter" data-level="2.15.1" data-path="r-basics.html"><a href="r-basics.html#plot"><i class="fa fa-check"></i><b>2.15.1</b> <code>plot</code></a></li>
<li class="chapter" data-level="2.15.2" data-path="r-basics.html"><a href="r-basics.html#hist"><i class="fa fa-check"></i><b>2.15.2</b> <code>hist</code></a></li>
<li class="chapter" data-level="2.15.3" data-path="r-basics.html"><a href="r-basics.html#boxplot"><i class="fa fa-check"></i><b>2.15.3</b> <code>boxplot</code></a></li>
<li class="chapter" data-level="2.15.4" data-path="r-basics.html"><a href="r-basics.html#image"><i class="fa fa-check"></i><b>2.15.4</b> <code>image</code></a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="r-basics.html"><a href="r-basics.html#exercises-6"><i class="fa fa-check"></i><b>2.16</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="programming-basics.html"><a href="programming-basics.html"><i class="fa fa-check"></i><b>3</b> Programming basics</a><ul>
<li class="chapter" data-level="3.1" data-path="programming-basics.html"><a href="programming-basics.html#conditionals"><i class="fa fa-check"></i><b>3.1</b> Conditional expressions</a></li>
<li class="chapter" data-level="3.2" data-path="programming-basics.html"><a href="programming-basics.html#defining-functions"><i class="fa fa-check"></i><b>3.2</b> Defining functions</a></li>
<li class="chapter" data-level="3.3" data-path="programming-basics.html"><a href="programming-basics.html#namespaces"><i class="fa fa-check"></i><b>3.3</b> Namespaces</a></li>
<li class="chapter" data-level="3.4" data-path="programming-basics.html"><a href="programming-basics.html#for-loops"><i class="fa fa-check"></i><b>3.4</b> For-loops</a></li>
<li class="chapter" data-level="3.5" data-path="programming-basics.html"><a href="programming-basics.html#vectorization"><i class="fa fa-check"></i><b>3.5</b> Vectorization and functionals</a></li>
<li class="chapter" data-level="3.6" data-path="programming-basics.html"><a href="programming-basics.html#exercises-7"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="tidyverse.html"><a href="tidyverse.html"><i class="fa fa-check"></i><b>4</b> The tidyverse</a><ul>
<li class="chapter" data-level="4.1" data-path="tidyverse.html"><a href="tidyverse.html#tidy-data"><i class="fa fa-check"></i><b>4.1</b> Tidy data</a></li>
<li class="chapter" data-level="4.2" data-path="tidyverse.html"><a href="tidyverse.html#exercises-8"><i class="fa fa-check"></i><b>4.2</b> Exercises</a></li>
<li class="chapter" data-level="4.3" data-path="tidyverse.html"><a href="tidyverse.html#manipulating-data-frames"><i class="fa fa-check"></i><b>4.3</b> Manipulating data frames</a><ul>
<li class="chapter" data-level="4.3.1" data-path="tidyverse.html"><a href="tidyverse.html#adding-a-column-with-mutate"><i class="fa fa-check"></i><b>4.3.1</b> Adding a column with <code>mutate</code></a></li>
<li class="chapter" data-level="4.3.2" data-path="tidyverse.html"><a href="tidyverse.html#subsetting-with-filter"><i class="fa fa-check"></i><b>4.3.2</b> Subsetting with <code>filter</code></a></li>
<li class="chapter" data-level="4.3.3" data-path="tidyverse.html"><a href="tidyverse.html#selecting-columns-with-select"><i class="fa fa-check"></i><b>4.3.3</b> Selecting columns with <code>select</code></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tidyverse.html"><a href="tidyverse.html#exercises-9"><i class="fa fa-check"></i><b>4.4</b> Exercises</a></li>
<li class="chapter" data-level="4.5" data-path="tidyverse.html"><a href="tidyverse.html#the-pipe"><i class="fa fa-check"></i><b>4.5</b> The pipe: <code>%&gt;%</code></a></li>
<li class="chapter" data-level="4.6" data-path="tidyverse.html"><a href="tidyverse.html#exercises-10"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
<li class="chapter" data-level="4.7" data-path="tidyverse.html"><a href="tidyverse.html#summarizing-data"><i class="fa fa-check"></i><b>4.7</b> Summarizing data</a><ul>
<li class="chapter" data-level="4.7.1" data-path="tidyverse.html"><a href="tidyverse.html#summarize"><i class="fa fa-check"></i><b>4.7.1</b> <code>summarize</code></a></li>
<li class="chapter" data-level="4.7.2" data-path="tidyverse.html"><a href="tidyverse.html#pull"><i class="fa fa-check"></i><b>4.7.2</b> <code>pull</code></a></li>
<li class="chapter" data-level="4.7.3" data-path="tidyverse.html"><a href="tidyverse.html#group-by"><i class="fa fa-check"></i><b>4.7.3</b> Group then summarize with <code>group_by</code></a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="tidyverse.html"><a href="tidyverse.html#sorting-data-frames"><i class="fa fa-check"></i><b>4.8</b> Sorting data frames</a><ul>
<li class="chapter" data-level="4.8.1" data-path="tidyverse.html"><a href="tidyverse.html#nested-sorting"><i class="fa fa-check"></i><b>4.8.1</b> Nested sorting</a></li>
<li class="chapter" data-level="4.8.2" data-path="tidyverse.html"><a href="tidyverse.html#the-top-n"><i class="fa fa-check"></i><b>4.8.2</b> The top <span class="math inline">\(n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="tidyverse.html"><a href="tidyverse.html#exercises-11"><i class="fa fa-check"></i><b>4.9</b> Exercises</a></li>
<li class="chapter" data-level="4.10" data-path="tidyverse.html"><a href="tidyverse.html#tibbles"><i class="fa fa-check"></i><b>4.10</b> Tibbles</a><ul>
<li class="chapter" data-level="4.10.1" data-path="tidyverse.html"><a href="tidyverse.html#tibbles-display-better"><i class="fa fa-check"></i><b>4.10.1</b> Tibbles display better</a></li>
<li class="chapter" data-level="4.10.2" data-path="tidyverse.html"><a href="tidyverse.html#subsets-of-tibbles-are-tibbles"><i class="fa fa-check"></i><b>4.10.2</b> Subsets of tibbles are tibbles</a></li>
<li class="chapter" data-level="4.10.3" data-path="tidyverse.html"><a href="tidyverse.html#tibbles-can-have-complex-entries"><i class="fa fa-check"></i><b>4.10.3</b> Tibbles can have complex entries</a></li>
<li class="chapter" data-level="4.10.4" data-path="tidyverse.html"><a href="tidyverse.html#tibbles-can-be-grouped"><i class="fa fa-check"></i><b>4.10.4</b> Tibbles can be grouped</a></li>
<li class="chapter" data-level="4.10.5" data-path="tidyverse.html"><a href="tidyverse.html#create-a-tibble-using-tibble-instead-of-data.frame"><i class="fa fa-check"></i><b>4.10.5</b> Create a tibble using <code>tibble</code> instead of <code>data.frame</code></a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="tidyverse.html"><a href="tidyverse.html#the-dot-operator"><i class="fa fa-check"></i><b>4.11</b> The dot operator</a></li>
<li class="chapter" data-level="4.12" data-path="tidyverse.html"><a href="tidyverse.html#do"><i class="fa fa-check"></i><b>4.12</b> <code>do</code></a></li>
<li class="chapter" data-level="4.13" data-path="tidyverse.html"><a href="tidyverse.html#the-purrr-package"><i class="fa fa-check"></i><b>4.13</b> The <strong>purrr</strong> package</a></li>
<li class="chapter" data-level="4.14" data-path="tidyverse.html"><a href="tidyverse.html#tidyverse-conditionals"><i class="fa fa-check"></i><b>4.14</b> Tidyverse conditionals</a><ul>
<li class="chapter" data-level="4.14.1" data-path="tidyverse.html"><a href="tidyverse.html#case_when"><i class="fa fa-check"></i><b>4.14.1</b> <code>case_when</code></a></li>
<li class="chapter" data-level="4.14.2" data-path="tidyverse.html"><a href="tidyverse.html#between"><i class="fa fa-check"></i><b>4.14.2</b> <code>between</code></a></li>
</ul></li>
<li class="chapter" data-level="4.15" data-path="tidyverse.html"><a href="tidyverse.html#exercises-12"><i class="fa fa-check"></i><b>4.15</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="importing-data.html"><a href="importing-data.html"><i class="fa fa-check"></i><b>5</b> Importing data</a><ul>
<li class="chapter" data-level="5.1" data-path="importing-data.html"><a href="importing-data.html#paths-and-the-working-directory"><i class="fa fa-check"></i><b>5.1</b> Paths and the working directory</a><ul>
<li class="chapter" data-level="5.1.1" data-path="importing-data.html"><a href="importing-data.html#the-filesystem"><i class="fa fa-check"></i><b>5.1.1</b> The filesystem</a></li>
<li class="chapter" data-level="5.1.2" data-path="importing-data.html"><a href="importing-data.html#relative-and-full-paths"><i class="fa fa-check"></i><b>5.1.2</b> Relative and full paths</a></li>
<li class="chapter" data-level="5.1.3" data-path="importing-data.html"><a href="importing-data.html#the-working-directory"><i class="fa fa-check"></i><b>5.1.3</b> The working directory</a></li>
<li class="chapter" data-level="5.1.4" data-path="importing-data.html"><a href="importing-data.html#generating-path-names"><i class="fa fa-check"></i><b>5.1.4</b> Generating path names</a></li>
<li class="chapter" data-level="5.1.5" data-path="importing-data.html"><a href="importing-data.html#copying-files-using-paths"><i class="fa fa-check"></i><b>5.1.5</b> Copying files using paths</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="importing-data.html"><a href="importing-data.html#the-readr-and-readxl-packages"><i class="fa fa-check"></i><b>5.2</b> The readr and readxl packages</a><ul>
<li class="chapter" data-level="5.2.1" data-path="importing-data.html"><a href="importing-data.html#readr"><i class="fa fa-check"></i><b>5.2.1</b> readr</a></li>
<li class="chapter" data-level="5.2.2" data-path="importing-data.html"><a href="importing-data.html#readxl"><i class="fa fa-check"></i><b>5.2.2</b> readxl</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="importing-data.html"><a href="importing-data.html#exercises-13"><i class="fa fa-check"></i><b>5.3</b> Exercises</a></li>
<li class="chapter" data-level="5.4" data-path="importing-data.html"><a href="importing-data.html#downloading-files"><i class="fa fa-check"></i><b>5.4</b> Downloading files</a></li>
<li class="chapter" data-level="5.5" data-path="importing-data.html"><a href="importing-data.html#r-base-importing-functions"><i class="fa fa-check"></i><b>5.5</b> R-base importing functions</a><ul>
<li class="chapter" data-level="5.5.1" data-path="importing-data.html"><a href="importing-data.html#scan"><i class="fa fa-check"></i><b>5.5.1</b> <code>scan</code></a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="importing-data.html"><a href="importing-data.html#text-versus-binary-files"><i class="fa fa-check"></i><b>5.6</b> Text versus binary files</a></li>
<li class="chapter" data-level="5.7" data-path="importing-data.html"><a href="importing-data.html#unicode-versus-ascii"><i class="fa fa-check"></i><b>5.7</b> Unicode versus ASCII</a></li>
<li class="chapter" data-level="5.8" data-path="importing-data.html"><a href="importing-data.html#organizing-data-with-spreadsheets"><i class="fa fa-check"></i><b>5.8</b> Organizing data with spreadsheets</a></li>
<li class="chapter" data-level="5.9" data-path="importing-data.html"><a href="importing-data.html#exercises-14"><i class="fa fa-check"></i><b>5.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Data Visualization</b></span></li>
<li class="chapter" data-level="6" data-path="introduction-to-data-visualization.html"><a href="introduction-to-data-visualization.html"><i class="fa fa-check"></i><b>6</b> Introduction to data visualization</a></li>
<li class="chapter" data-level="7" data-path="ggplot2.html"><a href="ggplot2.html"><i class="fa fa-check"></i><b>7</b> ggplot2</a><ul>
<li class="chapter" data-level="7.1" data-path="ggplot2.html"><a href="ggplot2.html#the-components-of-a-graph"><i class="fa fa-check"></i><b>7.1</b> The components of a graph</a></li>
<li class="chapter" data-level="7.2" data-path="ggplot2.html"><a href="ggplot2.html#ggplot-objects"><i class="fa fa-check"></i><b>7.2</b> <code>ggplot</code> objects</a></li>
<li class="chapter" data-level="7.3" data-path="ggplot2.html"><a href="ggplot2.html#geometries"><i class="fa fa-check"></i><b>7.3</b> Geometries</a></li>
<li class="chapter" data-level="7.4" data-path="ggplot2.html"><a href="ggplot2.html#aesthetic-mappings"><i class="fa fa-check"></i><b>7.4</b> Aesthetic mappings</a></li>
<li class="chapter" data-level="7.5" data-path="ggplot2.html"><a href="ggplot2.html#layers"><i class="fa fa-check"></i><b>7.5</b> Layers</a><ul>
<li class="chapter" data-level="7.5.1" data-path="ggplot2.html"><a href="ggplot2.html#tinkering-with-arguments"><i class="fa fa-check"></i><b>7.5.1</b> Tinkering with arguments</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="ggplot2.html"><a href="ggplot2.html#global-versus-local-aesthetic-mappings"><i class="fa fa-check"></i><b>7.6</b> Global versus local aesthetic mappings</a></li>
<li class="chapter" data-level="7.7" data-path="ggplot2.html"><a href="ggplot2.html#scales"><i class="fa fa-check"></i><b>7.7</b> Scales</a></li>
<li class="chapter" data-level="7.8" data-path="ggplot2.html"><a href="ggplot2.html#labels-and-titles"><i class="fa fa-check"></i><b>7.8</b> Labels and titles</a></li>
<li class="chapter" data-level="7.9" data-path="ggplot2.html"><a href="ggplot2.html#categories-as-colors"><i class="fa fa-check"></i><b>7.9</b> Categories as colors</a></li>
<li class="chapter" data-level="7.10" data-path="ggplot2.html"><a href="ggplot2.html#annotation-shapes-and-adjustments"><i class="fa fa-check"></i><b>7.10</b> Annotation, shapes, and adjustments</a></li>
<li class="chapter" data-level="7.11" data-path="ggplot2.html"><a href="ggplot2.html#add-on-packages"><i class="fa fa-check"></i><b>7.11</b> Add-on packages</a></li>
<li class="chapter" data-level="7.12" data-path="ggplot2.html"><a href="ggplot2.html#putting-it-all-together"><i class="fa fa-check"></i><b>7.12</b> Putting it all together</a></li>
<li class="chapter" data-level="7.13" data-path="ggplot2.html"><a href="ggplot2.html#qplot"><i class="fa fa-check"></i><b>7.13</b> Quick plots with <code>qplot</code></a></li>
<li class="chapter" data-level="7.14" data-path="ggplot2.html"><a href="ggplot2.html#grids-of-plots"><i class="fa fa-check"></i><b>7.14</b> Grids of plots</a></li>
<li class="chapter" data-level="7.15" data-path="ggplot2.html"><a href="ggplot2.html#exercises-15"><i class="fa fa-check"></i><b>7.15</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>8</b> Visualizing data distributions</a><ul>
<li class="chapter" data-level="8.1" data-path="distributions.html"><a href="distributions.html#variable-types"><i class="fa fa-check"></i><b>8.1</b> Variable types</a></li>
<li class="chapter" data-level="8.2" data-path="distributions.html"><a href="distributions.html#case-study-describing-student-heights"><i class="fa fa-check"></i><b>8.2</b> Case study: describing student heights</a></li>
<li class="chapter" data-level="8.3" data-path="distributions.html"><a href="distributions.html#distribution-function"><i class="fa fa-check"></i><b>8.3</b> Distribution function</a></li>
<li class="chapter" data-level="8.4" data-path="distributions.html"><a href="distributions.html#cdf-intro"><i class="fa fa-check"></i><b>8.4</b> Cumulative distribution functions</a></li>
<li class="chapter" data-level="8.5" data-path="distributions.html"><a href="distributions.html#histograms"><i class="fa fa-check"></i><b>8.5</b> Histograms</a></li>
<li class="chapter" data-level="8.6" data-path="distributions.html"><a href="distributions.html#smoothed-density"><i class="fa fa-check"></i><b>8.6</b> Smoothed density</a><ul>
<li class="chapter" data-level="8.6.1" data-path="distributions.html"><a href="distributions.html#interpreting-the-y-axis"><i class="fa fa-check"></i><b>8.6.1</b> Interpreting the y-axis</a></li>
<li class="chapter" data-level="8.6.2" data-path="distributions.html"><a href="distributions.html#densities-permit-stratification"><i class="fa fa-check"></i><b>8.6.2</b> Densities permit stratification</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="distributions.html"><a href="distributions.html#exercises-16"><i class="fa fa-check"></i><b>8.7</b> Exercises</a></li>
<li class="chapter" data-level="8.8" data-path="distributions.html"><a href="distributions.html#normal-distribution"><i class="fa fa-check"></i><b>8.8</b> The normal distribution</a></li>
<li class="chapter" data-level="8.9" data-path="distributions.html"><a href="distributions.html#standard-units"><i class="fa fa-check"></i><b>8.9</b> Standard units</a></li>
<li class="chapter" data-level="8.10" data-path="distributions.html"><a href="distributions.html#quantile-quantile-plots"><i class="fa fa-check"></i><b>8.10</b> Quantile-quantile plots</a></li>
<li class="chapter" data-level="8.11" data-path="distributions.html"><a href="distributions.html#percentiles"><i class="fa fa-check"></i><b>8.11</b> Percentiles</a></li>
<li class="chapter" data-level="8.12" data-path="distributions.html"><a href="distributions.html#boxplots"><i class="fa fa-check"></i><b>8.12</b> Boxplots</a></li>
<li class="chapter" data-level="8.13" data-path="distributions.html"><a href="distributions.html#stratification"><i class="fa fa-check"></i><b>8.13</b> Stratification</a></li>
<li class="chapter" data-level="8.14" data-path="distributions.html"><a href="distributions.html#student-height-cont"><i class="fa fa-check"></i><b>8.14</b> Case study: describing student heights (continued)</a></li>
<li class="chapter" data-level="8.15" data-path="distributions.html"><a href="distributions.html#exercises-17"><i class="fa fa-check"></i><b>8.15</b> Exercises</a></li>
<li class="chapter" data-level="8.16" data-path="distributions.html"><a href="distributions.html#other-geometries"><i class="fa fa-check"></i><b>8.16</b> ggplot2 geometries</a><ul>
<li class="chapter" data-level="8.16.1" data-path="distributions.html"><a href="distributions.html#barplots"><i class="fa fa-check"></i><b>8.16.1</b> Barplots</a></li>
<li class="chapter" data-level="8.16.2" data-path="distributions.html"><a href="distributions.html#histograms-1"><i class="fa fa-check"></i><b>8.16.2</b> Histograms</a></li>
<li class="chapter" data-level="8.16.3" data-path="distributions.html"><a href="distributions.html#density-plots"><i class="fa fa-check"></i><b>8.16.3</b> Density plots</a></li>
<li class="chapter" data-level="8.16.4" data-path="distributions.html"><a href="distributions.html#boxplots-1"><i class="fa fa-check"></i><b>8.16.4</b> Boxplots</a></li>
<li class="chapter" data-level="8.16.5" data-path="distributions.html"><a href="distributions.html#qq-plots"><i class="fa fa-check"></i><b>8.16.5</b> QQ-plots</a></li>
<li class="chapter" data-level="8.16.6" data-path="distributions.html"><a href="distributions.html#images"><i class="fa fa-check"></i><b>8.16.6</b> Images</a></li>
<li class="chapter" data-level="8.16.7" data-path="distributions.html"><a href="distributions.html#quick-plots"><i class="fa fa-check"></i><b>8.16.7</b> Quick plots</a></li>
</ul></li>
<li class="chapter" data-level="8.17" data-path="distributions.html"><a href="distributions.html#exercises-18"><i class="fa fa-check"></i><b>8.17</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="gapminder.html"><a href="gapminder.html"><i class="fa fa-check"></i><b>9</b> Data visualization in practice</a><ul>
<li class="chapter" data-level="9.1" data-path="gapminder.html"><a href="gapminder.html#case-study-new-insights-on-poverty"><i class="fa fa-check"></i><b>9.1</b> Case study: new insights on poverty</a><ul>
<li class="chapter" data-level="9.1.1" data-path="gapminder.html"><a href="gapminder.html#hans-roslings-quiz"><i class="fa fa-check"></i><b>9.1.1</b> Hans Rosling’s quiz</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="gapminder.html"><a href="gapminder.html#scatterplots"><i class="fa fa-check"></i><b>9.2</b> Scatterplots</a></li>
<li class="chapter" data-level="9.3" data-path="gapminder.html"><a href="gapminder.html#faceting"><i class="fa fa-check"></i><b>9.3</b> Faceting</a><ul>
<li class="chapter" data-level="9.3.1" data-path="gapminder.html"><a href="gapminder.html#facet_wrap"><i class="fa fa-check"></i><b>9.3.1</b> <code>facet_wrap</code></a></li>
<li class="chapter" data-level="9.3.2" data-path="gapminder.html"><a href="gapminder.html#fixed-scales-for-better-comparisons"><i class="fa fa-check"></i><b>9.3.2</b> Fixed scales for better comparisons</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="gapminder.html"><a href="gapminder.html#time-series-plots"><i class="fa fa-check"></i><b>9.4</b> Time series plots</a><ul>
<li class="chapter" data-level="9.4.1" data-path="gapminder.html"><a href="gapminder.html#labels-instead-of-legends"><i class="fa fa-check"></i><b>9.4.1</b> Labels instead of legends</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="gapminder.html"><a href="gapminder.html#data-transformations"><i class="fa fa-check"></i><b>9.5</b> Data transformations</a><ul>
<li class="chapter" data-level="9.5.1" data-path="gapminder.html"><a href="gapminder.html#log-transformation"><i class="fa fa-check"></i><b>9.5.1</b> Log transformation</a></li>
<li class="chapter" data-level="9.5.2" data-path="gapminder.html"><a href="gapminder.html#which-base"><i class="fa fa-check"></i><b>9.5.2</b> Which base?</a></li>
<li class="chapter" data-level="9.5.3" data-path="gapminder.html"><a href="gapminder.html#transform-the-values-or-the-scale"><i class="fa fa-check"></i><b>9.5.3</b> Transform the values or the scale?</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="gapminder.html"><a href="gapminder.html#visualizing-multimodal-distributions"><i class="fa fa-check"></i><b>9.6</b> Visualizing multimodal distributions</a></li>
<li class="chapter" data-level="9.7" data-path="gapminder.html"><a href="gapminder.html#comparing-multiple-distributions-with-boxplots-and-ridge-plots"><i class="fa fa-check"></i><b>9.7</b> Comparing multiple distributions with boxplots and ridge plots</a><ul>
<li class="chapter" data-level="9.7.1" data-path="gapminder.html"><a href="gapminder.html#boxplots-2"><i class="fa fa-check"></i><b>9.7.1</b> Boxplots</a></li>
<li class="chapter" data-level="9.7.2" data-path="gapminder.html"><a href="gapminder.html#ridge-plots"><i class="fa fa-check"></i><b>9.7.2</b> Ridge plots</a></li>
<li class="chapter" data-level="9.7.3" data-path="gapminder.html"><a href="gapminder.html#example-1970-versus-2010-income-distributions"><i class="fa fa-check"></i><b>9.7.3</b> Example: 1970 versus 2010 income distributions</a></li>
<li class="chapter" data-level="9.7.4" data-path="gapminder.html"><a href="gapminder.html#accessing-computed-variables"><i class="fa fa-check"></i><b>9.7.4</b> Accessing computed variables</a></li>
<li class="chapter" data-level="9.7.5" data-path="gapminder.html"><a href="gapminder.html#weighted-densities"><i class="fa fa-check"></i><b>9.7.5</b> Weighted densities</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="gapminder.html"><a href="gapminder.html#the-ecological-fallacy-and-importance-of-showing-the-data"><i class="fa fa-check"></i><b>9.8</b> The ecological fallacy and importance of showing the data</a><ul>
<li class="chapter" data-level="9.8.1" data-path="gapminder.html"><a href="gapminder.html#logit"><i class="fa fa-check"></i><b>9.8.1</b> Logistic transformation</a></li>
<li class="chapter" data-level="9.8.2" data-path="gapminder.html"><a href="gapminder.html#show-the-data"><i class="fa fa-check"></i><b>9.8.2</b> Show the data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html"><i class="fa fa-check"></i><b>10</b> Data visualization principles</a><ul>
<li class="chapter" data-level="10.1" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#encoding-data-using-visual-cues"><i class="fa fa-check"></i><b>10.1</b> Encoding data using visual cues</a></li>
<li class="chapter" data-level="10.2" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#know-when-to-include-0"><i class="fa fa-check"></i><b>10.2</b> Know when to include 0</a></li>
<li class="chapter" data-level="10.3" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#do-not-distort-quantities"><i class="fa fa-check"></i><b>10.3</b> Do not distort quantities</a></li>
<li class="chapter" data-level="10.4" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#order-categories-by-a-meaningful-value"><i class="fa fa-check"></i><b>10.4</b> Order categories by a meaningful value</a></li>
<li class="chapter" data-level="10.5" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#show-the-data-1"><i class="fa fa-check"></i><b>10.5</b> Show the data</a></li>
<li class="chapter" data-level="10.6" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#ease-comparisons"><i class="fa fa-check"></i><b>10.6</b> Ease comparisons</a><ul>
<li class="chapter" data-level="10.6.1" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#use-common-axes"><i class="fa fa-check"></i><b>10.6.1</b> Use common axes</a></li>
<li class="chapter" data-level="10.6.2" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#align-plots-vertically-to-see-horizontal-changes-and-horizontally-to-see-vertical-changes"><i class="fa fa-check"></i><b>10.6.2</b> Align plots vertically to see horizontal changes and horizontally to see vertical changes</a></li>
<li class="chapter" data-level="10.6.3" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#consider-transformations"><i class="fa fa-check"></i><b>10.6.3</b> Consider transformations</a></li>
<li class="chapter" data-level="10.6.4" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#visual-cues-to-be-compared-should-be-adjacent"><i class="fa fa-check"></i><b>10.6.4</b> Visual cues to be compared should be adjacent</a></li>
<li class="chapter" data-level="10.6.5" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#use-color"><i class="fa fa-check"></i><b>10.6.5</b> Use color</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#think-of-the-color-blind"><i class="fa fa-check"></i><b>10.7</b> Think of the color blind</a></li>
<li class="chapter" data-level="10.8" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#plots-for-two-variables"><i class="fa fa-check"></i><b>10.8</b> Plots for two variables</a><ul>
<li class="chapter" data-level="10.8.1" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#slope-charts"><i class="fa fa-check"></i><b>10.8.1</b> Slope charts</a></li>
<li class="chapter" data-level="10.8.2" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#bland-altman-plot"><i class="fa fa-check"></i><b>10.8.2</b> Bland-Altman plot</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#encoding-a-third-variable"><i class="fa fa-check"></i><b>10.9</b> Encoding a third variable</a></li>
<li class="chapter" data-level="10.10" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#avoid-pseudo-three-dimensional-plots"><i class="fa fa-check"></i><b>10.10</b> Avoid pseudo-three-dimensional plots</a></li>
<li class="chapter" data-level="10.11" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#avoid-too-many-significant-digits"><i class="fa fa-check"></i><b>10.11</b> Avoid too many significant digits</a></li>
<li class="chapter" data-level="10.12" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#know-your-audience"><i class="fa fa-check"></i><b>10.12</b> Know your audience</a></li>
<li class="chapter" data-level="10.13" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#exercises-19"><i class="fa fa-check"></i><b>10.13</b> Exercises</a></li>
<li class="chapter" data-level="10.14" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#vaccines"><i class="fa fa-check"></i><b>10.14</b> Case study: vaccines and infectious diseases</a></li>
<li class="chapter" data-level="10.15" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#exercises-20"><i class="fa fa-check"></i><b>10.15</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="robust-summaries.html"><a href="robust-summaries.html"><i class="fa fa-check"></i><b>11</b> Robust summaries</a><ul>
<li class="chapter" data-level="11.1" data-path="robust-summaries.html"><a href="robust-summaries.html#outliers"><i class="fa fa-check"></i><b>11.1</b> Outliers</a></li>
<li class="chapter" data-level="11.2" data-path="robust-summaries.html"><a href="robust-summaries.html#median"><i class="fa fa-check"></i><b>11.2</b> Median</a></li>
<li class="chapter" data-level="11.3" data-path="robust-summaries.html"><a href="robust-summaries.html#the-inter-quartile-range-iqr"><i class="fa fa-check"></i><b>11.3</b> The inter quartile range (IQR)</a></li>
<li class="chapter" data-level="11.4" data-path="robust-summaries.html"><a href="robust-summaries.html#tukeys-definition-of-an-outlier"><i class="fa fa-check"></i><b>11.4</b> Tukey’s definition of an outlier</a></li>
<li class="chapter" data-level="11.5" data-path="robust-summaries.html"><a href="robust-summaries.html#median-absolute-deviation"><i class="fa fa-check"></i><b>11.5</b> Median absolute deviation</a></li>
<li class="chapter" data-level="11.6" data-path="robust-summaries.html"><a href="robust-summaries.html#exercises-21"><i class="fa fa-check"></i><b>11.6</b> Exercises</a></li>
<li class="chapter" data-level="11.7" data-path="robust-summaries.html"><a href="robust-summaries.html#case-study-self-reported-student-heights"><i class="fa fa-check"></i><b>11.7</b> Case study: self-reported student heights</a></li>
</ul></li>
<li class="part"><span><b>III Statistics with R</b></span></li>
<li class="chapter" data-level="12" data-path="introduction-to-statistics-with-r.html"><a href="introduction-to-statistics-with-r.html"><i class="fa fa-check"></i><b>12</b> Introduction to statistics with R</a></li>
<li class="chapter" data-level="13" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>13</b> Probability</a><ul>
<li class="chapter" data-level="13.1" data-path="probability.html"><a href="probability.html#discrete-probability"><i class="fa fa-check"></i><b>13.1</b> Discrete probability</a><ul>
<li class="chapter" data-level="13.1.1" data-path="probability.html"><a href="probability.html#relative-frequency"><i class="fa fa-check"></i><b>13.1.1</b> Relative frequency</a></li>
<li class="chapter" data-level="13.1.2" data-path="probability.html"><a href="probability.html#notation"><i class="fa fa-check"></i><b>13.1.2</b> Notation</a></li>
<li class="chapter" data-level="13.1.3" data-path="probability.html"><a href="probability.html#probability-distributions"><i class="fa fa-check"></i><b>13.1.3</b> Probability distributions</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="probability.html"><a href="probability.html#monte-carlo-simulations-for-categorical-data"><i class="fa fa-check"></i><b>13.2</b> Monte Carlo simulations for categorical data</a><ul>
<li class="chapter" data-level="13.2.1" data-path="probability.html"><a href="probability.html#setting-the-random-seed"><i class="fa fa-check"></i><b>13.2.1</b> Setting the random seed</a></li>
<li class="chapter" data-level="13.2.2" data-path="probability.html"><a href="probability.html#with-and-without-replacement"><i class="fa fa-check"></i><b>13.2.2</b> With and without replacement</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="probability.html"><a href="probability.html#independence"><i class="fa fa-check"></i><b>13.3</b> Independence</a></li>
<li class="chapter" data-level="13.4" data-path="probability.html"><a href="probability.html#conditional-probabilities"><i class="fa fa-check"></i><b>13.4</b> Conditional probabilities</a></li>
<li class="chapter" data-level="13.5" data-path="probability.html"><a href="probability.html#addition-and-multiplication-rules"><i class="fa fa-check"></i><b>13.5</b> Addition and multiplication rules</a><ul>
<li class="chapter" data-level="13.5.1" data-path="probability.html"><a href="probability.html#multiplication-rule"><i class="fa fa-check"></i><b>13.5.1</b> Multiplication rule</a></li>
<li class="chapter" data-level="13.5.2" data-path="probability.html"><a href="probability.html#multiplication-rule-under-independence"><i class="fa fa-check"></i><b>13.5.2</b> Multiplication rule under independence</a></li>
<li class="chapter" data-level="13.5.3" data-path="probability.html"><a href="probability.html#addition-rule"><i class="fa fa-check"></i><b>13.5.3</b> Addition rule</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="probability.html"><a href="probability.html#combinations-and-permutations"><i class="fa fa-check"></i><b>13.6</b> Combinations and permutations</a><ul>
<li class="chapter" data-level="13.6.1" data-path="probability.html"><a href="probability.html#monte-carlo-example"><i class="fa fa-check"></i><b>13.6.1</b> Monte Carlo example</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="probability.html"><a href="probability.html#examples"><i class="fa fa-check"></i><b>13.7</b> Examples</a><ul>
<li class="chapter" data-level="13.7.1" data-path="probability.html"><a href="probability.html#monty-hall-problem"><i class="fa fa-check"></i><b>13.7.1</b> Monty Hall problem</a></li>
<li class="chapter" data-level="13.7.2" data-path="probability.html"><a href="probability.html#birthday-problem"><i class="fa fa-check"></i><b>13.7.2</b> Birthday problem</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="probability.html"><a href="probability.html#infinity-in-practice"><i class="fa fa-check"></i><b>13.8</b> Infinity in practice</a></li>
<li class="chapter" data-level="13.9" data-path="probability.html"><a href="probability.html#exercises-22"><i class="fa fa-check"></i><b>13.9</b> Exercises</a></li>
<li class="chapter" data-level="13.10" data-path="probability.html"><a href="probability.html#continuous-probability"><i class="fa fa-check"></i><b>13.10</b> Continuous probability</a></li>
<li class="chapter" data-level="13.11" data-path="probability.html"><a href="probability.html#theoretical-continuous-distributions"><i class="fa fa-check"></i><b>13.11</b> Theoretical continuous distributions</a><ul>
<li class="chapter" data-level="13.11.1" data-path="probability.html"><a href="probability.html#theoretical-distributions-as-approximations"><i class="fa fa-check"></i><b>13.11.1</b> Theoretical distributions as approximations</a></li>
<li class="chapter" data-level="13.11.2" data-path="probability.html"><a href="probability.html#the-probability-density"><i class="fa fa-check"></i><b>13.11.2</b> The probability density</a></li>
</ul></li>
<li class="chapter" data-level="13.12" data-path="probability.html"><a href="probability.html#monte-carlo-simulations-for-continuous-variables"><i class="fa fa-check"></i><b>13.12</b> Monte Carlo simulations for continuous variables</a></li>
<li class="chapter" data-level="13.13" data-path="probability.html"><a href="probability.html#continuous-distributions"><i class="fa fa-check"></i><b>13.13</b> Continuous distributions</a></li>
<li class="chapter" data-level="13.14" data-path="probability.html"><a href="probability.html#exercises-23"><i class="fa fa-check"></i><b>13.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="random-variables.html"><a href="random-variables.html"><i class="fa fa-check"></i><b>14</b> Random variables</a><ul>
<li class="chapter" data-level="14.1" data-path="random-variables.html"><a href="random-variables.html#random-variables-1"><i class="fa fa-check"></i><b>14.1</b> Random variables</a></li>
<li class="chapter" data-level="14.2" data-path="random-variables.html"><a href="random-variables.html#sampling-models"><i class="fa fa-check"></i><b>14.2</b> Sampling models</a></li>
<li class="chapter" data-level="14.3" data-path="random-variables.html"><a href="random-variables.html#the-probability-distribution-of-a-random-variable"><i class="fa fa-check"></i><b>14.3</b> The probability distribution of a random variable</a></li>
<li class="chapter" data-level="14.4" data-path="random-variables.html"><a href="random-variables.html#distributions-versus-probability-distributions"><i class="fa fa-check"></i><b>14.4</b> Distributions versus probability distributions</a></li>
<li class="chapter" data-level="14.5" data-path="random-variables.html"><a href="random-variables.html#notation-for-random-variables"><i class="fa fa-check"></i><b>14.5</b> Notation for random variables</a></li>
<li class="chapter" data-level="14.6" data-path="random-variables.html"><a href="random-variables.html#the-expected-value-and-standard-error"><i class="fa fa-check"></i><b>14.6</b> The expected value and standard error</a><ul>
<li class="chapter" data-level="14.6.1" data-path="random-variables.html"><a href="random-variables.html#population-sd-versus-the-sample-sd"><i class="fa fa-check"></i><b>14.6.1</b> Population SD versus the sample SD</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="random-variables.html"><a href="random-variables.html#central-limit-theorem"><i class="fa fa-check"></i><b>14.7</b> Central Limit Theorem</a><ul>
<li class="chapter" data-level="14.7.1" data-path="random-variables.html"><a href="random-variables.html#how-large-is-large-in-the-central-limit-theorem"><i class="fa fa-check"></i><b>14.7.1</b> How large is large in the Central Limit Theorem?</a></li>
</ul></li>
<li class="chapter" data-level="14.8" data-path="random-variables.html"><a href="random-variables.html#statistical-properties-of-averages"><i class="fa fa-check"></i><b>14.8</b> Statistical properties of averages</a></li>
<li class="chapter" data-level="14.9" data-path="random-variables.html"><a href="random-variables.html#law-of-large-numbers"><i class="fa fa-check"></i><b>14.9</b> Law of large numbers</a><ul>
<li class="chapter" data-level="14.9.1" data-path="random-variables.html"><a href="random-variables.html#misinterpreting-law-of-averages"><i class="fa fa-check"></i><b>14.9.1</b> Misinterpreting law of averages</a></li>
</ul></li>
<li class="chapter" data-level="14.10" data-path="random-variables.html"><a href="random-variables.html#exercises-24"><i class="fa fa-check"></i><b>14.10</b> Exercises</a></li>
<li class="chapter" data-level="14.11" data-path="random-variables.html"><a href="random-variables.html#case-study-the-big-short"><i class="fa fa-check"></i><b>14.11</b> Case study: The Big Short</a><ul>
<li class="chapter" data-level="14.11.1" data-path="random-variables.html"><a href="random-variables.html#interest-rates-explained-with-chance-model"><i class="fa fa-check"></i><b>14.11.1</b> Interest rates explained with chance model</a></li>
<li class="chapter" data-level="14.11.2" data-path="random-variables.html"><a href="random-variables.html#the-big-short"><i class="fa fa-check"></i><b>14.11.2</b> The Big Short</a></li>
</ul></li>
<li class="chapter" data-level="14.12" data-path="random-variables.html"><a href="random-variables.html#exercises-25"><i class="fa fa-check"></i><b>14.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>15</b> Statistical inference</a><ul>
<li class="chapter" data-level="15.1" data-path="inference.html"><a href="inference.html#polls"><i class="fa fa-check"></i><b>15.1</b> Polls</a><ul>
<li class="chapter" data-level="15.1.1" data-path="inference.html"><a href="inference.html#the-sampling-model-for-polls"><i class="fa fa-check"></i><b>15.1.1</b> The sampling model for polls</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="inference.html"><a href="inference.html#populations-samples-parameters-and-estimates"><i class="fa fa-check"></i><b>15.2</b> Populations, samples, parameters, and estimates</a><ul>
<li class="chapter" data-level="15.2.1" data-path="inference.html"><a href="inference.html#the-sample-average"><i class="fa fa-check"></i><b>15.2.1</b> The sample average</a></li>
<li class="chapter" data-level="15.2.2" data-path="inference.html"><a href="inference.html#parameters"><i class="fa fa-check"></i><b>15.2.2</b> Parameters</a></li>
<li class="chapter" data-level="15.2.3" data-path="inference.html"><a href="inference.html#polling-versus-forecasting"><i class="fa fa-check"></i><b>15.2.3</b> Polling versus forecasting</a></li>
<li class="chapter" data-level="15.2.4" data-path="inference.html"><a href="inference.html#properties-of-our-estimate-expected-value-and-standard-error"><i class="fa fa-check"></i><b>15.2.4</b> Properties of our estimate: expected value and standard error</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="inference.html"><a href="inference.html#exercises-26"><i class="fa fa-check"></i><b>15.3</b> Exercises</a></li>
<li class="chapter" data-level="15.4" data-path="inference.html"><a href="inference.html#clt"><i class="fa fa-check"></i><b>15.4</b> Central Limit Theorem in practice</a><ul>
<li class="chapter" data-level="15.4.1" data-path="inference.html"><a href="inference.html#a-monte-carlo-simulation"><i class="fa fa-check"></i><b>15.4.1</b> A Monte Carlo simulation</a></li>
<li class="chapter" data-level="15.4.2" data-path="inference.html"><a href="inference.html#the-spread"><i class="fa fa-check"></i><b>15.4.2</b> The spread</a></li>
<li class="chapter" data-level="15.4.3" data-path="inference.html"><a href="inference.html#bias-why-not-run-a-very-large-poll"><i class="fa fa-check"></i><b>15.4.3</b> Bias: why not run a very large poll?</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="inference.html"><a href="inference.html#exercises-27"><i class="fa fa-check"></i><b>15.5</b> Exercises</a></li>
<li class="chapter" data-level="15.6" data-path="inference.html"><a href="inference.html#confidence-intervals"><i class="fa fa-check"></i><b>15.6</b> Confidence intervals</a><ul>
<li class="chapter" data-level="15.6.1" data-path="inference.html"><a href="inference.html#a-monte-carlo-simulation-1"><i class="fa fa-check"></i><b>15.6.1</b> A Monte Carlo simulation</a></li>
<li class="chapter" data-level="15.6.2" data-path="inference.html"><a href="inference.html#the-correct-language"><i class="fa fa-check"></i><b>15.6.2</b> The correct language</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="inference.html"><a href="inference.html#exercises-28"><i class="fa fa-check"></i><b>15.7</b> Exercises</a></li>
<li class="chapter" data-level="15.8" data-path="inference.html"><a href="inference.html#power"><i class="fa fa-check"></i><b>15.8</b> Power</a></li>
<li class="chapter" data-level="15.9" data-path="inference.html"><a href="inference.html#p-values"><i class="fa fa-check"></i><b>15.9</b> p-values</a></li>
<li class="chapter" data-level="15.10" data-path="inference.html"><a href="inference.html#association-tests"><i class="fa fa-check"></i><b>15.10</b> Association tests</a><ul>
<li class="chapter" data-level="15.10.1" data-path="inference.html"><a href="inference.html#lady-tasting-tea"><i class="fa fa-check"></i><b>15.10.1</b> Lady Tasting Tea</a></li>
<li class="chapter" data-level="15.10.2" data-path="inference.html"><a href="inference.html#two-by-two-tables"><i class="fa fa-check"></i><b>15.10.2</b> Two-by-two tables</a></li>
<li class="chapter" data-level="15.10.3" data-path="inference.html"><a href="inference.html#chi-square-test"><i class="fa fa-check"></i><b>15.10.3</b> Chi-square Test</a></li>
<li class="chapter" data-level="15.10.4" data-path="inference.html"><a href="inference.html#odds-ratio"><i class="fa fa-check"></i><b>15.10.4</b> The odds ratio</a></li>
<li class="chapter" data-level="15.10.5" data-path="inference.html"><a href="inference.html#confidence-intervals-for-the-odds-ratio"><i class="fa fa-check"></i><b>15.10.5</b> Confidence intervals for the odds ratio</a></li>
<li class="chapter" data-level="15.10.6" data-path="inference.html"><a href="inference.html#small-count-correction"><i class="fa fa-check"></i><b>15.10.6</b> Small count correction</a></li>
<li class="chapter" data-level="15.10.7" data-path="inference.html"><a href="inference.html#large-samples-small-p-values"><i class="fa fa-check"></i><b>15.10.7</b> Large samples, small p-values</a></li>
</ul></li>
<li class="chapter" data-level="15.11" data-path="inference.html"><a href="inference.html#exercises-29"><i class="fa fa-check"></i><b>15.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>16</b> Statistical models</a><ul>
<li class="chapter" data-level="16.1" data-path="models.html"><a href="models.html#poll-aggregators"><i class="fa fa-check"></i><b>16.1</b> Poll aggregators</a><ul>
<li class="chapter" data-level="16.1.1" data-path="models.html"><a href="models.html#poll-data"><i class="fa fa-check"></i><b>16.1.1</b> Poll data</a></li>
<li class="chapter" data-level="16.1.2" data-path="models.html"><a href="models.html#pollster-bias"><i class="fa fa-check"></i><b>16.1.2</b> Pollster bias</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="models.html"><a href="models.html#data-driven-model"><i class="fa fa-check"></i><b>16.2</b> Data-driven models</a></li>
<li class="chapter" data-level="16.3" data-path="models.html"><a href="models.html#exercises-30"><i class="fa fa-check"></i><b>16.3</b> Exercises</a></li>
<li class="chapter" data-level="16.4" data-path="models.html"><a href="models.html#bayesian-statistics"><i class="fa fa-check"></i><b>16.4</b> Bayesian statistics</a><ul>
<li class="chapter" data-level="16.4.1" data-path="models.html"><a href="models.html#bayes-theorem"><i class="fa fa-check"></i><b>16.4.1</b> Bayes theorem</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="models.html"><a href="models.html#bayes-theorem-simulation"><i class="fa fa-check"></i><b>16.5</b> Bayes theorem simulation</a><ul>
<li class="chapter" data-level="16.5.1" data-path="models.html"><a href="models.html#bayes-in-practice"><i class="fa fa-check"></i><b>16.5.1</b> Bayes in practice</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="models.html"><a href="models.html#hierarchical-models"><i class="fa fa-check"></i><b>16.6</b> Hierarchical models</a></li>
<li class="chapter" data-level="16.7" data-path="models.html"><a href="models.html#exercises-31"><i class="fa fa-check"></i><b>16.7</b> Exercises</a></li>
<li class="chapter" data-level="16.8" data-path="models.html"><a href="models.html#election-forecasting"><i class="fa fa-check"></i><b>16.8</b> Case study: election forecasting</a><ul>
<li class="chapter" data-level="16.8.1" data-path="models.html"><a href="models.html#bayesian-approach"><i class="fa fa-check"></i><b>16.8.1</b> Bayesian approach</a></li>
<li class="chapter" data-level="16.8.2" data-path="models.html"><a href="models.html#the-general-bias"><i class="fa fa-check"></i><b>16.8.2</b> The general bias</a></li>
<li class="chapter" data-level="16.8.3" data-path="models.html"><a href="models.html#mathematical-representations-of-models"><i class="fa fa-check"></i><b>16.8.3</b> Mathematical representations of models</a></li>
<li class="chapter" data-level="16.8.4" data-path="models.html"><a href="models.html#predicting-the-electoral-college"><i class="fa fa-check"></i><b>16.8.4</b> Predicting the electoral college</a></li>
<li class="chapter" data-level="16.8.5" data-path="models.html"><a href="models.html#forecasting"><i class="fa fa-check"></i><b>16.8.5</b> Forecasting</a></li>
</ul></li>
<li class="chapter" data-level="16.9" data-path="models.html"><a href="models.html#exercises-32"><i class="fa fa-check"></i><b>16.9</b> Exercises</a></li>
<li class="chapter" data-level="16.10" data-path="models.html"><a href="models.html#t-dist"><i class="fa fa-check"></i><b>16.10</b> The t-distribution</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>17</b> Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="regression.html"><a href="regression.html#case-study-is-height-hereditary"><i class="fa fa-check"></i><b>17.1</b> Case study: is height hereditary?</a></li>
<li class="chapter" data-level="17.2" data-path="regression.html"><a href="regression.html#corr-coef"><i class="fa fa-check"></i><b>17.2</b> The correlation coefficient</a><ul>
<li class="chapter" data-level="17.2.1" data-path="regression.html"><a href="regression.html#sample-correlation-is-a-random-variable"><i class="fa fa-check"></i><b>17.2.1</b> Sample correlation is a random variable</a></li>
<li class="chapter" data-level="17.2.2" data-path="regression.html"><a href="regression.html#correlation-is-not-always-a-useful-summary"><i class="fa fa-check"></i><b>17.2.2</b> Correlation is not always a useful summary</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="regression.html"><a href="regression.html#conditional-expectation"><i class="fa fa-check"></i><b>17.3</b> Conditional expectations</a></li>
<li class="chapter" data-level="17.4" data-path="regression.html"><a href="regression.html#the-regression-line"><i class="fa fa-check"></i><b>17.4</b> The regression line</a><ul>
<li class="chapter" data-level="17.4.1" data-path="regression.html"><a href="regression.html#regression-improves-precision"><i class="fa fa-check"></i><b>17.4.1</b> Regression improves precision</a></li>
<li class="chapter" data-level="17.4.2" data-path="regression.html"><a href="regression.html#bivariate-normal-distribution-advanced"><i class="fa fa-check"></i><b>17.4.2</b> Bivariate normal distribution (advanced)</a></li>
<li class="chapter" data-level="17.4.3" data-path="regression.html"><a href="regression.html#variance-explained"><i class="fa fa-check"></i><b>17.4.3</b> Variance explained</a></li>
<li class="chapter" data-level="17.4.4" data-path="regression.html"><a href="regression.html#warning-there-are-two-regression-lines"><i class="fa fa-check"></i><b>17.4.4</b> Warning: there are two regression lines</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="regression.html"><a href="regression.html#exercises-33"><i class="fa fa-check"></i><b>17.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>18</b> Linear models</a><ul>
<li class="chapter" data-level="18.1" data-path="linear-models.html"><a href="linear-models.html#case-study-moneyball"><i class="fa fa-check"></i><b>18.1</b> Case study: Moneyball</a><ul>
<li class="chapter" data-level="18.1.1" data-path="linear-models.html"><a href="linear-models.html#sabermetics"><i class="fa fa-check"></i><b>18.1.1</b> Sabermetics</a></li>
<li class="chapter" data-level="18.1.2" data-path="linear-models.html"><a href="linear-models.html#baseball-basics"><i class="fa fa-check"></i><b>18.1.2</b> Baseball basics</a></li>
<li class="chapter" data-level="18.1.3" data-path="linear-models.html"><a href="linear-models.html#no-awards-for-bb"><i class="fa fa-check"></i><b>18.1.3</b> No awards for BB</a></li>
<li class="chapter" data-level="18.1.4" data-path="linear-models.html"><a href="linear-models.html#base-on-balls-or-stolen-bases"><i class="fa fa-check"></i><b>18.1.4</b> Base on balls or stolen bases?</a></li>
<li class="chapter" data-level="18.1.5" data-path="linear-models.html"><a href="linear-models.html#regression-applied-to-baseball-statistics"><i class="fa fa-check"></i><b>18.1.5</b> Regression applied to baseball statistics</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="linear-models.html"><a href="linear-models.html#confounding"><i class="fa fa-check"></i><b>18.2</b> Confounding</a><ul>
<li class="chapter" data-level="18.2.1" data-path="linear-models.html"><a href="linear-models.html#understanding-confounding-through-stratification"><i class="fa fa-check"></i><b>18.2.1</b> Understanding confounding through stratification</a></li>
<li class="chapter" data-level="18.2.2" data-path="linear-models.html"><a href="linear-models.html#multivariate-regression"><i class="fa fa-check"></i><b>18.2.2</b> Multivariate regression</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="linear-models.html"><a href="linear-models.html#lse"><i class="fa fa-check"></i><b>18.3</b> Least squares estimates</a><ul>
<li class="chapter" data-level="18.3.1" data-path="linear-models.html"><a href="linear-models.html#interpreting-linear-models"><i class="fa fa-check"></i><b>18.3.1</b> Interpreting linear models</a></li>
<li class="chapter" data-level="18.3.2" data-path="linear-models.html"><a href="linear-models.html#least-squares-estimates-lse"><i class="fa fa-check"></i><b>18.3.2</b> Least Squares Estimates (LSE)</a></li>
<li class="chapter" data-level="18.3.3" data-path="linear-models.html"><a href="linear-models.html#the-lm-function"><i class="fa fa-check"></i><b>18.3.3</b> The <code>lm</code> function</a></li>
<li class="chapter" data-level="18.3.4" data-path="linear-models.html"><a href="linear-models.html#lse-are-random-variables"><i class="fa fa-check"></i><b>18.3.4</b> LSE are random variables</a></li>
<li class="chapter" data-level="18.3.5" data-path="linear-models.html"><a href="linear-models.html#predicted-values-are-random-variables"><i class="fa fa-check"></i><b>18.3.5</b> Predicted values are random variables</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="linear-models.html"><a href="linear-models.html#exercises-34"><i class="fa fa-check"></i><b>18.4</b> Exercises</a></li>
<li class="chapter" data-level="18.5" data-path="linear-models.html"><a href="linear-models.html#linear-regression-in-the-tidyverse"><i class="fa fa-check"></i><b>18.5</b> Linear regression in the tidyverse</a><ul>
<li class="chapter" data-level="18.5.1" data-path="linear-models.html"><a href="linear-models.html#the-broom-package"><i class="fa fa-check"></i><b>18.5.1</b> The broom package</a></li>
</ul></li>
<li class="chapter" data-level="18.6" data-path="linear-models.html"><a href="linear-models.html#exercises-35"><i class="fa fa-check"></i><b>18.6</b> Exercises</a></li>
<li class="chapter" data-level="18.7" data-path="linear-models.html"><a href="linear-models.html#case-study-moneyball-continued"><i class="fa fa-check"></i><b>18.7</b> Case study: Moneyball (continued)</a><ul>
<li class="chapter" data-level="18.7.1" data-path="linear-models.html"><a href="linear-models.html#adding-salary-and-position-information"><i class="fa fa-check"></i><b>18.7.1</b> Adding salary and position information</a></li>
<li class="chapter" data-level="18.7.2" data-path="linear-models.html"><a href="linear-models.html#picking-nine-players"><i class="fa fa-check"></i><b>18.7.2</b> Picking nine players</a></li>
</ul></li>
<li class="chapter" data-level="18.8" data-path="linear-models.html"><a href="linear-models.html#the-regression-fallacy"><i class="fa fa-check"></i><b>18.8</b> The regression fallacy</a></li>
<li class="chapter" data-level="18.9" data-path="linear-models.html"><a href="linear-models.html#measurement-error-models"><i class="fa fa-check"></i><b>18.9</b> Measurement error models</a></li>
<li class="chapter" data-level="18.10" data-path="linear-models.html"><a href="linear-models.html#exercises-36"><i class="fa fa-check"></i><b>18.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html"><i class="fa fa-check"></i><b>19</b> Association is not causation</a><ul>
<li class="chapter" data-level="19.1" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#spurious-correlation"><i class="fa fa-check"></i><b>19.1</b> Spurious correlation</a></li>
<li class="chapter" data-level="19.2" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#outliers-1"><i class="fa fa-check"></i><b>19.2</b> Outliers</a></li>
<li class="chapter" data-level="19.3" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#reversing-cause-and-effect"><i class="fa fa-check"></i><b>19.3</b> Reversing cause and effect</a></li>
<li class="chapter" data-level="19.4" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#confounders"><i class="fa fa-check"></i><b>19.4</b> Confounders</a><ul>
<li class="chapter" data-level="19.4.1" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#example-uc-berkeley-admissions"><i class="fa fa-check"></i><b>19.4.1</b> Example: UC Berkeley admissions</a></li>
<li class="chapter" data-level="19.4.2" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#confounding-explained-graphically"><i class="fa fa-check"></i><b>19.4.2</b> Confounding explained graphically</a></li>
<li class="chapter" data-level="19.4.3" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#average-after-stratifying"><i class="fa fa-check"></i><b>19.4.3</b> Average after stratifying</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#simpsons-paradox"><i class="fa fa-check"></i><b>19.5</b> Simpson’s paradox</a></li>
<li class="chapter" data-level="19.6" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#exercises-37"><i class="fa fa-check"></i><b>19.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Data Wrangling</b></span></li>
<li class="chapter" data-level="20" data-path="introduction-to-data-wrangling.html"><a href="introduction-to-data-wrangling.html"><i class="fa fa-check"></i><b>20</b> Introduction to data wrangling</a></li>
<li class="chapter" data-level="21" data-path="reshaping-data.html"><a href="reshaping-data.html"><i class="fa fa-check"></i><b>21</b> Reshaping data</a><ul>
<li class="chapter" data-level="21.1" data-path="reshaping-data.html"><a href="reshaping-data.html#gather"><i class="fa fa-check"></i><b>21.1</b> <code>gather</code></a></li>
<li class="chapter" data-level="21.2" data-path="reshaping-data.html"><a href="reshaping-data.html#spread"><i class="fa fa-check"></i><b>21.2</b> <code>spread</code></a></li>
<li class="chapter" data-level="21.3" data-path="reshaping-data.html"><a href="reshaping-data.html#separate"><i class="fa fa-check"></i><b>21.3</b> <code>separate</code></a></li>
<li class="chapter" data-level="21.4" data-path="reshaping-data.html"><a href="reshaping-data.html#unite"><i class="fa fa-check"></i><b>21.4</b> <code>unite</code></a></li>
<li class="chapter" data-level="21.5" data-path="reshaping-data.html"><a href="reshaping-data.html#exercises-38"><i class="fa fa-check"></i><b>21.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="joining-tables.html"><a href="joining-tables.html"><i class="fa fa-check"></i><b>22</b> Joining tables</a><ul>
<li class="chapter" data-level="22.1" data-path="joining-tables.html"><a href="joining-tables.html#joins"><i class="fa fa-check"></i><b>22.1</b> Joins</a><ul>
<li class="chapter" data-level="22.1.1" data-path="joining-tables.html"><a href="joining-tables.html#left-join"><i class="fa fa-check"></i><b>22.1.1</b> Left join</a></li>
<li class="chapter" data-level="22.1.2" data-path="joining-tables.html"><a href="joining-tables.html#right-join"><i class="fa fa-check"></i><b>22.1.2</b> Right join</a></li>
<li class="chapter" data-level="22.1.3" data-path="joining-tables.html"><a href="joining-tables.html#inner-join"><i class="fa fa-check"></i><b>22.1.3</b> Inner join</a></li>
<li class="chapter" data-level="22.1.4" data-path="joining-tables.html"><a href="joining-tables.html#full-join"><i class="fa fa-check"></i><b>22.1.4</b> Full join</a></li>
<li class="chapter" data-level="22.1.5" data-path="joining-tables.html"><a href="joining-tables.html#semi-join"><i class="fa fa-check"></i><b>22.1.5</b> Semi join</a></li>
<li class="chapter" data-level="22.1.6" data-path="joining-tables.html"><a href="joining-tables.html#anti-join"><i class="fa fa-check"></i><b>22.1.6</b> Anti join</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="joining-tables.html"><a href="joining-tables.html#binding"><i class="fa fa-check"></i><b>22.2</b> Binding</a><ul>
<li class="chapter" data-level="22.2.1" data-path="joining-tables.html"><a href="joining-tables.html#binding-columns"><i class="fa fa-check"></i><b>22.2.1</b> Binding columns</a></li>
<li class="chapter" data-level="22.2.2" data-path="joining-tables.html"><a href="joining-tables.html#binding-by-rows"><i class="fa fa-check"></i><b>22.2.2</b> Binding by rows</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="joining-tables.html"><a href="joining-tables.html#set-operators"><i class="fa fa-check"></i><b>22.3</b> Set operators</a><ul>
<li class="chapter" data-level="22.3.1" data-path="joining-tables.html"><a href="joining-tables.html#intersect"><i class="fa fa-check"></i><b>22.3.1</b> Intersect</a></li>
<li class="chapter" data-level="22.3.2" data-path="joining-tables.html"><a href="joining-tables.html#union"><i class="fa fa-check"></i><b>22.3.2</b> Union</a></li>
<li class="chapter" data-level="22.3.3" data-path="joining-tables.html"><a href="joining-tables.html#setdiff"><i class="fa fa-check"></i><b>22.3.3</b> <code>setdiff</code></a></li>
<li class="chapter" data-level="22.3.4" data-path="joining-tables.html"><a href="joining-tables.html#setequal"><i class="fa fa-check"></i><b>22.3.4</b> <code>setequal</code></a></li>
</ul></li>
<li class="chapter" data-level="22.4" data-path="joining-tables.html"><a href="joining-tables.html#exercises-39"><i class="fa fa-check"></i><b>22.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="web-scraping.html"><a href="web-scraping.html"><i class="fa fa-check"></i><b>23</b> Web scraping</a><ul>
<li class="chapter" data-level="23.1" data-path="web-scraping.html"><a href="web-scraping.html#html"><i class="fa fa-check"></i><b>23.1</b> HTML</a></li>
<li class="chapter" data-level="23.2" data-path="web-scraping.html"><a href="web-scraping.html#the-rvest-package"><i class="fa fa-check"></i><b>23.2</b> The rvest package</a></li>
<li class="chapter" data-level="23.3" data-path="web-scraping.html"><a href="web-scraping.html#css-selectors"><i class="fa fa-check"></i><b>23.3</b> CSS selectors</a></li>
<li class="chapter" data-level="23.4" data-path="web-scraping.html"><a href="web-scraping.html#json"><i class="fa fa-check"></i><b>23.4</b> JSON</a></li>
<li class="chapter" data-level="23.5" data-path="web-scraping.html"><a href="web-scraping.html#exercises-40"><i class="fa fa-check"></i><b>23.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="string-processing.html"><a href="string-processing.html"><i class="fa fa-check"></i><b>24</b> String processing</a><ul>
<li class="chapter" data-level="24.1" data-path="string-processing.html"><a href="string-processing.html#stringr"><i class="fa fa-check"></i><b>24.1</b> The stringr package</a></li>
<li class="chapter" data-level="24.2" data-path="string-processing.html"><a href="string-processing.html#case-study-1-us-murders-data"><i class="fa fa-check"></i><b>24.2</b> Case study 1: US murders data</a></li>
<li class="chapter" data-level="24.3" data-path="string-processing.html"><a href="string-processing.html#case-study-2-self-reported-heights"><i class="fa fa-check"></i><b>24.3</b> Case study 2: self-reported heights</a></li>
<li class="chapter" data-level="24.4" data-path="string-processing.html"><a href="string-processing.html#how-to-escape-when-defining-strings"><i class="fa fa-check"></i><b>24.4</b> How to <em>escape</em> when defining strings</a></li>
<li class="chapter" data-level="24.5" data-path="string-processing.html"><a href="string-processing.html#regular-expressions"><i class="fa fa-check"></i><b>24.5</b> Regular expressions</a><ul>
<li class="chapter" data-level="24.5.1" data-path="string-processing.html"><a href="string-processing.html#strings-are-a-regexp"><i class="fa fa-check"></i><b>24.5.1</b> Strings are a regexp</a></li>
<li class="chapter" data-level="24.5.2" data-path="string-processing.html"><a href="string-processing.html#special-characters"><i class="fa fa-check"></i><b>24.5.2</b> Special characters</a></li>
<li class="chapter" data-level="24.5.3" data-path="string-processing.html"><a href="string-processing.html#character-classes"><i class="fa fa-check"></i><b>24.5.3</b> Character classes</a></li>
<li class="chapter" data-level="24.5.4" data-path="string-processing.html"><a href="string-processing.html#anchors"><i class="fa fa-check"></i><b>24.5.4</b> Anchors</a></li>
<li class="chapter" data-level="24.5.5" data-path="string-processing.html"><a href="string-processing.html#quantifiers"><i class="fa fa-check"></i><b>24.5.5</b> Quantifiers</a></li>
<li class="chapter" data-level="24.5.6" data-path="string-processing.html"><a href="string-processing.html#white-space-s"><i class="fa fa-check"></i><b>24.5.6</b> White space <code>\s</code></a></li>
<li class="chapter" data-level="24.5.7" data-path="string-processing.html"><a href="string-processing.html#quantifiers-1"><i class="fa fa-check"></i><b>24.5.7</b> Quantifiers: <code>*</code>, <code>?</code>, <code>+</code></a></li>
<li class="chapter" data-level="24.5.8" data-path="string-processing.html"><a href="string-processing.html#not"><i class="fa fa-check"></i><b>24.5.8</b> Not</a></li>
<li class="chapter" data-level="24.5.9" data-path="string-processing.html"><a href="string-processing.html#groups"><i class="fa fa-check"></i><b>24.5.9</b> Groups</a></li>
</ul></li>
<li class="chapter" data-level="24.6" data-path="string-processing.html"><a href="string-processing.html#search-and-replace-with-regex"><i class="fa fa-check"></i><b>24.6</b> Search and replace with regex</a><ul>
<li class="chapter" data-level="24.6.1" data-path="string-processing.html"><a href="string-processing.html#search-and-replace-using-groups"><i class="fa fa-check"></i><b>24.6.1</b> Search and replace using groups</a></li>
</ul></li>
<li class="chapter" data-level="24.7" data-path="string-processing.html"><a href="string-processing.html#testing-and-improving"><i class="fa fa-check"></i><b>24.7</b> Testing and improving</a></li>
<li class="chapter" data-level="24.8" data-path="string-processing.html"><a href="string-processing.html#trimming"><i class="fa fa-check"></i><b>24.8</b> Trimming</a></li>
<li class="chapter" data-level="24.9" data-path="string-processing.html"><a href="string-processing.html#changing-lettercase"><i class="fa fa-check"></i><b>24.9</b> Changing lettercase</a></li>
<li class="chapter" data-level="24.10" data-path="string-processing.html"><a href="string-processing.html#case-study-2-self-reported-heights-continued"><i class="fa fa-check"></i><b>24.10</b> Case study 2: self-reported heights (continued)</a><ul>
<li class="chapter" data-level="24.10.1" data-path="string-processing.html"><a href="string-processing.html#the-extract-function"><i class="fa fa-check"></i><b>24.10.1</b> The <code>extract</code> function</a></li>
<li class="chapter" data-level="24.10.2" data-path="string-processing.html"><a href="string-processing.html#putting-it-all-together-1"><i class="fa fa-check"></i><b>24.10.2</b> Putting it all together</a></li>
</ul></li>
<li class="chapter" data-level="24.11" data-path="string-processing.html"><a href="string-processing.html#string-splitting"><i class="fa fa-check"></i><b>24.11</b> String splitting</a></li>
<li class="chapter" data-level="24.12" data-path="string-processing.html"><a href="string-processing.html#case-study-3-extracting-tables-from-a-pdf"><i class="fa fa-check"></i><b>24.12</b> Case study 3: extracting tables from a PDF</a></li>
<li class="chapter" data-level="24.13" data-path="string-processing.html"><a href="string-processing.html#recode"><i class="fa fa-check"></i><b>24.13</b> Recoding</a></li>
<li class="chapter" data-level="24.14" data-path="string-processing.html"><a href="string-processing.html#exercises-41"><i class="fa fa-check"></i><b>24.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html"><i class="fa fa-check"></i><b>25</b> Parsing dates and times</a><ul>
<li class="chapter" data-level="25.1" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html#the-date-data-type"><i class="fa fa-check"></i><b>25.1</b> The date data type</a></li>
<li class="chapter" data-level="25.2" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html#lubridate"><i class="fa fa-check"></i><b>25.2</b> The lubridate package</a></li>
<li class="chapter" data-level="25.3" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html#exercises-42"><i class="fa fa-check"></i><b>25.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="text-mining.html"><a href="text-mining.html"><i class="fa fa-check"></i><b>26</b> Text mining</a><ul>
<li class="chapter" data-level="26.1" data-path="text-mining.html"><a href="text-mining.html#case-study-trump-tweets"><i class="fa fa-check"></i><b>26.1</b> Case study: Trump tweets</a></li>
<li class="chapter" data-level="26.2" data-path="text-mining.html"><a href="text-mining.html#text-as-data"><i class="fa fa-check"></i><b>26.2</b> Text as data</a></li>
<li class="chapter" data-level="26.3" data-path="text-mining.html"><a href="text-mining.html#sentiment-analysis"><i class="fa fa-check"></i><b>26.3</b> Sentiment analysis</a></li>
<li class="chapter" data-level="26.4" data-path="text-mining.html"><a href="text-mining.html#exercises-43"><i class="fa fa-check"></i><b>26.4</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Machine Learning</b></span></li>
<li class="chapter" data-level="27" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html"><i class="fa fa-check"></i><b>27</b> Introduction to machine learning</a><ul>
<li class="chapter" data-level="27.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#notation-1"><i class="fa fa-check"></i><b>27.1</b> Notation</a></li>
<li class="chapter" data-level="27.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#an-example"><i class="fa fa-check"></i><b>27.2</b> An example</a></li>
<li class="chapter" data-level="27.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#exercises-44"><i class="fa fa-check"></i><b>27.3</b> Exercises</a></li>
<li class="chapter" data-level="27.4" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#evaluation-metrics"><i class="fa fa-check"></i><b>27.4</b> Evaluation metrics</a><ul>
<li class="chapter" data-level="27.4.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#training-and-test-sets"><i class="fa fa-check"></i><b>27.4.1</b> Training and test sets</a></li>
<li class="chapter" data-level="27.4.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#overall-accuracy"><i class="fa fa-check"></i><b>27.4.2</b> Overall accuracy</a></li>
<li class="chapter" data-level="27.4.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#the-confusion-matrix"><i class="fa fa-check"></i><b>27.4.3</b> The confusion matrix</a></li>
<li class="chapter" data-level="27.4.4" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#sensitivity-and-specificity"><i class="fa fa-check"></i><b>27.4.4</b> Sensitivity and specificity</a></li>
<li class="chapter" data-level="27.4.5" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#balanced-accuracy-and-f_1-score"><i class="fa fa-check"></i><b>27.4.5</b> Balanced accuracy and <span class="math inline">\(F_1\)</span> score</a></li>
<li class="chapter" data-level="27.4.6" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#prevalence-matters-in-practice"><i class="fa fa-check"></i><b>27.4.6</b> Prevalence matters in practice</a></li>
<li class="chapter" data-level="27.4.7" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#roc-and-precision-recall-curves"><i class="fa fa-check"></i><b>27.4.7</b> ROC and precision-recall curves</a></li>
<li class="chapter" data-level="27.4.8" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#loss-function"><i class="fa fa-check"></i><b>27.4.8</b> The loss function</a></li>
</ul></li>
<li class="chapter" data-level="27.5" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#exercises-45"><i class="fa fa-check"></i><b>27.5</b> Exercises</a></li>
<li class="chapter" data-level="27.6" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-probabilities-and-expectations"><i class="fa fa-check"></i><b>27.6</b> Conditional probabilities and expectations</a><ul>
<li class="chapter" data-level="27.6.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-probabilities-1"><i class="fa fa-check"></i><b>27.6.1</b> Conditional probabilities</a></li>
<li class="chapter" data-level="27.6.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-expectations"><i class="fa fa-check"></i><b>27.6.2</b> Conditional expectations</a></li>
<li class="chapter" data-level="27.6.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-expectation-minimizes-squared-loss-function"><i class="fa fa-check"></i><b>27.6.3</b> Conditional expectation minimizes squared loss function</a></li>
</ul></li>
<li class="chapter" data-level="27.7" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#exercises-46"><i class="fa fa-check"></i><b>27.7</b> Exercises</a></li>
<li class="chapter" data-level="27.8" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#two-or-seven"><i class="fa fa-check"></i><b>27.8</b> Case study: is it a 2 or a 7?</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>28</b> Smoothing</a><ul>
<li class="chapter" data-level="28.1" data-path="smoothing.html"><a href="smoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>28.1</b> Bin smoothing</a></li>
<li class="chapter" data-level="28.2" data-path="smoothing.html"><a href="smoothing.html#kernels"><i class="fa fa-check"></i><b>28.2</b> Kernels</a></li>
<li class="chapter" data-level="28.3" data-path="smoothing.html"><a href="smoothing.html#local-weighted-regression-loess"><i class="fa fa-check"></i><b>28.3</b> Local weighted regression (loess)</a><ul>
<li class="chapter" data-level="28.3.1" data-path="smoothing.html"><a href="smoothing.html#fitting-parabolas"><i class="fa fa-check"></i><b>28.3.1</b> Fitting parabolas</a></li>
<li class="chapter" data-level="28.3.2" data-path="smoothing.html"><a href="smoothing.html#beware-of-default-smoothing-parameters"><i class="fa fa-check"></i><b>28.3.2</b> Beware of default smoothing parameters</a></li>
</ul></li>
<li class="chapter" data-level="28.4" data-path="smoothing.html"><a href="smoothing.html#smoothing-ml-connection"><i class="fa fa-check"></i><b>28.4</b> Connecting smoothing to machine learning</a></li>
<li class="chapter" data-level="28.5" data-path="smoothing.html"><a href="smoothing.html#exercises-47"><i class="fa fa-check"></i><b>28.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>29</b> Cross validation</a><ul>
<li class="chapter" data-level="29.1" data-path="cross-validation.html"><a href="cross-validation.html#knn-cv-intro"><i class="fa fa-check"></i><b>29.1</b> Motivation with k-nearest neighbors</a><ul>
<li class="chapter" data-level="29.1.1" data-path="cross-validation.html"><a href="cross-validation.html#over-training"><i class="fa fa-check"></i><b>29.1.1</b> Over-training</a></li>
<li class="chapter" data-level="29.1.2" data-path="cross-validation.html"><a href="cross-validation.html#over-smoothing"><i class="fa fa-check"></i><b>29.1.2</b> Over-smoothing</a></li>
<li class="chapter" data-level="29.1.3" data-path="cross-validation.html"><a href="cross-validation.html#picking-the-k-in-knn"><i class="fa fa-check"></i><b>29.1.3</b> Picking the <span class="math inline">\(k\)</span> in kNN</a></li>
</ul></li>
<li class="chapter" data-level="29.2" data-path="cross-validation.html"><a href="cross-validation.html#mathematical-description-of-cross-validation"><i class="fa fa-check"></i><b>29.2</b> Mathematical description of cross validation</a></li>
<li class="chapter" data-level="29.3" data-path="cross-validation.html"><a href="cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>29.3</b> K-fold cross validation</a></li>
<li class="chapter" data-level="29.4" data-path="cross-validation.html"><a href="cross-validation.html#exercises-48"><i class="fa fa-check"></i><b>29.4</b> Exercises</a></li>
<li class="chapter" data-level="29.5" data-path="cross-validation.html"><a href="cross-validation.html#bootstrap"><i class="fa fa-check"></i><b>29.5</b> Bootstrap</a></li>
<li class="chapter" data-level="29.6" data-path="cross-validation.html"><a href="cross-validation.html#exercises-49"><i class="fa fa-check"></i><b>29.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>30</b> The caret package</a><ul>
<li class="chapter" data-level="30.1" data-path="caret.html"><a href="caret.html#the-caret-train-functon"><i class="fa fa-check"></i><b>30.1</b> The caret <code>train</code> functon</a></li>
<li class="chapter" data-level="30.2" data-path="caret.html"><a href="caret.html#caret-cv"><i class="fa fa-check"></i><b>30.2</b> Cross validation</a></li>
<li class="chapter" data-level="30.3" data-path="caret.html"><a href="caret.html#example-fitting-with-loess"><i class="fa fa-check"></i><b>30.3</b> Example: fitting with loess</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html"><i class="fa fa-check"></i><b>31</b> Examples of algorithms</a><ul>
<li class="chapter" data-level="31.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#linear-regression"><i class="fa fa-check"></i><b>31.1</b> Linear regression</a><ul>
<li class="chapter" data-level="31.1.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#the-predict-function"><i class="fa fa-check"></i><b>31.1.1</b> The <code>predict</code> function</a></li>
</ul></li>
<li class="chapter" data-level="31.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-50"><i class="fa fa-check"></i><b>31.2</b> Exercises</a></li>
<li class="chapter" data-level="31.3" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#logistic-regression"><i class="fa fa-check"></i><b>31.3</b> Logistic regression</a><ul>
<li class="chapter" data-level="31.3.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#generalized-linear-models"><i class="fa fa-check"></i><b>31.3.1</b> Generalized linear models</a></li>
<li class="chapter" data-level="31.3.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#logistic-regression-with-more-than-one-predictor"><i class="fa fa-check"></i><b>31.3.2</b> Logistic regression with more than one predictor</a></li>
</ul></li>
<li class="chapter" data-level="31.4" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-51"><i class="fa fa-check"></i><b>31.4</b> Exercises</a></li>
<li class="chapter" data-level="31.5" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>31.5</b> k-nearest neighbors</a></li>
<li class="chapter" data-level="31.6" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-52"><i class="fa fa-check"></i><b>31.6</b> Exercises</a></li>
<li class="chapter" data-level="31.7" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#generative-models"><i class="fa fa-check"></i><b>31.7</b> Generative models</a><ul>
<li class="chapter" data-level="31.7.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#naive-bayes"><i class="fa fa-check"></i><b>31.7.1</b> Naive Bayes</a></li>
<li class="chapter" data-level="31.7.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#controlling-prevalence"><i class="fa fa-check"></i><b>31.7.2</b> Controlling prevalence</a></li>
<li class="chapter" data-level="31.7.3" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>31.7.3</b> Quadratic discriminant analysis</a></li>
<li class="chapter" data-level="31.7.4" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>31.7.4</b> Linear discriminant analysis</a></li>
<li class="chapter" data-level="31.7.5" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#connection-to-distance"><i class="fa fa-check"></i><b>31.7.5</b> Connection to distance</a></li>
</ul></li>
<li class="chapter" data-level="31.8" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#case-study-more-than-three-classes"><i class="fa fa-check"></i><b>31.8</b> Case study: more than three classes</a></li>
<li class="chapter" data-level="31.9" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-53"><i class="fa fa-check"></i><b>31.9</b> Exercises</a></li>
<li class="chapter" data-level="31.10" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#classification-and-regression-trees-cart"><i class="fa fa-check"></i><b>31.10</b> Classification and regression trees (CART)</a><ul>
<li class="chapter" data-level="31.10.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>31.10.1</b> The curse of dimensionality</a></li>
<li class="chapter" data-level="31.10.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#cart-motivation"><i class="fa fa-check"></i><b>31.10.2</b> CART motivation</a></li>
<li class="chapter" data-level="31.10.3" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#regression-trees"><i class="fa fa-check"></i><b>31.10.3</b> Regression trees</a></li>
<li class="chapter" data-level="31.10.4" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#classification-decision-trees"><i class="fa fa-check"></i><b>31.10.4</b> Classification (decision) trees</a></li>
</ul></li>
<li class="chapter" data-level="31.11" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#random-forests"><i class="fa fa-check"></i><b>31.11</b> Random forests</a></li>
<li class="chapter" data-level="31.12" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-54"><i class="fa fa-check"></i><b>31.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html"><i class="fa fa-check"></i><b>32</b> Machine learning in practice</a><ul>
<li class="chapter" data-level="32.1" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#preprocessing"><i class="fa fa-check"></i><b>32.1</b> Preprocessing</a></li>
<li class="chapter" data-level="32.2" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#k-nearest-neighbor-and-random-forest"><i class="fa fa-check"></i><b>32.2</b> k-nearest neighbor and random forest</a></li>
<li class="chapter" data-level="32.3" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#variable-importance"><i class="fa fa-check"></i><b>32.3</b> Variable importance</a></li>
<li class="chapter" data-level="32.4" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#visual-assessments"><i class="fa fa-check"></i><b>32.4</b> Visual assessments</a></li>
<li class="chapter" data-level="32.5" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#ensembles"><i class="fa fa-check"></i><b>32.5</b> Ensembles</a></li>
<li class="chapter" data-level="32.6" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#exercises-55"><i class="fa fa-check"></i><b>32.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="33" data-path="large-datasets.html"><a href="large-datasets.html"><i class="fa fa-check"></i><b>33</b> Large datasets</a><ul>
<li class="chapter" data-level="33.1" data-path="large-datasets.html"><a href="large-datasets.html#matrix-algebra"><i class="fa fa-check"></i><b>33.1</b> Matrix algebra</a><ul>
<li class="chapter" data-level="33.1.1" data-path="large-datasets.html"><a href="large-datasets.html#notation-2"><i class="fa fa-check"></i><b>33.1.1</b> Notation</a></li>
<li class="chapter" data-level="33.1.2" data-path="large-datasets.html"><a href="large-datasets.html#converting-a-vector-to-a-matrix"><i class="fa fa-check"></i><b>33.1.2</b> Converting a vector to a matrix</a></li>
<li class="chapter" data-level="33.1.3" data-path="large-datasets.html"><a href="large-datasets.html#row-and-column-summaries"><i class="fa fa-check"></i><b>33.1.3</b> Row and column summaries</a></li>
<li class="chapter" data-level="33.1.4" data-path="large-datasets.html"><a href="large-datasets.html#apply"><i class="fa fa-check"></i><b>33.1.4</b> <code>apply</code></a></li>
<li class="chapter" data-level="33.1.5" data-path="large-datasets.html"><a href="large-datasets.html#filtering-columns-based-on-summaries"><i class="fa fa-check"></i><b>33.1.5</b> Filtering columns based on summaries</a></li>
<li class="chapter" data-level="33.1.6" data-path="large-datasets.html"><a href="large-datasets.html#indexing-with-matrices"><i class="fa fa-check"></i><b>33.1.6</b> Indexing with matrices</a></li>
<li class="chapter" data-level="33.1.7" data-path="large-datasets.html"><a href="large-datasets.html#binarizing-the-data"><i class="fa fa-check"></i><b>33.1.7</b> Binarizing the data</a></li>
<li class="chapter" data-level="33.1.8" data-path="large-datasets.html"><a href="large-datasets.html#vectorization-for-matrices"><i class="fa fa-check"></i><b>33.1.8</b> Vectorization for matrices</a></li>
<li class="chapter" data-level="33.1.9" data-path="large-datasets.html"><a href="large-datasets.html#matrix-algebra-operations"><i class="fa fa-check"></i><b>33.1.9</b> Matrix algebra operations</a></li>
</ul></li>
<li class="chapter" data-level="33.2" data-path="large-datasets.html"><a href="large-datasets.html#exercises-56"><i class="fa fa-check"></i><b>33.2</b> Exercises</a></li>
<li class="chapter" data-level="33.3" data-path="large-datasets.html"><a href="large-datasets.html#distance"><i class="fa fa-check"></i><b>33.3</b> Distance</a><ul>
<li class="chapter" data-level="33.3.1" data-path="large-datasets.html"><a href="large-datasets.html#euclidean-distance"><i class="fa fa-check"></i><b>33.3.1</b> Euclidean distance</a></li>
<li class="chapter" data-level="33.3.2" data-path="large-datasets.html"><a href="large-datasets.html#distance-in-higher-dimensions"><i class="fa fa-check"></i><b>33.3.2</b> Distance in higher dimensions</a></li>
<li class="chapter" data-level="33.3.3" data-path="large-datasets.html"><a href="large-datasets.html#euclidean-distance-example"><i class="fa fa-check"></i><b>33.3.3</b> Euclidean distance example</a></li>
<li class="chapter" data-level="33.3.4" data-path="large-datasets.html"><a href="large-datasets.html#predictor-space"><i class="fa fa-check"></i><b>33.3.4</b> Predictor space</a></li>
<li class="chapter" data-level="33.3.5" data-path="large-datasets.html"><a href="large-datasets.html#distance-between-predictors"><i class="fa fa-check"></i><b>33.3.5</b> Distance between predictors</a></li>
</ul></li>
<li class="chapter" data-level="33.4" data-path="large-datasets.html"><a href="large-datasets.html#exercises-57"><i class="fa fa-check"></i><b>33.4</b> Exercises</a></li>
<li class="chapter" data-level="33.5" data-path="large-datasets.html"><a href="large-datasets.html#dimension-reduction"><i class="fa fa-check"></i><b>33.5</b> Dimension reduction</a><ul>
<li class="chapter" data-level="33.5.1" data-path="large-datasets.html"><a href="large-datasets.html#preserving-distance"><i class="fa fa-check"></i><b>33.5.1</b> Preserving distance</a></li>
<li class="chapter" data-level="33.5.2" data-path="large-datasets.html"><a href="large-datasets.html#linear-transformations-advanced"><i class="fa fa-check"></i><b>33.5.2</b> Linear transformations (advanced)</a></li>
<li class="chapter" data-level="33.5.3" data-path="large-datasets.html"><a href="large-datasets.html#orthogonal-transformations-advanced"><i class="fa fa-check"></i><b>33.5.3</b> Orthogonal transformations (advanced)</a></li>
<li class="chapter" data-level="33.5.4" data-path="large-datasets.html"><a href="large-datasets.html#principal-component-analysis"><i class="fa fa-check"></i><b>33.5.4</b> Principal component analysis</a></li>
<li class="chapter" data-level="33.5.5" data-path="large-datasets.html"><a href="large-datasets.html#iris-example"><i class="fa fa-check"></i><b>33.5.5</b> Iris example</a></li>
<li class="chapter" data-level="33.5.6" data-path="large-datasets.html"><a href="large-datasets.html#mnist-example"><i class="fa fa-check"></i><b>33.5.6</b> MNIST example</a></li>
</ul></li>
<li class="chapter" data-level="33.6" data-path="large-datasets.html"><a href="large-datasets.html#exercises-58"><i class="fa fa-check"></i><b>33.6</b> Exercises</a></li>
<li class="chapter" data-level="33.7" data-path="large-datasets.html"><a href="large-datasets.html#recommendation-systems"><i class="fa fa-check"></i><b>33.7</b> Recommendation systems</a><ul>
<li class="chapter" data-level="33.7.1" data-path="large-datasets.html"><a href="large-datasets.html#movielens-data"><i class="fa fa-check"></i><b>33.7.1</b> Movielens data</a></li>
<li class="chapter" data-level="33.7.2" data-path="large-datasets.html"><a href="large-datasets.html#recommendation-systems-as-a-machine-learning-challenge"><i class="fa fa-check"></i><b>33.7.2</b> Recommendation systems as a machine learning challenge</a></li>
<li class="chapter" data-level="33.7.3" data-path="large-datasets.html"><a href="large-datasets.html#netflix-loss-function"><i class="fa fa-check"></i><b>33.7.3</b> Loss function</a></li>
<li class="chapter" data-level="33.7.4" data-path="large-datasets.html"><a href="large-datasets.html#a-first-model"><i class="fa fa-check"></i><b>33.7.4</b> A first model</a></li>
<li class="chapter" data-level="33.7.5" data-path="large-datasets.html"><a href="large-datasets.html#modeling-movie-effects"><i class="fa fa-check"></i><b>33.7.5</b> Modeling movie effects</a></li>
<li class="chapter" data-level="33.7.6" data-path="large-datasets.html"><a href="large-datasets.html#user-effects"><i class="fa fa-check"></i><b>33.7.6</b> User effects</a></li>
</ul></li>
<li class="chapter" data-level="33.8" data-path="large-datasets.html"><a href="large-datasets.html#exercises-59"><i class="fa fa-check"></i><b>33.8</b> Exercises</a></li>
<li class="chapter" data-level="33.9" data-path="large-datasets.html"><a href="large-datasets.html#regularization"><i class="fa fa-check"></i><b>33.9</b> Regularization</a><ul>
<li class="chapter" data-level="33.9.1" data-path="large-datasets.html"><a href="large-datasets.html#motivation"><i class="fa fa-check"></i><b>33.9.1</b> Motivation</a></li>
<li class="chapter" data-level="33.9.2" data-path="large-datasets.html"><a href="large-datasets.html#penalized-least-squares"><i class="fa fa-check"></i><b>33.9.2</b> Penalized least squares</a></li>
<li class="chapter" data-level="33.9.3" data-path="large-datasets.html"><a href="large-datasets.html#choosing-the-penalty-terms"><i class="fa fa-check"></i><b>33.9.3</b> Choosing the penalty terms</a></li>
</ul></li>
<li class="chapter" data-level="33.10" data-path="large-datasets.html"><a href="large-datasets.html#exercises-60"><i class="fa fa-check"></i><b>33.10</b> Exercises</a></li>
<li class="chapter" data-level="33.11" data-path="large-datasets.html"><a href="large-datasets.html#matrix-factorization"><i class="fa fa-check"></i><b>33.11</b> Matrix factorization</a><ul>
<li class="chapter" data-level="33.11.1" data-path="large-datasets.html"><a href="large-datasets.html#factors-analysis"><i class="fa fa-check"></i><b>33.11.1</b> Factors analysis</a></li>
<li class="chapter" data-level="33.11.2" data-path="large-datasets.html"><a href="large-datasets.html#connection-to-svd-and-pca"><i class="fa fa-check"></i><b>33.11.2</b> Connection to SVD and PCA</a></li>
</ul></li>
<li class="chapter" data-level="33.12" data-path="large-datasets.html"><a href="large-datasets.html#exercises-61"><i class="fa fa-check"></i><b>33.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="34" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>34</b> Clustering</a><ul>
<li class="chapter" data-level="34.1" data-path="clustering.html"><a href="clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>34.1</b> Hierarchical clustering</a></li>
<li class="chapter" data-level="34.2" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>34.2</b> k-means</a></li>
<li class="chapter" data-level="34.3" data-path="clustering.html"><a href="clustering.html#heatmaps"><i class="fa fa-check"></i><b>34.3</b> Heatmaps</a></li>
<li class="chapter" data-level="34.4" data-path="clustering.html"><a href="clustering.html#filtering-features"><i class="fa fa-check"></i><b>34.4</b> Filtering features</a></li>
<li class="chapter" data-level="34.5" data-path="clustering.html"><a href="clustering.html#exercises-62"><i class="fa fa-check"></i><b>34.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Productivity Tools</b></span></li>
<li class="chapter" data-level="35" data-path="introduction-to-productivity-tools.html"><a href="introduction-to-productivity-tools.html"><i class="fa fa-check"></i><b>35</b> Introduction to productivity tools</a></li>
<li class="chapter" data-level="36" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html"><i class="fa fa-check"></i><b>36</b> Installing R and RStudio</a><ul>
<li class="chapter" data-level="36.1" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html#installing-r"><i class="fa fa-check"></i><b>36.1</b> Installing R</a></li>
<li class="chapter" data-level="36.2" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html#installing-rstudio"><i class="fa fa-check"></i><b>36.2</b> Installing RStudio</a></li>
</ul></li>
<li class="chapter" data-level="37" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html"><i class="fa fa-check"></i><b>37</b> Accessing the terminal and installing Git</a><ul>
<li class="chapter" data-level="37.1" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html#terminal-on-mac"><i class="fa fa-check"></i><b>37.1</b> Accessing the terminal on a Mac</a></li>
<li class="chapter" data-level="37.2" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html#installing-git-on-the-mac"><i class="fa fa-check"></i><b>37.2</b> Installing Git on the Mac</a></li>
<li class="chapter" data-level="37.3" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html#installing-git-and-git-bash-on-windows"><i class="fa fa-check"></i><b>37.3</b> Installing Git and Git Bash on Windows</a></li>
<li class="chapter" data-level="37.4" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html#terminal-on-windows"><i class="fa fa-check"></i><b>37.4</b> Accessing the terminal on Windows</a></li>
</ul></li>
<li class="chapter" data-level="38" data-path="unix.html"><a href="unix.html"><i class="fa fa-check"></i><b>38</b> Organizing with Unix</a><ul>
<li class="chapter" data-level="38.1" data-path="unix.html"><a href="unix.html#naming-convention"><i class="fa fa-check"></i><b>38.1</b> Naming convention</a></li>
<li class="chapter" data-level="38.2" data-path="unix.html"><a href="unix.html#the-terminal"><i class="fa fa-check"></i><b>38.2</b> The terminal</a></li>
<li class="chapter" data-level="38.3" data-path="unix.html"><a href="unix.html#filesystem"><i class="fa fa-check"></i><b>38.3</b> The filesystem</a><ul>
<li class="chapter" data-level="38.3.1" data-path="unix.html"><a href="unix.html#directories-and-subdirectories"><i class="fa fa-check"></i><b>38.3.1</b> Directories and subdirectories</a></li>
<li class="chapter" data-level="38.3.2" data-path="unix.html"><a href="unix.html#the-home-directory"><i class="fa fa-check"></i><b>38.3.2</b> The home directory</a></li>
<li class="chapter" data-level="38.3.3" data-path="unix.html"><a href="unix.html#working-directory"><i class="fa fa-check"></i><b>38.3.3</b> Working directory</a></li>
<li class="chapter" data-level="38.3.4" data-path="unix.html"><a href="unix.html#paths"><i class="fa fa-check"></i><b>38.3.4</b> Paths</a></li>
</ul></li>
<li class="chapter" data-level="38.4" data-path="unix.html"><a href="unix.html#unix-commands"><i class="fa fa-check"></i><b>38.4</b> Unix commands</a><ul>
<li class="chapter" data-level="38.4.1" data-path="unix.html"><a href="unix.html#ls-listing-directory-content"><i class="fa fa-check"></i><b>38.4.1</b> <code>ls</code>: Listing directory content</a></li>
<li class="chapter" data-level="38.4.2" data-path="unix.html"><a href="unix.html#mkdir-and-rmdir-make-and-remove-a-directory"><i class="fa fa-check"></i><b>38.4.2</b> <code>mkdir</code> and <code>rmdir</code>: make and remove a directory</a></li>
<li class="chapter" data-level="38.4.3" data-path="unix.html"><a href="unix.html#cd-navigating-the-filesystem-by-changing-directories"><i class="fa fa-check"></i><b>38.4.3</b> <code>cd</code>: navigating the filesystem by changing directories</a></li>
</ul></li>
<li class="chapter" data-level="38.5" data-path="unix.html"><a href="unix.html#some-examples"><i class="fa fa-check"></i><b>38.5</b> Some examples</a></li>
<li class="chapter" data-level="38.6" data-path="unix.html"><a href="unix.html#more-unix-commands"><i class="fa fa-check"></i><b>38.6</b> More Unix commands</a><ul>
<li class="chapter" data-level="38.6.1" data-path="unix.html"><a href="unix.html#mv-moving-files"><i class="fa fa-check"></i><b>38.6.1</b> <code>mv</code>: moving files</a></li>
<li class="chapter" data-level="38.6.2" data-path="unix.html"><a href="unix.html#cp-copying-files"><i class="fa fa-check"></i><b>38.6.2</b> <code>cp</code>: copying files</a></li>
<li class="chapter" data-level="38.6.3" data-path="unix.html"><a href="unix.html#rm-removing-files"><i class="fa fa-check"></i><b>38.6.3</b> <code>rm</code>: removing files</a></li>
<li class="chapter" data-level="38.6.4" data-path="unix.html"><a href="unix.html#less-looking-at-a-file"><i class="fa fa-check"></i><b>38.6.4</b> <code>less</code>: looking at a file</a></li>
</ul></li>
<li class="chapter" data-level="38.7" data-path="unix.html"><a href="unix.html#prep-project"><i class="fa fa-check"></i><b>38.7</b> Preparing for a data science project</a></li>
<li class="chapter" data-level="38.8" data-path="unix.html"><a href="unix.html#advanced-unix"><i class="fa fa-check"></i><b>38.8</b> Advanced Unix</a><ul>
<li class="chapter" data-level="38.8.1" data-path="unix.html"><a href="unix.html#arguments"><i class="fa fa-check"></i><b>38.8.1</b> Arguments</a></li>
<li class="chapter" data-level="38.8.2" data-path="unix.html"><a href="unix.html#getting-help"><i class="fa fa-check"></i><b>38.8.2</b> Getting help</a></li>
<li class="chapter" data-level="38.8.3" data-path="unix.html"><a href="unix.html#pipes"><i class="fa fa-check"></i><b>38.8.3</b> Pipes</a></li>
<li class="chapter" data-level="38.8.4" data-path="unix.html"><a href="unix.html#wild-cards"><i class="fa fa-check"></i><b>38.8.4</b> Wild cards</a></li>
<li class="chapter" data-level="38.8.5" data-path="unix.html"><a href="unix.html#environment-variables"><i class="fa fa-check"></i><b>38.8.5</b> Environment variables</a></li>
<li class="chapter" data-level="38.8.6" data-path="unix.html"><a href="unix.html#shells"><i class="fa fa-check"></i><b>38.8.6</b> Shells</a></li>
<li class="chapter" data-level="38.8.7" data-path="unix.html"><a href="unix.html#executables"><i class="fa fa-check"></i><b>38.8.7</b> Executables</a></li>
<li class="chapter" data-level="38.8.8" data-path="unix.html"><a href="unix.html#permissions-and-file-types"><i class="fa fa-check"></i><b>38.8.8</b> Permissions and file types</a></li>
<li class="chapter" data-level="38.8.9" data-path="unix.html"><a href="unix.html#commands-you-should-learn"><i class="fa fa-check"></i><b>38.8.9</b> Commands you should learn</a></li>
<li class="chapter" data-level="38.8.10" data-path="unix.html"><a href="unix.html#file-manipulation-in-r"><i class="fa fa-check"></i><b>38.8.10</b> File manipulation in R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="39" data-path="git.html"><a href="git.html"><i class="fa fa-check"></i><b>39</b> Git and GitHub</a><ul>
<li class="chapter" data-level="39.1" data-path="git.html"><a href="git.html#why-use-git-and-github"><i class="fa fa-check"></i><b>39.1</b> Why use Git and GitHub?</a></li>
<li class="chapter" data-level="39.2" data-path="git.html"><a href="git.html#github-accounts"><i class="fa fa-check"></i><b>39.2</b> GitHub accounts</a></li>
<li class="chapter" data-level="39.3" data-path="git.html"><a href="git.html#github-repos"><i class="fa fa-check"></i><b>39.3</b> GitHub repositories</a></li>
<li class="chapter" data-level="39.4" data-path="git.html"><a href="git.html#git-overview"><i class="fa fa-check"></i><b>39.4</b> Overview of Git</a><ul>
<li class="chapter" data-level="39.4.1" data-path="git.html"><a href="git.html#clone"><i class="fa fa-check"></i><b>39.4.1</b> Clone</a></li>
</ul></li>
<li class="chapter" data-level="39.5" data-path="git.html"><a href="git.html#init"><i class="fa fa-check"></i><b>39.5</b> Initializing a Git directory</a></li>
<li class="chapter" data-level="39.6" data-path="git.html"><a href="git.html#rstudio-git"><i class="fa fa-check"></i><b>39.6</b> Using Git and GitHub in RStudio</a></li>
</ul></li>
<li class="chapter" data-level="40" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html"><i class="fa fa-check"></i><b>40</b> Reproducible projects with RStudio and R markdown</a><ul>
<li class="chapter" data-level="40.1" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#rstudio-projects"><i class="fa fa-check"></i><b>40.1</b> RStudio projects</a></li>
<li class="chapter" data-level="40.2" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#r-markdown"><i class="fa fa-check"></i><b>40.2</b> R markdown</a><ul>
<li class="chapter" data-level="40.2.1" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#the-header"><i class="fa fa-check"></i><b>40.2.1</b> The header</a></li>
<li class="chapter" data-level="40.2.2" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#r-code-chunks"><i class="fa fa-check"></i><b>40.2.2</b> R code chunks</a></li>
<li class="chapter" data-level="40.2.3" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#global-options"><i class="fa fa-check"></i><b>40.2.3</b> Global options</a></li>
<li class="chapter" data-level="40.2.4" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#knitr"><i class="fa fa-check"></i><b>40.2.4</b> knitR</a></li>
<li class="chapter" data-level="40.2.5" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#more-on-r-markdown"><i class="fa fa-check"></i><b>40.2.5</b> More on R markdown</a></li>
</ul></li>
<li class="chapter" data-level="40.3" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#organizing"><i class="fa fa-check"></i><b>40.3</b> Organizing a data science project</a><ul>
<li class="chapter" data-level="40.3.1" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#create-directories-in-unix"><i class="fa fa-check"></i><b>40.3.1</b> Create directories in Unix</a></li>
<li class="chapter" data-level="40.3.2" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#create-an-rstudio-project"><i class="fa fa-check"></i><b>40.3.2</b> Create an RStudio project</a></li>
<li class="chapter" data-level="40.3.3" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#edit-some-r-scripts"><i class="fa fa-check"></i><b>40.3.3</b> Edit some R scripts</a></li>
<li class="chapter" data-level="40.3.4" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#create-some-more-directories-using-unix"><i class="fa fa-check"></i><b>40.3.4</b> Create some more directories using Unix</a></li>
<li class="chapter" data-level="40.3.5" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#add-a-readme-file"><i class="fa fa-check"></i><b>40.3.5</b> Add a README file</a></li>
<li class="chapter" data-level="40.3.6" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#initializing-a-git-directory"><i class="fa fa-check"></i><b>40.3.6</b> Initializing a Git directory</a></li>
<li class="chapter" data-level="40.3.7" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#add-commit-and-push-files-using-rstudio"><i class="fa fa-check"></i><b>40.3.7</b> Add, commit, and push files using RStudio</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="large-datasets" class="section level1">
<h1><span class="header-section-number">Chapter 33</span> Large datasets</h1>
<p>Machine learning problems often involve datasets that are as large or larger than the MNIST dataset. There is a variety of computational techniques and statistical concepts that are useful for the analysis of large datasets. In this chapter we scratch the surface of these techniques and concepts by describing matrix algebra, dimension reduction, regularization and matrix factorization. We use recommendation systems related to movie ratings as a motivating example.</p>
<div id="matrix-algebra" class="section level2">
<h2><span class="header-section-number">33.1</span> Matrix algebra</h2>
<p>In machine learning, situations in which all predictors are numeric, or can be converted to numeric in a meaningful way, are common. The digits data set is an example: every pixel records a number between 0 and 255. Let’s load the data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(dslabs)
<span class="cf">if</span>(<span class="op">!</span><span class="kw">exists</span>(<span class="st">&quot;mnist&quot;</span>)) mnist &lt;-<span class="st"> </span><span class="kw">read_mnist</span>()</code></pre></div>
<p>In these cases, it is often convenient to save the predictors in a matrix and the outcome in a vector rather than using a data frame. You can see that the predictors are saved as a matrix:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">class</span>(mnist<span class="op">$</span>train<span class="op">$</span>images)
<span class="co">#&gt; [1] &quot;matrix&quot;</span></code></pre></div>
<p>This matrix represents 60,000 digits, so for the examples in this chapter, we will take a more manageable subset. We will take the first 1,000 predictors <code>x</code> and labels <code>y</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>images[<span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>,] 
y &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>labels[<span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>]</code></pre></div>
<p>The main reason for using matrices is that certain mathematical operations needed to develop efficient code can be performed using techniques from a branch of mathematics called <em>linear algebra</em>. In fact, linear algebra and matrix notation are key elements of the language used in academic papers describing machine learning techniques. We will not cover linear algebra in detail here, but will demonstrate how to use matrices in R so that you can apply the linear algebra techniques already implemented in base R or other packages.</p>
<p>To motivate the use of matrices, we will pose five questions/challenges:</p>
<p>1. Do some digits require more ink than others? Study the distribution of the total pixel darkness and how it varies by digits.</p>
<p>2. Are some pixels uninformative? Study the variation of each pixel and remove predictors (columns) associated with pixels that don’t change much and thus can’t provide much information for classification.</p>
<p>3. Can we remove smudges? First, look at the distribution of all pixel values. Use this to pick a cutoff to define unwritten space. Then, set anything below that cutoff to 0.</p>
<p>4. Binarize the data. First, look at the distribution of all pixel values. Use this to pick a cutoff to distinguish between writing and no writing. Then, convert all entries into either 1 or 0, respectively.</p>
<p>5. Scale each of the predictors in each entry to have the same average and standard deviation.</p>
<p>To complete these, we will have to perform mathematical operations involving several variables. The <strong>tidyverse</strong> is not developed to perform these types of mathematical operations. For this task, it is convenient to use matrices.</p>
<p>Before we do this, we will introduce matrix notation and basic R code to define and operate on matrices.</p>
<div id="notation-2" class="section level3">
<h3><span class="header-section-number">33.1.1</span> Notation</h3>
<p>In matrix algebra, we have three main types of objects: scalars, vectors, and matrices. A scalar is just one number, for example <span class="math inline">\(a = 1\)</span>. To denote scalars in matrix notation, we usually use a lower case letter and do not bold.</p>
<p>Vectors are like the numeric vectors we define in R: they include several scalar entries. For example, the column containing the first pixel:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">length</span>(x[,<span class="dv">1</span>])
<span class="co">#&gt; [1] 1000</span></code></pre></div>
<p>has 1,000 entries. In matrix algebra, we use the following notation for a vector representing a feature/predictor:</p>
<p><span class="math display">\[ 
\begin{pmatrix}
x_1\\\
x_2\\\
\vdots\\\
x_N
\end{pmatrix}
\]</span></p>
<p>Similarly, we can use math notation to represent different features mathematically by adding an index:</p>
<p><span class="math display">\[ 
\mathbf{X}_1 = \begin{pmatrix}
x_{1,1}\\
\vdots\\
x_{N,1}
\end{pmatrix} \mbox{ and }
\mathbf{X}_2 = \begin{pmatrix}
x_{1,2}\\
\vdots\\
x_{N,2}
\end{pmatrix}
\]</span></p>
<p>If we are writing out a column, such as <span class="math inline">\(\mathbf{X}_1\)</span>, in a sentence we often use the notation: <span class="math inline">\(\mathbf{X}_1 = ( x_{1,1}, \dots x_{N,1})^\top\)</span> with <span class="math inline">\(^\top\)</span> the transpose operation that converts columns into rows and rows into columns.</p>
<p>A matrix can be defined as a series of vectors of the same size joined together as columns:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">5</span>
x_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="dv">6</span><span class="op">:</span><span class="dv">10</span>
<span class="kw">cbind</span>(x_<span class="dv">1</span>, x_<span class="dv">2</span>)
<span class="co">#&gt;      x_1 x_2</span>
<span class="co">#&gt; [1,]   1   6</span>
<span class="co">#&gt; [2,]   2   7</span>
<span class="co">#&gt; [3,]   3   8</span>
<span class="co">#&gt; [4,]   4   9</span>
<span class="co">#&gt; [5,]   5  10</span></code></pre></div>
<p>Mathematically, we represent them with bold upper case letters:</p>
<p><span class="math display">\[ 
\mathbf{X} = [ \mathbf{X}_1 \mathbf{X}_2 ] = \begin{pmatrix}
x_{1,1}&amp;x_{1,2}\\
\vdots\\
x_{N,1}&amp;x_{N,2}
\end{pmatrix}
\]</span></p>
<p>The <em>dimension</em> of a matrix is often an important characteristic needed to assure that certain operations can be performed. The dimension is a two-number summary defined as the number of rows <span class="math inline">\(\times\)</span> the number of columns. In R, we can extract the dimension of a matrix with the function <code>dim</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(x)
<span class="co">#&gt; [1] 1000  784</span></code></pre></div>
<p>Vectors can be thought of as <span class="math inline">\(N\times 1\)</span> matrices. However, in R, a vector does not have dimensions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(x_<span class="dv">1</span>)
<span class="co">#&gt; NULL</span></code></pre></div>
<p>Yet we explicitly convert a vector into a matrix using the function <code>as.matrix</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(<span class="kw">as.matrix</span>(x_<span class="dv">1</span>))
<span class="co">#&gt; [1] 5 1</span></code></pre></div>
<p>We can use this notation to denote an arbitrary number of predictors with the following <span class="math inline">\(N\times p\)</span> matrix, for example, with <span class="math inline">\(p=784\)</span>:</p>
<p><span class="math display">\[
\mathbf{X} = \begin{pmatrix}
  x_{1,1}&amp;\dots &amp; x_{1,p} \\
  x_{2,1}&amp;\dots &amp; x_{2,p} \\
   &amp; \vdots &amp; \\
  x_{N,1}&amp;\dots &amp; x_{N,p} 
  \end{pmatrix}
\]</span></p>
<p>We stored this matrix in x:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(x)
<span class="co">#&gt; [1] 1000  784</span></code></pre></div>
<p>We will now learn several useful operations related to matrix algebra. We use three of the motivating questions listed above.</p>
</div>
<div id="converting-a-vector-to-a-matrix" class="section level3">
<h3><span class="header-section-number">33.1.2</span> Converting a vector to a matrix</h3>
<p>It is often useful to convert a vector to a matrix. For example, because the variables are pixels on a grid, we can convert the rows of pixel intensities into a matrix representing this grid.</p>
<p>We can convert a vector into a matrix with the <code>matrix</code> function and specifying the number of rows and columns that the resulting matrix should have. The matrix is filled in <strong>by column</strong>: the first column is filled first, then the second and so on. This example helps illustrate:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">my_vector &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">15</span>
mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(my_vector, <span class="dv">5</span>, <span class="dv">3</span>)
mat
<span class="co">#&gt;      [,1] [,2] [,3]</span>
<span class="co">#&gt; [1,]    1    6   11</span>
<span class="co">#&gt; [2,]    2    7   12</span>
<span class="co">#&gt; [3,]    3    8   13</span>
<span class="co">#&gt; [4,]    4    9   14</span>
<span class="co">#&gt; [5,]    5   10   15</span></code></pre></div>
<p>We can fill by row by using the <code>byrow</code> argument. So, for example, to <em>transpose</em> the matrix <code>mat</code>, we can use:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mat_t &lt;-<span class="st"> </span><span class="kw">matrix</span>(my_vector, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
mat_t
<span class="co">#&gt;      [,1] [,2] [,3] [,4] [,5]</span>
<span class="co">#&gt; [1,]    1    2    3    4    5</span>
<span class="co">#&gt; [2,]    6    7    8    9   10</span>
<span class="co">#&gt; [3,]   11   12   13   14   15</span></code></pre></div>
<p>When we turn the columns into rows, we refer to the operations as <em>transposing</em> the matrix. The function <code>t</code> can be used to directly transpose a matrix:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">identical</span>(<span class="kw">t</span>(mat), mat_t)
<span class="co">#&gt; [1] TRUE</span></code></pre></div>
<p><strong>Warning</strong>: The <code>matrix</code> function recycles values in the vector <strong>without warning</strong> if the product of columns and rows does not match the length of the vector:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">matrix</span>(my_vector, <span class="dv">4</span>, <span class="dv">5</span>)
<span class="co">#&gt; Warning in matrix(my_vector, 4, 5): data length [15] is not a sub-</span>
<span class="co">#&gt; multiple or multiple of the number of rows [4]</span>
<span class="co">#&gt;      [,1] [,2] [,3] [,4] [,5]</span>
<span class="co">#&gt; [1,]    1    5    9   13    2</span>
<span class="co">#&gt; [2,]    2    6   10   14    3</span>
<span class="co">#&gt; [3,]    3    7   11   15    4</span>
<span class="co">#&gt; [4,]    4    8   12    1    5</span></code></pre></div>
<p>To put the pixel intensities of our, say, 3rd entry, which is a 4 into grid, we can use:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">grid &lt;-<span class="st"> </span><span class="kw">matrix</span>(x[<span class="dv">3</span>,], <span class="dv">28</span>, <span class="dv">28</span>)</code></pre></div>
<p>To confirm that in fact we have done this correctly, we can use the function <code>image</code>, which shows an image of its third argument. The top of this plot is pixel 1, which is shown at the bottom so the image is flipped. To code below includes code showing how to flip it back:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">image</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">28</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">28</span>, grid)
<span class="kw">image</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">28</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">28</span>, grid[, <span class="dv">28</span><span class="op">:</span><span class="dv">1</span>])</code></pre></div>
<p><img src="book_files/figure-html/matrix-image-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="row-and-column-summaries" class="section level3">
<h3><span class="header-section-number">33.1.3</span> Row and column summaries</h3>
<p>For the first task, related to total pixel darkness, we want to sum the values of each row and then visualize how these values vary by digit.</p>
<p>The function <code>rowSums</code> takes a matrix as input and computes the desired values:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sums &lt;-<span class="st"> </span><span class="kw">rowSums</span>(x)</code></pre></div>
<p>We can also compute the averages with <code>rowMeans</code> if we want the values to remain between 0 and 255:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">avg &lt;-<span class="st"> </span><span class="kw">rowMeans</span>(x)</code></pre></div>
<p>Once we have this, we can simply generate a boxplot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tibble</span>(<span class="dt">labels =</span> <span class="kw">as.factor</span>(y), <span class="dt">row_averages =</span> avg) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">qplot</span>(labels, row_averages, <span class="dt">data =</span> ., <span class="dt">geom =</span> <span class="st">&quot;boxplot&quot;</span>) </code></pre></div>
<p><img src="book_files/figure-html/boxplot-of-digit-averages-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>From this plot we see that, not surprisingly, 1s use less ink than the other digits.</p>
<p>We can compute the column sums and averages using the function <code>colSums</code> and <code>colMeans</code>, respectively.</p>
<p>The <strong>matrixStats</strong> package adds functions that performs operations on each row or column very efficiently, including the functions <code>rowSds</code> and <code>colSds</code>.</p>
</div>
<div id="apply" class="section level3">
<h3><span class="header-section-number">33.1.4</span> <code>apply</code></h3>
<p>The functions just described are performing an operation similar to what <code>sapply</code> and the <strong>purrr</strong> function <code>map</code> do: apply the same function to a part of your object. In this case, the function is applied to either each row or each column. The <code>apply</code> function lets you apply any function, not just <code>sum</code> or <code>mean</code>, to a matrix. The first argument is the matrix, the second is the dimension, 1 for rows, 2 for columns, and the third is the function. So, for example, <code>rowMeans</code> can be written as:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">avgs &lt;-<span class="st"> </span><span class="kw">apply</span>(x, <span class="dv">1</span>, mean)</code></pre></div>
<p>But notice that just like with <code>sapply</code> and <code>map</code>, we can perform any function. So if we wanted the standard deviation for each column, we could write:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sds &lt;-<span class="st"> </span><span class="kw">apply</span>(x, <span class="dv">2</span>, sd)</code></pre></div>
<p>The tradeoff for this flexibility is that these operations are not as fast as dedicated functions such as <code>rowMeans</code>.</p>
</div>
<div id="filtering-columns-based-on-summaries" class="section level3">
<h3><span class="header-section-number">33.1.5</span> Filtering columns based on summaries</h3>
<p>We now turn to task 2: studying the variation of each pixel and removing columns associated with pixels that don’t change much and thus do not inform the classification. Although a simplistic approach, we will quantify the variation of each pixel with its standard deviation across all entries. Since each column represents a pixel, we use the <code>colSds</code> function from the <strong>matrixStats</strong> package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(matrixStats)
sds &lt;-<span class="st"> </span><span class="kw">colSds</span>(x)</code></pre></div>
<p>A quick look at the distribution of these values shows that some pixels have very low entry to entry variability:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qplot</span>(sds, <span class="dt">bins =</span> <span class="st">&quot;30&quot;</span>, <span class="dt">color =</span> <span class="kw">I</span>(<span class="st">&quot;black&quot;</span>))</code></pre></div>
<p><img src="book_files/figure-html/sds-histogram-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>This makes sense since we don’t write in some parts of the box. Here is the variance plotted by location:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">image</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">28</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">28</span>, <span class="kw">matrix</span>(sds, <span class="dv">28</span>, <span class="dv">28</span>)[, <span class="dv">28</span><span class="op">:</span><span class="dv">1</span>])</code></pre></div>
<p><img src="book_files/figure-html/pixel-variance-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>We see that there is little variation in the corners.</p>
<p>We could remove features that have no variation since these can’t help us predict. In Section <a href="r-basics.html#matrices">2.4.7</a>, we described the operations used to extract columns:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x[ ,<span class="kw">c</span>(<span class="dv">351</span>,<span class="dv">352</span>)]</code></pre></div>
<p>and rows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x[<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>),]</code></pre></div>
<p>We can also use logical indexes to determine which columns or rows to keep. So if we wanted to remove uninformative predictors from our matrix, we could write this one line of code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">new_x &lt;-<span class="st"> </span>x[ ,<span class="kw">colSds</span>(x) <span class="op">&gt;</span><span class="st"> </span><span class="dv">60</span>]
<span class="kw">dim</span>(new_x)
<span class="co">#&gt; [1] 1000  314</span></code></pre></div>
<p>Only the columns for which the standard deviation is above 60 are kept, which removes over half the predictors.</p>
<p>Here we add an important warning related to subsetting matrices: if you select one column or one row, the result is no longer a matrix but a vector.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">class</span>(x[,<span class="dv">1</span>])
<span class="co">#&gt; [1] &quot;integer&quot;</span>
<span class="kw">dim</span>(x[<span class="dv">1</span>,])
<span class="co">#&gt; NULL</span></code></pre></div>
<p>However, we can preserve the matrix class by using the argument <code>drop=FALSE</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">class</span>(x[ , <span class="dv">1</span>, <span class="dt">drop=</span><span class="ot">FALSE</span>])
<span class="co">#&gt; [1] &quot;matrix&quot;</span>
<span class="kw">dim</span>(x[, <span class="dv">1</span>, <span class="dt">drop=</span><span class="ot">FALSE</span>])
<span class="co">#&gt; [1] 1000    1</span></code></pre></div>
</div>
<div id="indexing-with-matrices" class="section level3">
<h3><span class="header-section-number">33.1.6</span> Indexing with matrices</h3>
<p>We can quickly make a histogram of all the values in our dataset. We saw how we can turn vectors into matrices. We can also undo this and turn matrices into vectors. The operation will happen by row:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">15</span>, <span class="dv">5</span>, <span class="dv">3</span>)
<span class="kw">as.vector</span>(mat)
<span class="co">#&gt;  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15</span></code></pre></div>
<p>To see a histogram of all our predictor data, we can use:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qplot</span>(<span class="kw">as.vector</span>(x), <span class="dt">bins =</span> <span class="dv">30</span>, <span class="dt">color =</span> <span class="kw">I</span>(<span class="st">&quot;black&quot;</span>))</code></pre></div>
<p><img src="book_files/figure-html/histogram-all-pixels-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We notice a clear dichotomy which is explained as parts of the image with ink and parts without. If we think that values below, say, 50 are smudges, we can quickly make them zero using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">new_x &lt;-<span class="st"> </span>x
new_x[new_x <span class="op">&lt;</span><span class="st"> </span><span class="dv">50</span>] &lt;-<span class="st"> </span><span class="dv">0</span></code></pre></div>
<p>To see what this does, we look at a smaller matrix:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">15</span>, <span class="dv">5</span>, <span class="dv">3</span>)
mat[mat <span class="op">&lt;</span><span class="st"> </span><span class="dv">3</span>] &lt;-<span class="st"> </span><span class="dv">0</span>
mat
<span class="co">#&gt;      [,1] [,2] [,3]</span>
<span class="co">#&gt; [1,]    0    6   11</span>
<span class="co">#&gt; [2,]    0    7   12</span>
<span class="co">#&gt; [3,]    3    8   13</span>
<span class="co">#&gt; [4,]    4    9   14</span>
<span class="co">#&gt; [5,]    5   10   15</span></code></pre></div>
<p>We can also use logical operations with matrix logical:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">15</span>, <span class="dv">5</span>, <span class="dv">3</span>)
mat[mat <span class="op">&gt;</span><span class="st"> </span><span class="dv">6</span> <span class="op">&amp;</span><span class="st"> </span>mat <span class="op">&lt;</span><span class="st"> </span><span class="dv">12</span>] &lt;-<span class="st"> </span><span class="dv">0</span>
mat
<span class="co">#&gt;      [,1] [,2] [,3]</span>
<span class="co">#&gt; [1,]    1    6    0</span>
<span class="co">#&gt; [2,]    2    0   12</span>
<span class="co">#&gt; [3,]    3    0   13</span>
<span class="co">#&gt; [4,]    4    0   14</span>
<span class="co">#&gt; [5,]    5    0   15</span></code></pre></div>
</div>
<div id="binarizing-the-data" class="section level3">
<h3><span class="header-section-number">33.1.7</span> Binarizing the data</h3>
<p>The histogram above seems to suggest that this data is mostly binary. A pixel either has ink or does not. Using what we have learned, we can binarize the data using just matrix operations:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bin_x &lt;-<span class="st"> </span>x
bin_x[bin_x <span class="op">&lt;</span><span class="st"> </span><span class="dv">255</span><span class="op">/</span><span class="dv">2</span>] &lt;-<span class="st"> </span><span class="dv">0</span> 
bin_x[bin_x <span class="op">&gt;</span><span class="st"> </span><span class="dv">255</span><span class="op">/</span><span class="dv">2</span>] &lt;-<span class="st"> </span><span class="dv">1</span></code></pre></div>
<p>We can also convert to a matrix of logicals and then coerce to numbers like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bin_X &lt;-<span class="st"> </span>(x <span class="op">&gt;</span><span class="st"> </span><span class="dv">255</span><span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="dv">1</span></code></pre></div>
<!--
We can see that at least the entry we looked at before does not change much:

<img src="book_files/figure-html/binarized-image-1.png" width="100%" style="display: block; margin: auto;" />
-->
</div>
<div id="vectorization-for-matrices" class="section level3">
<h3><span class="header-section-number">33.1.8</span> Vectorization for matrices</h3>
<p>In R, if we subtract a vector from a matrix, the first element of the vector is subtracted from the first row, the second element from the second row, and so on. Using mathematical notation, we would write it as follows:</p>
<p><span class="math display">\[
 \begin{pmatrix}
  X_{1,1}&amp;\dots &amp; X_{1,p} \\
  X_{2,1}&amp;\dots &amp; X_{2,p} \\
   &amp; \vdots &amp; \\
  X_{N,1}&amp;\dots &amp; X_{N,p} 
  \end{pmatrix}
-
\begin{pmatrix}
a_1\\\
a_2\\\
\vdots\\\
a_N
\end{pmatrix}
=
\begin{pmatrix}
  X_{1,1}-a_1&amp;\dots &amp; X_{1,p} -a_1\\
  X_{2,1}-a_2&amp;\dots &amp; X_{2,p} -a_2\\
   &amp; \vdots &amp; \\
  X_{N,1}-a_n&amp;\dots &amp; X_{N,p} -a_n
  \end{pmatrix}
\]</span></p>
<p>The same holds true for other arithmetic operations. This implies that we can scale each row of a matrix like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(x <span class="op">-</span><span class="st"> </span><span class="kw">rowMeans</span>(x)) <span class="op">/</span><span class="st"> </span><span class="kw">rowSds</span>(x)</code></pre></div>
<p>If you want to scale each column, be careful since this approach does not work for columns. To perform a similar operation, we convert the columns to rows using the transpose <code>t</code>, proceed as above, and then transpose back:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t</span>(<span class="kw">t</span>(X) <span class="op">-</span><span class="st"> </span><span class="kw">colMeans</span>(X))</code></pre></div>
<p>We can also use a function called <code>sweep</code> that works similarly to <code>apply</code>. It takes each entry of a vector and subtracts it from the corresponding row or column.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X_mean_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">sweep</span>(x, <span class="dv">2</span>, <span class="kw">colMeans</span>(x))</code></pre></div>
<p>The function <code>sweep</code> actually has another argument that lets you define the arithmetic operation. So to divide by the standard deviation, we do the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x_mean_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">sweep</span>(x, <span class="dv">2</span>, <span class="kw">colMeans</span>(x))
x_standardized &lt;-<span class="st"> </span><span class="kw">sweep</span>(x_mean_<span class="dv">0</span>, <span class="dv">2</span>, <span class="kw">colSds</span>(x), <span class="dt">FUN =</span> <span class="st">&quot;/&quot;</span>)</code></pre></div>
</div>
<div id="matrix-algebra-operations" class="section level3">
<h3><span class="header-section-number">33.1.9</span> Matrix algebra operations</h3>
<p>Finally, although we do not cover matrix algebra operations such as matrix multiplication, we share here the relevant commands for those that know the mathematics and want to learn the code:</p>
<p>1. Matrix multiplication is done with <code>%*%</code>. For example, the cross product is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t</span>(x) <span class="op">%*%</span><span class="st"> </span>x</code></pre></div>
<p>2. We can compute the cross product directly with the function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">crossprod</span>(x)</code></pre></div>
<p>3. To compute the inverse of a function, we use <code>solve</code>. Here it is applied to the cross product:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">solve</span>(<span class="kw">crossprod</span>(x))</code></pre></div>
<p>4. The QR decomposition is readily available by using the <code>qr</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qr</span>(x)</code></pre></div>
</div>
</div>
<div id="exercises-56" class="section level2">
<h2><span class="header-section-number">33.2</span> Exercises</h2>
<p>1. Create a 100 by 10 matrix of randomly generated normal numbers. Put the result in <code>x</code>.</p>
<p>2. Apply the three R functions that give you the dimension of <code>x</code>, the number of rows of <code>x</code>, and the number of columns of <code>x</code>, respectively.</p>
<p>3. Add the scalar 1 to row 1, the scalar 2 to row 2, and so on, to the matrix <code>x</code>.</p>
<p>4. Add the scalar 1 to column 1, the scalar 2 to column 2, and so on, to the matrix <code>x</code>. Hint: use <code>sweep</code> with <code>FUN = &quot;+&quot;</code>.</p>
<p>5. Compute the average of each row of <code>x</code>.</p>
<p>6. Compute the average of each column of <code>x</code>.</p>
<p>7. For each digit in the MNIST training data, compute the proportion of pixels that are in a <em>grey area</em>, defined as values between 50 and 205. Make boxplot by digit class. Hint: use logical operators and <code>rowMeans</code>.</p>

</div>
<div id="distance" class="section level2">
<h2><span class="header-section-number">33.3</span> Distance</h2>
<p>Many of the analyses we perform with high-dimensional data relate directly or indirectly to distance. Most clustering and machine learning techniques rely on being able to define distance between observations, using features or predictors.</p>
<div id="euclidean-distance" class="section level3">
<h3><span class="header-section-number">33.3.1</span> Euclidean distance</h3>
<p>As a review, let’s define the distance between two points, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, on a Cartesian plane.</p>
<p><img src="book_files/figure-html/euclidean-distance-1.png" width="35%" style="display: block; margin: auto;" /></p>
<p>The Euclidean distance between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is simply:</p>
<p><span class="math display">\[
\mbox{dist}(A,B) = \sqrt{ (A_x-B_x)^2 + (A_y-B_y)^2}
\]</span></p>
<p>This definition applies to the case of one dimension, in which the distance between two numbers is simply the absolute value of their difference. So if our two one-dimensional numbers are <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, the distance is:</p>
<p><span class="math display">\[
\mbox{dist}(A,B) = \sqrt{ (A - B)^2 } = | A - B |
\]</span></p>
</div>
<div id="distance-in-higher-dimensions" class="section level3">
<h3><span class="header-section-number">33.3.2</span> Distance in higher dimensions</h3>
<p>Earlier we introduced a training dataset with feature matrix measurements for 784 features. For illustrative purposes, we will look at a random sample of 2s and 7s.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(dslabs)

<span class="cf">if</span>(<span class="op">!</span><span class="kw">exists</span>(<span class="st">&quot;mnist&quot;</span>)) mnist &lt;-<span class="st"> </span><span class="kw">read_mnist</span>()

<span class="kw">set.seed</span>(<span class="dv">1995</span>)
ind &lt;-<span class="st"> </span><span class="kw">which</span>(mnist<span class="op">$</span>train<span class="op">$</span>labels <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">7</span>)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sample</span>(<span class="dv">500</span>)
x &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>images[ind,]
y &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>labels[ind]</code></pre></div>
<p>The predictors are in <code>x</code> and the labels in <code>y</code>.</p>
<p>For the purposes of, for example, smoothing, we are interested in describing distance between observation; in this case, digits. Later, for the purposes of selecting features, we might also be interested in finding pixels that <em>behave similarly</em> across samples.</p>
<p>To define distance, we need to know what <em>points</em> are since mathematical distance is computed between points. With high dimensional data, points are no longer on the Cartesian plane. Instead, points are in higher dimensions. We can no longer visualize them and need to think abstractly. For example, predictors <span class="math inline">\(\mathbf{X}_i\)</span> are defined as a point in 784 dimensional space: <span class="math inline">\(\mathbf{X}_i = (x_{i,1},\dots,x_{i,784})^\top\)</span>.</p>
<p>Once we define points this way, the Euclidean distance is defined very similarly as it was for two dimensions. For example, the distance between the predictors for two observations, say observations <span class="math inline">\(i=1\)</span> and <span class="math inline">\(i=2\)</span>, is:</p>
<p><span class="math display">\[
\mbox{dist}(1,2) = \sqrt{ \sum_{j=1}^{784} (x_{1,j}-x_{2,j })^2 }
\]</span></p>
<p>This is just one non-negative number, just as it is for two dimensions.</p>
</div>
<div id="euclidean-distance-example" class="section level3">
<h3><span class="header-section-number">33.3.3</span> Euclidean distance example</h3>
<p>The labels for the first three observations are:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]
<span class="co">#&gt; [1] 7 2 7</span></code></pre></div>
<p>The vectors of predictors for each of these observations are:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x_<span class="dv">1</span> &lt;-<span class="st"> </span>x[<span class="dv">1</span>,]
x_<span class="dv">2</span> &lt;-<span class="st"> </span>x[<span class="dv">2</span>,]
x_<span class="dv">3</span> &lt;-<span class="st"> </span>x[<span class="dv">3</span>,]</code></pre></div>
<p>The first two numbers are seven and the third one is a 2. We expect the distances between the same number:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(<span class="kw">sum</span>((x_<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>x_<span class="dv">2</span>)<span class="op">^</span><span class="dv">2</span>))
<span class="co">#&gt; [1] 3273</span></code></pre></div>
<p>to be smaller than between different numbers:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(<span class="kw">sum</span>((x_<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>x_<span class="dv">3</span>)<span class="op">^</span><span class="dv">2</span>))
<span class="co">#&gt; [1] 2311</span>
<span class="kw">sqrt</span>(<span class="kw">sum</span>((x_<span class="dv">2</span> <span class="op">-</span><span class="st"> </span>x_<span class="dv">3</span>)<span class="op">^</span><span class="dv">2</span>))
<span class="co">#&gt; [1] 2636</span></code></pre></div>
<p>As expected, the 7s are closer to each other.</p>
<p>A faster way to compute this is using matrix algebra:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(<span class="kw">crossprod</span>(x_<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>x_<span class="dv">2</span>))
<span class="co">#&gt;      [,1]</span>
<span class="co">#&gt; [1,] 3273</span>
<span class="kw">sqrt</span>(<span class="kw">crossprod</span>(x_<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>x_<span class="dv">3</span>))
<span class="co">#&gt;      [,1]</span>
<span class="co">#&gt; [1,] 2311</span>
<span class="kw">sqrt</span>(<span class="kw">crossprod</span>(x_<span class="dv">2</span> <span class="op">-</span><span class="st"> </span>x_<span class="dv">3</span>))
<span class="co">#&gt;      [,1]</span>
<span class="co">#&gt; [1,] 2636</span></code></pre></div>
<p>We can also compute <strong>all</strong> the distances at once relatively quickly using the function <code>dist</code>, which computes the distance between each row and produces an object of class <code>dist</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span><span class="kw">dist</span>(x)
<span class="kw">class</span>(d)
<span class="co">#&gt; [1] &quot;dist&quot;</span></code></pre></div>
<p>There are several machine learning related functions in R that take objects of class <code>dist</code> as input. To access the entries using row and column indices, we need to coerce it into a matrix. We can see the distance we calculated above like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">as.matrix</span>(d)[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]
<span class="co">#&gt;      1    2    3</span>
<span class="co">#&gt; 1    0 3273 2311</span>
<span class="co">#&gt; 2 3273    0 2636</span>
<span class="co">#&gt; 3 2311 2636    0</span></code></pre></div>
<p>We can quickly see an image of these distances using this code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">image</span>(<span class="kw">as.matrix</span>(d))</code></pre></div>
<p>If we order this distance by the labels, we can see that, in general, the twos are closer to each other and the sevens are closer to each other:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">image</span>(<span class="kw">as.matrix</span>(d)[<span class="kw">order</span>(y), <span class="kw">order</span>(y)])</code></pre></div>
<p><img src="book_files/figure-html/diatance-image-ordered-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>One thing we notice here is that there appears to be more uniformity in how the sevens are drawn, since they appear to be closer (more red) to other sevens than twos are to other twos.</p>
</div>
<div id="predictor-space" class="section level3">
<h3><span class="header-section-number">33.3.4</span> Predictor space</h3>
<p><em>Predictor space</em> is a concept that is often used to describe machine learning algorithms. The term <em>space</em> refers to a mathematical definition that we don’t describe in detail here. Instead, we provide a simplified explanation to help understand the term predictor space when used in the context of machine learning algorithms.</p>
<p>The predictor space can be thought of as the collection of all possible vectors of predictors that should be considered for the machine learning challenge in question. Each member of the space is referred to as a <em>point</em>. For example, in the 2 or 7 dataset, the predictor space consists of all pairs <span class="math inline">\((x_1, x_2)\)</span> such that both <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are within 0 and 1. This particular <em>space</em> can be represented graphically as a square. In the MNIST dataset the predictor space consists of all 784-th dimensional vectors with each vector element an integer between 0 and 256. An essential element of a predictor space is that we need to define a function that provides the distance between any two points. In most cases we use Euclidean distance, but there are other possibilities. A particular case in which we can’t simply use Euclidean distance is when we have categorical predictors.</p>
<p>Defining a predictor space is useful in machine learning because we do things like define neighborhoods of points, as required by many smoothing techniques. For example, we can define a neighborhood as all the points that are within 2 units away from a predefined center. If the points are two-dimensional and we use Euclidean distance, this neighborhood is graphically represented as a circle with radius 2. In three dimensions the neighborhood is a sphere. We will soon learn about algorithms that partition the space into non-overlapping regions and then make different predictions for each region using the data in the region.</p>
</div>
<div id="distance-between-predictors" class="section level3">
<h3><span class="header-section-number">33.3.5</span> Distance between predictors</h3>
<p>We can also compute distances between predictors. If <span class="math inline">\(N\)</span> is the number of observations, the distance between two predictors, say 1 and 2, is:</p>
<p><span class="math display">\[
\mbox{dist}(1,2) = \sqrt{ \sum_{i=1}^{N} (x_{i,1}-x_{i,2})^2 }
\]</span></p>
<p>To compute the distance between all pairs of the 784 predictors, we can transpose the matrix first and then use <code>dist</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span><span class="kw">dist</span>(<span class="kw">t</span>(x))
<span class="kw">dim</span>(<span class="kw">as.matrix</span>(d))
<span class="co">#&gt; [1] 784 784</span></code></pre></div>
<!--
An interesting thing to note here is that if we pick a predictor (a pixel), we can see which pixels are close. That is, the pair of pixels either have ink in the same images (small distance) or they don't (large distance). The distance between, for example, and all other pixels is given by:


```r
d_492 <- as.matrix(d)[492,]
```
 
We can now see the spatial pattern of these distances with the following code:


```r
image(1:28, 1:28, matrix(d_492, 28, 28))
```

<img src="book_files/figure-html/distnace-rows-1.png" width="70%" style="display: block; margin: auto;" />

Not surprisingly, points physically nearby are mathematically closer.
-->
</div>
</div>
<div id="exercises-57" class="section level2">
<h2><span class="header-section-number">33.4</span> Exercises</h2>
<p>1. Load the following dataset:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;tissue_gene_expression&quot;</span>)</code></pre></div>
<p>This dataset includes a matrix <code>x</code></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(tissue_gene_expression<span class="op">$</span>x)</code></pre></div>
<p>with the gene expression measured on 500 genes for 189 biological samples representing seven different tissues. The tissue type is stored in <code>y</code></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(tissue_gene_expression<span class="op">$</span>y)</code></pre></div>
<p>Compute the distance between each observation and store it in an object <code>d</code>.</p>
<p>2. Compare the distance between the first two observations (both cerebellums), the 39th and 40th (both colons), and the 73rd and 74th (both endometriums). See if the observations of the same tissue type are closer to each other.</p>
<p>3. We see that indeed observations of the same tissue type are closer to each other in the six tissue examples we just examined. Make a plot of all the distances using the <code>image</code> function to see if this pattern is general. Hint: convert <code>d</code> to a matrix first.</p>

</div>
<div id="dimension-reduction" class="section level2">
<h2><span class="header-section-number">33.5</span> Dimension reduction</h2>
<p>A typical machine learning challenge will include a large number of predictors, which makes visualization somewhat challenging. We have shown methods for visualizing univariate and paired data, but plots that reveal relationships between many variables are more complicated in higher dimensions. For example, to compare each of the 784 features in our predicting digits example, we would have to create, for example, 306,936 scatterplots. Creating one single scatter-plot of the data is impossible due to the high dimensionality.</p>
<p>Here we describe powerful techniques useful for exploratory data analysis, among other things, generally referred to as <em>dimension reduction</em>. The general idea is to reduce the dimension of the dataset while preserving important characteristics, such as the distance between features or observations. With fewer dimensions, visualization then becomes more feasible. The technique behind it all, the singular value decomposition, is also useful in other contexts. Principal component analysis (PCA) is the approach we will be showing. Before applying PCA to high-dimensional datasets, we will motivate the ideas behind with a simple example.</p>
<div id="preserving-distance" class="section level3">
<h3><span class="header-section-number">33.5.1</span> Preserving distance</h3>
<p>We consider an example with twin heights. Some pairs are adults, the others are children. Here we simulate 100 two-dimensional points that represent the number of standard deviations each individual is from the mean height. Each point is a pair of twins. We use the <code>mvrnorm</code> function from the <strong>MASS</strong> package to simulate bivariate normal data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1988</span>)
<span class="kw">library</span>(MASS)
n &lt;-<span class="st"> </span><span class="dv">100</span>
Sigma &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">9</span>, <span class="dv">9</span> <span class="op">*</span><span class="st"> </span><span class="fl">0.9</span>, <span class="dv">9</span> <span class="op">*</span><span class="st"> </span><span class="fl">0.92</span>, <span class="dv">9</span> <span class="op">*</span><span class="st"> </span><span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>)
x &lt;-<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">mvrnorm</span>(n <span class="op">/</span><span class="st"> </span><span class="dv">2</span>, <span class="kw">c</span>(<span class="dv">69</span>, <span class="dv">69</span>), Sigma),
           <span class="kw">mvrnorm</span>(n <span class="op">/</span><span class="st"> </span><span class="dv">2</span>, <span class="kw">c</span>(<span class="dv">55</span>, <span class="dv">55</span>), Sigma))</code></pre></div>
<p>A scatterplot quickly reveals that the correlation is high and that there are two groups of twins, the adults (upper right points) and the children (lower left points):</p>
<!--
<img src="book_files/figure-html/simulated-twin-heights-1.png" width="50%" style="display: block; margin: auto;" />
-->
<p><img src="book_files/figure-html/distance-illustration-1.png" width="40%" style="display: block; margin: auto;" /></p>
<p>Our features are <span class="math inline">\(N\)</span> two-dimensional points, the two heights, and, for illustrative purposes, we will act as if visualizing two dimensions is too challenging. We therefore want to reduce the dimensions from two to one, but still be able to understand important characteristics of the data, for example that the observations cluster into two groups: adults and children.</p>
<p>Let’s consider a specific challenge: we want a one-dimensional summary of our predictors from which we can approximate the distance between any two observations. In the figure above we show the distance between observation 1 and 2 (blue), and observation 1 and 51 (red). Note that the blue line is shorter, which implies 1 and 2 are closer.</p>
<p>We can compute these distances using <code>dist</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span><span class="kw">dist</span>(x)
<span class="kw">as.matrix</span>(d)[<span class="dv">1</span>, <span class="dv">2</span>]
<span class="co">#&gt; [1] 1.98</span>
<span class="kw">as.matrix</span>(d)[<span class="dv">2</span>, <span class="dv">51</span>]
<span class="co">#&gt; [1] 18.7</span></code></pre></div>
<p>This distance is based on two dimensions and we need a distance approximation based on just one.</p>
<p>Let’s start with the naive approach of simply removing one of the two dimensions. Let’s compare the actual distances to the distance computed with just the first dimension:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z &lt;-<span class="st"> </span>x[,<span class="dv">1</span>]</code></pre></div>
<p>Here are the approximate distances versus the original distances: <img src="book_files/figure-html/one-dim-approx-to-dist-1.png" width="40%" style="display: block; margin: auto;" /></p>
<p>The plot looks about the same if we use the second dimension. We obtain a general underestimation. This is to be expected because we are adding more positive quantities in the distance calculation as we increase the number of dimensions. If instead we use an average, like this</p>
<p><span class="math display">\[\sqrt{ \frac{1}{2} \sum_{j=1}^2 (X_{i,j}-X_{i,j})^2 },\]</span></p>
<p>then the underestimation goes away. We divide the distance by <span class="math inline">\(\sqrt{2}\)</span> to achieve the correction.</p>
<p><img src="book_files/figure-html/distance-approx-1-1.png" width="40%" style="display: block; margin: auto;" /></p>
<p>This actually works pretty well and we get a typical difference of:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(<span class="kw">dist</span>(x) <span class="op">-</span><span class="st"> </span><span class="kw">dist</span>(z)<span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">2</span>))
<span class="co">#&gt; [1] 1.21</span></code></pre></div>
<p>Now, can we pick a one-dimensional summary that makes this approximation even better?</p>
<p>If we look back at the previous scatterplot and visualize a line between any pair of points, the length of this line is the distance between the two points. These lines tend to go along the direction of the diagonal. Notice that if we instead plot the difference versus the average:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z  &lt;-<span class="st"> </span><span class="kw">cbind</span>((x[,<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>x[,<span class="dv">1</span>])<span class="op">/</span><span class="dv">2</span>,  x[,<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span>x[,<span class="dv">1</span>])</code></pre></div>
<p>we can see how the distance between points is mostly explained by the first dimension: the average.</p>
<p><img src="book_files/figure-html/rotation-1.png" width="40%" style="display: block; margin: auto;" /></p>
<p>This means that we can ignore the second dimension and not lose too much information. If the line is completely flat, we lose no information at all. Using the first dimension of this transformed matrix we obtain an even better approximation:</p>
<p><img src="book_files/figure-html/distance-approx-2-1.png" width="40%" style="display: block; margin: auto;" /></p>
<p>with the typical difference improved by about 35%:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(<span class="kw">dist</span>(x) <span class="op">-</span><span class="st"> </span><span class="kw">dist</span>(z[,<span class="dv">1</span>])<span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">2</span>))
<span class="co">#&gt; [1] 0.315</span></code></pre></div>
<p>Later we learn that <code>z[,1]</code> is the first principal component of the matrix <code>x</code>.</p>
</div>
<div id="linear-transformations-advanced" class="section level3">
<h3><span class="header-section-number">33.5.2</span> Linear transformations (advanced)</h3>
<p>Note that each row of <span class="math inline">\(X\)</span> was transformed using a linear transformation. For any row <span class="math inline">\(i\)</span>, the first entry was:</p>
<p><span class="math display">\[Z_{i,1} = a_{1,1} X_{i,1} + a_{2,1} X_{i,2}\]</span></p>
<p>with <span class="math inline">\(a_{1,1} = 0.5\)</span> and <span class="math inline">\(a_{2,1} = 0.5\)</span>.</p>
<p>The second entry was also a linear transformation:</p>
<p><span class="math display">\[Z_{i,2} = a_{1,2} X_{i,1} + a_{2,2} X_{i,2}\]</span></p>
<p>with <span class="math inline">\(a_{1,2} = 1\)</span> and <span class="math inline">\(a_{2,2} = -1\)</span>.</p>
<p>We can also use linear transformation to get <span class="math inline">\(X\)</span> back from <span class="math inline">\(Z\)</span>:</p>
<p><span class="math display">\[X_{i,1} = b_{1,1} Z_{i,1} + b_{2,1} Z_{i,2}\]</span></p>
<p>with <span class="math inline">\(b_{1,2} = 1\)</span> and <span class="math inline">\(b_{2,1} = 0.5\)</span> and</p>
<p><span class="math display">\[X_{i,2} = b_{2,1} Z_{i,1} + b_{2,2} Z_{i,2}\]</span></p>
<p>with <span class="math inline">\(b_{2,1} = 1\)</span> and <span class="math inline">\(a_{1,2} = -0.5\)</span>.</p>
<p>If you are familiar with linear algebra, we can write the operation we just performed like this:</p>
<p><span class="math display">\[
Z = X A
\mbox{ with }
A = \,
\begin{pmatrix}
1/2&amp;1\\
1/2&amp;-1\\
\end{pmatrix}.
\]</span></p>
<p>And that we can transform back by simply multiplying by <span class="math inline">\(A^{-1}\)</span> as follows:</p>
<p><span class="math display">\[
X = Z A^{-1} 
\mbox{ with }
A^{-1} = \,
\begin{pmatrix}
1&amp;1\\
1/2&amp;-1/2\\
\end{pmatrix}.
\]</span></p>
<p>Dimension reduction can often be described as applying a transformation <span class="math inline">\(A\)</span> to a matrix <span class="math inline">\(X\)</span> with many columns that <em>moves</em> the information contained in <span class="math inline">\(X\)</span> to the first few columns of <span class="math inline">\(Z=AX\)</span>, then keeping just these few informative columns, thus reducing the dimension of the vectors contained in the rows.</p>
</div>
<div id="orthogonal-transformations-advanced" class="section level3">
<h3><span class="header-section-number">33.5.3</span> Orthogonal transformations (advanced)</h3>
<p>Note that we divided the above by <span class="math inline">\(\sqrt{2}\)</span> to account for the differences in dimensions when comparing a 2 dimension distance to a 1 dimension distance. We can actually guarantee that the distance scales remain the same if we re-scale the columns of <span class="math inline">\(A\)</span> to assure that the sum of squares is 1</p>
<p><span class="math display">\[a_{1,1}^2 + a_{2,1}^2 = 1\mbox{ and } a_{1,2}^2 + a_{2,2}^2=1,\]</span></p>
<p>and that the correlation of the columns is 0:</p>
<p><span class="math display">\[
a_{1,1} a_{1,2} + a_{2,1} a_{2,2} = 0.
\]</span></p>
<p>Remember that if the columns are centered to have average 0, then the sum of squares is equivalent to the variance or standard deviation squared.</p>
<p>In our example, to achieve orthogonality, we multiply the first set of coefficients (first column of <span class="math inline">\(A\)</span>) by <span class="math inline">\(\sqrt{2}\)</span> and the second by <span class="math inline">\(1/\sqrt{2}\)</span>, then we get the same exact distance if we use both dimensions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z[,<span class="dv">1</span>] &lt;-<span class="st"> </span>(x[,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>x[,<span class="dv">2</span>]) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">2</span>)
z[,<span class="dv">2</span>] &lt;-<span class="st"> </span>(x[,<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span>x[,<span class="dv">1</span>]) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">2</span>)</code></pre></div>
<p>This gives us a transformation that preserves the distance between any two points:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">max</span>(<span class="kw">dist</span>(z) <span class="op">-</span><span class="st"> </span><span class="kw">dist</span>(x))
<span class="co">#&gt; [1] 3.24e-14</span></code></pre></div>
<!--
<img src="book_files/figure-html/orthogonal-transformation-1.png" width="70%" style="display: block; margin: auto;" />
-->
<p>and an improved approximation if we use just the first dimension:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(<span class="kw">dist</span>(x) <span class="op">-</span><span class="st"> </span><span class="kw">dist</span>(z[,<span class="dv">1</span>]))
<span class="co">#&gt; [1] 0.315</span></code></pre></div>
<p>In this case <span class="math inline">\(Z\)</span> is called an orthogonal rotation of <span class="math inline">\(X\)</span>: it preserves the distances between rows.</p>
<p>Note that by using the transformation above we can summarize the distance between any two pairs of twins with just one dimension. For example, one-dimensional data exploration of the first dimension of <span class="math inline">\(Z\)</span> clearly shows that there are two groups, adults and children:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">qplot</span>(z[,<span class="dv">1</span>], <span class="dt">bins =</span> <span class="dv">20</span>, <span class="dt">color =</span> <span class="kw">I</span>(<span class="st">&quot;black&quot;</span>))</code></pre></div>
<p><img src="book_files/figure-html/twins-pc-1-hist-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We successfully reduced the number of dimensions from two to one with very little loss of information.</p>
<p>The reason we were able to do this is because the columns of <span class="math inline">\(X\)</span> were very correlated:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(x[,<span class="dv">1</span>], x[,<span class="dv">2</span>])
<span class="co">#&gt; [1] 0.988</span></code></pre></div>
<p>and the transformation produced uncorrelated columns with “independent” information in each column:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(z[,<span class="dv">1</span>], z[,<span class="dv">2</span>])
<span class="co">#&gt; [1] 0.0876</span></code></pre></div>
<p>One way this insight may be useful in a machine learning application is that we can reduce the complexity of a model by using just <span class="math inline">\(Z_1\)</span> rather than both <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>.</p>
<p>It is actually common to obtain data with several highly correlated predictors. In these cases PCA, which we describe next, can be quite useful for reducing the complexity of the model being fit.</p>
</div>
<div id="principal-component-analysis" class="section level3">
<h3><span class="header-section-number">33.5.4</span> Principal component analysis</h3>
<p>In the computation above, the total variability in our data can be defined as the sum of the sum of squares of the columns. We assume the columns are centered, so this sum is equivalent to the sum of the variances of each column:</p>
<p><span class="math display">\[
v_1 + v_2, \mbox{ with } v_1 = \frac{1}{N}\sum_{i=1}^N X_{i,1}^2 \mbox{ and } v_2 =  \frac{1}{N}\sum_{i=1}^N X_{i,2}^2 
\]</span></p>
<p>We can compute <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span> using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">colMeans</span>(x<span class="op">^</span><span class="dv">2</span>) 
<span class="co">#&gt; [1] 3904 3902</span></code></pre></div>
<p>and we can show mathematically that if we apply an orthogonal transformation as above, then the total variation remains the same:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(<span class="kw">colMeans</span>(x<span class="op">^</span><span class="dv">2</span>))
<span class="co">#&gt; [1] 7806</span>
<span class="kw">sum</span>(<span class="kw">colMeans</span>(z<span class="op">^</span><span class="dv">2</span>))
<span class="co">#&gt; [1] 7806</span></code></pre></div>
<p>However, while the variability in the two columns of <code>X</code> is about the same, in the transformed version <span class="math inline">\(Z\)</span> 99% of the variability is included in just the first dimension:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">v &lt;-<span class="st"> </span><span class="kw">colMeans</span>(z<span class="op">^</span><span class="dv">2</span>)
v<span class="op">/</span><span class="kw">sum</span>(v)
<span class="co">#&gt; [1] 1.00e+00 9.93e-05</span></code></pre></div>
<p>The <em>first principal component (PC)</em> of a matrix <span class="math inline">\(X\)</span> is the linear orthogonal transformation of <span class="math inline">\(X\)</span> that maximizes this variability. The function <code>prcomp</code> provides this info:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pca &lt;-<span class="st"> </span><span class="kw">prcomp</span>(x)
pca<span class="op">$</span>rotation
<span class="co">#&gt;         PC1    PC2</span>
<span class="co">#&gt; [1,] -0.702  0.712</span>
<span class="co">#&gt; [2,] -0.712 -0.702</span></code></pre></div>
<p>Note that the first PC is almost the same as that provided by the <span class="math inline">\((X_1 + X_2) / \sqrt{2}\)</span> we used earlier (except perhaps for a sign change that is arbitrary).</p>
<p>The function PCA returns both the rotation needed to transform <span class="math inline">\(X\)</span> so that the variability of the columns is decreasing from most variable to least (accessed with <code>$rotation</code>) as well as the resulting new matrix (accessed with <code>$x</code>). By default the columns of <span class="math inline">\(X\)</span> are first centered.</p>
<p>So, using the matrix multiplication shown above, we have that the following are the same (demonstrated by a difference between elements of essentially zero):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">a &lt;-<span class="st"> </span><span class="kw">sweep</span>(x, <span class="dv">2</span>, <span class="kw">colMeans</span>(x)) 
b &lt;-<span class="st"> </span>pca<span class="op">$</span>x <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(pca<span class="op">$</span>rotation)
<span class="kw">max</span>(<span class="kw">abs</span>(a <span class="op">-</span><span class="st"> </span>b))
<span class="co">#&gt; [1] 3.55e-15</span></code></pre></div>
<p>The rotation is orthogonal which means that the inverse is its transpose. So we also have that these two are identical:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">a &lt;-<span class="st"> </span><span class="kw">sweep</span>(x, <span class="dv">2</span>, <span class="kw">colMeans</span>(x)) <span class="op">%*%</span><span class="st"> </span>pca<span class="op">$</span>rotation
b &lt;-<span class="st"> </span>pca<span class="op">$</span>x 
<span class="kw">max</span>(<span class="kw">abs</span>(a <span class="op">-</span><span class="st"> </span>b))
<span class="co">#&gt; [1] 0</span></code></pre></div>
<p>We can visualize these to see how the first component summarizes the data. In the plot below red represents high values and blue negative values (later we learn why we call these weights and patterns):</p>
<p><img src="book_files/figure-html/illustrate-pca-twin-heights-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>It turns out that we can find this linear transformation not just for two dimensions but for matrices of any dimension <span class="math inline">\(p\)</span>.</p>
<p>For a multidimensional matrix with <span class="math inline">\(X\)</span> with <span class="math inline">\(p\)</span> columns, we can find a transformation that creates <span class="math inline">\(Z\)</span> that preserves distance between rows, but with the variance of the columns in decreasing order. The second column is the second principal component, the third column is the third principal component, and so on. As in our example, if after a certain number of columns, say <span class="math inline">\(k\)</span>, the variances of the columns of <span class="math inline">\(Z_j\)</span>, <span class="math inline">\(j&gt;k\)</span> are very small, it means these dimensions have little to contribute to the distance and we can approximate distance between any two points with just <span class="math inline">\(k\)</span> dimensions. If <span class="math inline">\(k\)</span> is much smaller than <span class="math inline">\(p\)</span>, then we can achieve a very efficient summary of our data.</p>
</div>
<div id="iris-example" class="section level3">
<h3><span class="header-section-number">33.5.5</span> Iris example</h3>
<p>The iris data is a widely used example in data analysis courses. It includes four botanical measurements related to three flower species:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">names</span>(iris)
<span class="co">#&gt; [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot;  &quot;Petal.Length&quot; &quot;Petal.Width&quot; </span>
<span class="co">#&gt; [5] &quot;Species&quot;</span></code></pre></div>
<p>If you print <code>iris$Species</code> you will see that the data is ordered by the species.</p>
<p>Let’s compute the distance between each observation. You can clearly see the three species with one species very different from the other two:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span>iris[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.matrix</span>()
d &lt;-<span class="st"> </span><span class="kw">dist</span>(x)
<span class="kw">image</span>(<span class="kw">as.matrix</span>(d), <span class="dt">col =</span> <span class="kw">rev</span>(RColorBrewer<span class="op">::</span><span class="kw">brewer.pal</span>(<span class="dv">9</span>, <span class="st">&quot;RdBu&quot;</span>)))</code></pre></div>
<p><img src="book_files/figure-html/iris-distances-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Our predictors here have four dimensions, but three are very correlated:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(x)
<span class="co">#&gt;              Sepal.Length Sepal.Width Petal.Length Petal.Width</span>
<span class="co">#&gt; Sepal.Length        1.000      -0.118        0.872       0.818</span>
<span class="co">#&gt; Sepal.Width        -0.118       1.000       -0.428      -0.366</span>
<span class="co">#&gt; Petal.Length        0.872      -0.428        1.000       0.963</span>
<span class="co">#&gt; Petal.Width         0.818      -0.366        0.963       1.000</span></code></pre></div>
<p>If we apply PCA, we should be able to approximate this distance with just two dimensions, compressing the highly correlated dimensions. Using the <code>summary</code> function we can see the variability explained by each PC:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pca &lt;-<span class="st"> </span><span class="kw">prcomp</span>(x)
<span class="kw">summary</span>(pca)
<span class="co">#&gt; Importance of components:</span>
<span class="co">#&gt;                          PC1    PC2    PC3     PC4</span>
<span class="co">#&gt; Standard deviation     2.056 0.4926 0.2797 0.15439</span>
<span class="co">#&gt; Proportion of Variance 0.925 0.0531 0.0171 0.00521</span>
<span class="co">#&gt; Cumulative Proportion  0.925 0.9777 0.9948 1.00000</span></code></pre></div>
<p>The first two dimensions account for 97% of the variability. Thus we should be able to approximate the distance very well with two dimensions. We can visualize the results of PCA:</p>
<p><img src="book_files/figure-html/illustrate-pca-twin-heights-iris-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>And see that the first pattern is sepal length, petal length, and petal width (red) in one direction and sepal width (blue) in the other. The second pattern is the sepal length and petal width in one direction (blue) and petal length and petal width in the other (red). You can see from the weights that the first PC1 drives most of the variability and it clearly separates the first third of samples (setosa) from the second two thirds (versicolor and virginica). If you look at the second column of the weights, you notice that it somewhat separates versicolor (red) from virginica (blue).</p>
<p>We can see this better by plotting the first two PCs with color representing the species:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data.frame</span>(pca<span class="op">$</span>x[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>], <span class="dt">Species=</span>iris<span class="op">$</span>Species) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(PC1,PC2, <span class="dt">fill =</span> Species))<span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">cex=</span><span class="dv">3</span>, <span class="dt">pch=</span><span class="dv">21</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_fixed</span>(<span class="dt">ratio =</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="book_files/figure-html/iris-pca-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We see that the first two dimensions preserve the distance:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d_approx &lt;-<span class="st"> </span><span class="kw">dist</span>(pca<span class="op">$</span>x[, <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>])
<span class="kw">qplot</span>(d, d_approx) <span class="op">+</span><span class="st"> </span><span class="kw">geom_abline</span>(<span class="dt">color=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="book_files/figure-html/dist-approx-4-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>This example is more realistic than the first artificial example we used, since we showed how we can visualize the data using two dimensions when the data was four-dimensional.</p>
</div>
<div id="mnist-example" class="section level3">
<h3><span class="header-section-number">33.5.6</span> MNIST example</h3>
<p>The written digits example has 784 features. Is there any room for data reduction? Can we create simple machine learning algorithms using fewer features?</p>
<p>Let’s load the data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dslabs)
<span class="cf">if</span>(<span class="op">!</span><span class="kw">exists</span>(<span class="st">&quot;mnist&quot;</span>)) mnist &lt;-<span class="st"> </span><span class="kw">read_mnist</span>()</code></pre></div>
<p>Because the pixels are so small, we expect pixels close to each other on the grid to be correlated, meaning that dimension reduction should be possible.</p>
<p>Let’s try PCA and explore the variance of the PCs. This will take a few seconds as it is a rather large matrix.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">col_means &lt;-<span class="st"> </span><span class="kw">colMeans</span>(mnist<span class="op">$</span>test<span class="op">$</span>images)
pca &lt;-<span class="st"> </span><span class="kw">prcomp</span>(mnist<span class="op">$</span>train<span class="op">$</span>images)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pc &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(mnist<span class="op">$</span>test<span class="op">$</span>images)
<span class="kw">qplot</span>(pc, pca<span class="op">$</span>sdev)</code></pre></div>
<p><img src="book_files/figure-html/mnist-pca-variance-explained-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We can see that the first few PCs already explain a large percent of the variability:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(pca)<span class="op">$</span>importance[,<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>] 
<span class="co">#&gt;                            PC1     PC2      PC3      PC4      PC5</span>
<span class="co">#&gt; Standard deviation     576.823 493.238 459.8993 429.8562 408.5668</span>
<span class="co">#&gt; Proportion of Variance   0.097   0.071   0.0617   0.0539   0.0487</span>
<span class="co">#&gt; Cumulative Proportion    0.097   0.168   0.2297   0.2836   0.3323</span></code></pre></div>
<p>And just by looking at the first two PCs we see information about the class. Here is a random sample of 2,000 digits:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data.frame</span>(<span class="dt">PC1 =</span> pca<span class="op">$</span>x[,<span class="dv">1</span>], <span class="dt">PC2 =</span> pca<span class="op">$</span>x[,<span class="dv">2</span>],
           <span class="dt">label=</span><span class="kw">factor</span>(mnist<span class="op">$</span>train<span class="op">$</span>label)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">sample_n</span>(<span class="dv">2000</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(PC1, PC2, <span class="dt">fill=</span>label))<span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">cex=</span><span class="dv">3</span>, <span class="dt">pch=</span><span class="dv">21</span>)</code></pre></div>
<p><img src="book_files/figure-html/mnist-pca-1-2-scatter-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We can also <em>see</em> the linear combinations on the grid to get an idea of what is getting weighted:</p>
<p><img src="book_files/figure-html/mnist-pca-1-4-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>The lower variance PCs appear related to unimportant variability in the corners:</p>
<p><img src="book_files/figure-html/mnist-pca-last,-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Now let’s apply the transformation we learned with the training data to the test data, reduce the dimension and run knn on just a small number of dimensions.</p>
<p>We try 36 dimensions since this explains about 80% of the data. First fit the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
k &lt;-<span class="st"> </span><span class="dv">36</span>
x_train &lt;-<span class="st"> </span>pca<span class="op">$</span>x[,<span class="dv">1</span><span class="op">:</span>k]
y &lt;-<span class="st"> </span><span class="kw">factor</span>(mnist<span class="op">$</span>train<span class="op">$</span>labels)
fit &lt;-<span class="st"> </span><span class="kw">knn3</span>(x_train, y)</code></pre></div>
<p>Now transform the test set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x_test &lt;-<span class="st"> </span><span class="kw">sweep</span>(mnist<span class="op">$</span>test<span class="op">$</span>images, <span class="dv">2</span>, col_means) <span class="op">%*%</span><span class="st"> </span>pca<span class="op">$</span>rotation
x_test &lt;-<span class="st"> </span>x_test[,<span class="dv">1</span><span class="op">:</span>k]</code></pre></div>
<p>And we are ready to predict and see how we do:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, x_test, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)
<span class="kw">confusionMatrix</span>(y_hat, <span class="kw">factor</span>(mnist<span class="op">$</span>test<span class="op">$</span>labels))<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]
<span class="co">#&gt; Accuracy </span>
<span class="co">#&gt;    0.975</span></code></pre></div>
<p>With just 36 dimensions we get an accuracy well above 0.95.</p>
</div>
</div>
<div id="exercises-58" class="section level2">
<h2><span class="header-section-number">33.6</span> Exercises</h2>
<p>1. We want to explore the <code>tissue_gene_expression</code> predictors by plotting them.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;tissue_gene_expression&quot;</span>)
<span class="kw">dim</span>(tissue_gene_expression<span class="op">$</span>x)</code></pre></div>
<p>We want to get an idea of which observations are close to each other, but the predictors are 500-dimensional so plotting is difficult. Plot the first two principal components with color representing tissue type.</p>
<p>2. The predictors for each observation are measured on the same device and experimental procedure. This introduces biases that can affect all the predictors from one observation. For each observation, compute the average across all predictors and then plot this against the first PC with color representing tissue. Report the correlation.</p>
<p>3. We see an association with the first PC and the observation averages. Redo the PCA but only after removing the center.</p>
<p>4. For the first 10 PCs, make a boxplot showing the values for each tissue.</p>
<p>5. Plot the percent variance explained by PC number. Hint: use the <code>summary</code> function.</p>

</div>
<div id="recommendation-systems" class="section level2">
<h2><span class="header-section-number">33.7</span> Recommendation systems</h2>
<p>Recommendation systems use ratings that <em>users</em> have given <em>items</em> to make specific recommendations. Companies that sell many products to many customers and permit these customers to rate their products, like Amazon, are able to collect massive datasets that can be used to predict what rating a particular user will give a specific item. Items for which a high rating is predicted for a given user are then recommended to that user.</p>
<p>Netflix uses a recommendation system to predict how many <em>stars</em> a user will give a specific movie. One star suggests it is not a good movie, whereas five stars suggests it is an excellent movie. Here, we provide the basics of how these recommendations are made, motivated by some of the approaches taken by the winners of the <em>Netflix challenges</em>.</p>
<p>In October 2006, Netflix offered a challenge to the data science community: improve our recommendation algorithm by 10% and win a million dollars. In September 2009, the winners were announced<a href="#fn109" class="footnoteRef" id="fnref109"><sup>109</sup></a>. You can read a good summary of how the winning algorithm was put together here: <a href="http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/" class="uri">http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/</a> and a more detailed explanation here: <a href="http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf" class="uri">http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf</a>. We will now show you some of the data analysis strategies used by the winning team.</p>
<div id="movielens-data" class="section level3">
<h3><span class="header-section-number">33.7.1</span> Movielens data</h3>
<p>The Netflix data is not publicly available, but the GroupLens research lab<a href="#fn110" class="footnoteRef" id="fnref110"><sup>110</sup></a> generated their own database with over 20 million ratings for over 27,000 movies by more than 138,000 users. We make a small subset of this data available via the <strong>dslabs</strong> package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(dslabs)
<span class="kw">data</span>(<span class="st">&quot;movielens&quot;</span>)</code></pre></div>
<p>We can see this table is in tidy format with thousands of rows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">movielens <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tibble</span>()
<span class="co">#&gt; # A tibble: 100,004 x 7</span>
<span class="co">#&gt;   movieId title              year genres         userId rating timestamp</span>
<span class="co">#&gt;     &lt;int&gt; &lt;chr&gt;             &lt;int&gt; &lt;fct&gt;           &lt;int&gt;  &lt;dbl&gt;     &lt;int&gt;</span>
<span class="co">#&gt; 1      31 Dangerous Minds    1995 Drama               1    2.5    1.26e9</span>
<span class="co">#&gt; 2    1029 Dumbo              1941 Animation|Chi…      1    3      1.26e9</span>
<span class="co">#&gt; 3    1061 Sleepers           1996 Thriller            1    3      1.26e9</span>
<span class="co">#&gt; 4    1129 Escape from New …  1981 Action|Advent…      1    2      1.26e9</span>
<span class="co">#&gt; 5    1172 Cinema Paradiso …  1989 Drama               1    4      1.26e9</span>
<span class="co">#&gt; # … with 1e+05 more rows</span></code></pre></div>
<p>Each row represents a rating given by one user to one movie.</p>
<p>We can see the number of unique users that provided ratings and how many unique movies were rated:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">movielens <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">n_users =</span> <span class="kw">n_distinct</span>(userId),
            <span class="dt">n_movies =</span> <span class="kw">n_distinct</span>(movieId))
<span class="co">#&gt;   n_users n_movies</span>
<span class="co">#&gt; 1     671     9066</span></code></pre></div>
<p>If we multiply those two numbers, we get a number larger than 5 million, yet our data table has about 100,000 rows. This implies that not every user rated every movie. So we can think of these data as a very large matrix, with users on the rows and movies on the columns, with many empty cells. The <code>gather</code> function permits us to convert it to this format, but if we try it for the entire matrix, it will crash R. Let’s show the matrix for seven users and four movies.</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
userId
</th>
<th style="text-align:right;">
Forrest Gump
</th>
<th style="text-align:right;">
Pulp Fiction
</th>
<th style="text-align:right;">
Shawshank Redemption
</th>
<th style="text-align:right;">
Silence of the Lambs
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
5.0
</td>
<td style="text-align:right;">
3.5
</td>
<td style="text-align:right;">
4.5
</td>
<td style="text-align:right;">
NA
</td>
</tr>
<tr>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:right;">
5.0
</td>
<td style="text-align:right;">
2.0
</td>
<td style="text-align:right;">
5.0
</td>
</tr>
<tr>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
4.0
</td>
<td style="text-align:right;">
NA
</td>
</tr>
<tr>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
2.5
</td>
<td style="text-align:right;">
5.0
</td>
<td style="text-align:right;">
5.0
</td>
<td style="text-align:right;">
4.5
</td>
</tr>
<tr>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
5.0
</td>
<td style="text-align:right;">
5.0
</td>
<td style="text-align:right;">
4.0
</td>
<td style="text-align:right;">
3.0
</td>
</tr>
<tr>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
2.0
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
4.5
</td>
<td style="text-align:right;">
0.5
</td>
</tr>
</tbody>
</table>
<p>You can think of the task of a recommendation system as filling in the <code>NA</code>s in the table above. To see how <em>sparse</em> the matrix is, here is the matrix for a random sample of 100 movies and 100 users with yellow indicating a user/movie combination for which we have a rating.</p>
<p><img src="book_files/figure-html/sparsity-of-movie-recs-1.png" width="40%" style="display: block; margin: auto;" /></p>
<p>This machine learning challenge is more complicated than what we have studied up to now because each outcome <span class="math inline">\(Y\)</span> has a different set of predictors. To see this, note that if we are predicting the rating for movie <span class="math inline">\(i\)</span> by user <span class="math inline">\(u\)</span>, in principle, all other ratings related to movie <span class="math inline">\(i\)</span> and by user <span class="math inline">\(u\)</span> may be used as predictors, but different users rate different movies and a different number of movies. Furthermore, we may be able to use information from other movies that we have determined are similar to movie <span class="math inline">\(i\)</span> or from users determined to be similar to user <span class="math inline">\(u\)</span>. In essence, the entire matrix can be used as predictors for each cell.</p>
<p>Let’s look at some of the general properties of the data to better understand the challenges.</p>
<p>The first thing we notice is that some movies get rated more than others. Below is the distribution. This should not surprise us given that there are blockbuster movies watched by millions and artsy, independent movies watched by just a few. Our second observation is that some users are more active than others at rating movies:</p>
<p><img src="book_files/figure-html/movie-id-and-user-hists-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="recommendation-systems-as-a-machine-learning-challenge" class="section level3">
<h3><span class="header-section-number">33.7.2</span> Recommendation systems as a machine learning challenge</h3>
<p>To see how this is a type of machine learning, notice that we need to build an algorithm with data we have collected that will then be applied outside our control, as users look for movie recommendations. So let’s create a test set to assess the accuracy of the models we implement.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
<span class="kw">set.seed</span>(<span class="dv">755</span>)
test_index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y =</span> movielens<span class="op">$</span>rating, <span class="dt">times =</span> <span class="dv">1</span>, <span class="dt">p =</span> <span class="fl">0.2</span>, 
                                  <span class="dt">list =</span> <span class="ot">FALSE</span>)
train_set &lt;-<span class="st"> </span>movielens[<span class="op">-</span>test_index,]
test_set &lt;-<span class="st"> </span>movielens[test_index,]</code></pre></div>
<p>To make sure we don’t include users and movies in the test set that do not appear in the training set, we remove these entries using the <code>semi_join</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">test_set &lt;-<span class="st"> </span>test_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">semi_join</span>(train_set, <span class="dt">by =</span> <span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">semi_join</span>(train_set, <span class="dt">by =</span> <span class="st">&quot;userId&quot;</span>)</code></pre></div>
</div>
<div id="netflix-loss-function" class="section level3">
<h3><span class="header-section-number">33.7.3</span> Loss function</h3>
<p>The Netflix challenge used the typical error loss: they decided on a winner based on the residual mean squared error (RMSE) on a test set. We define <span class="math inline">\(y_{u,i}\)</span> as the rating for movie <span class="math inline">\(i\)</span> by user <span class="math inline">\(u\)</span> and denote our prediction with <span class="math inline">\(\hat{y}_{u,i}\)</span>. The RMSE is then defined as:</p>
<p><span class="math display">\[
\mbox{RMSE} = \sqrt{\frac{1}{N} \sum_{u,i}^{} \left( \hat{y}_{u,i} - y_{u,i} \right)^2 }
\]</span> with <span class="math inline">\(N\)</span> being the number of user/movie combinations and the sum occurring over all these combinations.</p>
<p>Remember that we can interpret the RMSE similarly to a standard deviation: it is the typical error we make when predicting a movie rating. If this number is larger than 1, it means our typical error is larger than one star, which is not good.</p>
<p>Let’s write a function that computes the RMSE for vectors of ratings and their corresponding predictors:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">RMSE &lt;-<span class="st"> </span><span class="cf">function</span>(true_ratings, predicted_ratings){
    <span class="kw">sqrt</span>(<span class="kw">mean</span>((true_ratings <span class="op">-</span><span class="st"> </span>predicted_ratings)<span class="op">^</span><span class="dv">2</span>))
  }</code></pre></div>
</div>
<div id="a-first-model" class="section level3">
<h3><span class="header-section-number">33.7.4</span> A first model</h3>
<p>Let’s start by building the simplest possible recommendation system: we predict the same rating for all movies regardless of user. What number should this prediction be? We can use a model based approach to answer this. A model that assumes the same rating for all movies and users with all the differences explained by random variation would look like this:</p>
<p><span class="math display">\[
Y_{u,i} = \mu + \varepsilon_{u,i}
\]</span></p>
<p>with <span class="math inline">\(\varepsilon_{i,u}\)</span> independent errors sampled from the same distribution centered at 0 and <span class="math inline">\(\mu\)</span> the “true” rating for all movies. We know that the estimate that minimizes the RMSE is the least squares estimate of <span class="math inline">\(\mu\)</span> and, in this case, is the average of all ratings:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu_hat &lt;-<span class="st"> </span><span class="kw">mean</span>(train_set<span class="op">$</span>rating)
mu_hat
<span class="co">#&gt; [1] 3.54</span></code></pre></div>
<p>If we predict all unknown ratings with <span class="math inline">\(\hat{\mu}\)</span> we obtain the following RMSE:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">naive_rmse &lt;-<span class="st"> </span><span class="kw">RMSE</span>(test_set<span class="op">$</span>rating, mu_hat)
naive_rmse
<span class="co">#&gt; [1] 1.05</span></code></pre></div>
<p>Keep in mind that if you plug in any other number, you get a higher RMSE. For example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">predictions &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">3</span>, <span class="kw">nrow</span>(test_set))
<span class="kw">RMSE</span>(test_set<span class="op">$</span>rating, predictions)
<span class="co">#&gt; [1] 1.19</span></code></pre></div>
<p>From looking at the distribution of ratings, we can visualize that this is the standard deviation of that distribution. We get a RMSE of about 1. To win the grand prize of $1,000,000, a participating team had to get an RMSE of about 0.857. So we can definitely do better!</p>
<p>As we go along, we will be comparing different approaches. Let’s start by creating a results table with this naive approach:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rmse_results &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">method =</span> <span class="st">&quot;Just the average&quot;</span>, <span class="dt">RMSE =</span> naive_rmse)</code></pre></div>
</div>
<div id="modeling-movie-effects" class="section level3">
<h3><span class="header-section-number">33.7.5</span> Modeling movie effects</h3>
<p>We know from experience that some movies are just generally rated higher than others. This intuition, that different movies are rated differently, is confirmed by data. We can augment our previous model by adding the term <span class="math inline">\(b_i\)</span> to represent average ranking for movie <span class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
\]</span></p>
<p>Statistics textbooks refer to the <span class="math inline">\(b\)</span>s as effects. However, in the Netflix challenge papers, they refer to them as “bias”, thus the <span class="math inline">\(b\)</span> notation.</p>
<p>We can again use least squares to estimate the <span class="math inline">\(b_i\)</span> in the following way:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span><span class="kw">lm</span>(rating <span class="op">~</span><span class="st"> </span><span class="kw">as.factor</span>(movieId), <span class="dt">data =</span> movielens)</code></pre></div>
<p>Because there are thousands of <span class="math inline">\(b_i\)</span> as each movie gets one, the <code>lm()</code> function will be very slow here. We therefore don’t recommend running the code above. But in this particular situation, we know that the least squares estimate <span class="math inline">\(\hat{b}_i\)</span> is just the average of <span class="math inline">\(Y_{u,i} - \hat{\mu}\)</span> for each movie <span class="math inline">\(i\)</span>. So we can compute them this way (we will drop the <code>hat</code> notation in the code to represent estimates going forward):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu &lt;-<span class="st"> </span><span class="kw">mean</span>(train_set<span class="op">$</span>rating) 
movie_avgs &lt;-<span class="st"> </span>train_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(movieId) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">b_i =</span> <span class="kw">mean</span>(rating <span class="op">-</span><span class="st"> </span>mu))</code></pre></div>
<p>We can see that these estimates vary substantially:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qplot</span>(b_i, <span class="dt">data =</span> movie_avgs, <span class="dt">bins =</span> <span class="dv">10</span>, <span class="dt">color =</span> <span class="kw">I</span>(<span class="st">&quot;black&quot;</span>))</code></pre></div>
<p><img src="book_files/figure-html/movie-effects-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Remember <span class="math inline">\(\hat{\mu}=3.5\)</span> so a <span class="math inline">\(b_i = 1.5\)</span> implies a perfect five star rating.</p>
<p>Let’s see how much our prediction improves once we use <span class="math inline">\(\hat{y}_{u,i} = \hat{\mu} + \hat{b}_i\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">predicted_ratings &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>test_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">left_join</span>(movie_avgs, <span class="dt">by=</span><span class="st">&#39;movieId&#39;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">pull</span>(b_i)
<span class="kw">RMSE</span>(predicted_ratings, test_set<span class="op">$</span>rating)
<span class="co">#&gt; [1] 0.989</span></code></pre></div>
<p>We already see an improvement. But can we make it better?</p>
</div>
<div id="user-effects" class="section level3">
<h3><span class="header-section-number">33.7.6</span> User effects</h3>
<p>Let’s compute the average rating for user <span class="math inline">\(u\)</span> for those that have rated over 100 movies:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(userId) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">b_u =</span> <span class="kw">mean</span>(rating)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">n</span>()<span class="op">&gt;=</span><span class="dv">100</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(b_u)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">30</span>, <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>)</code></pre></div>
<p><img src="book_files/figure-html/user-effect-hist-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Notice that there is substantial variability across users as well: some users are very cranky and others love every movie. This implies that a further improvement to our model may be:</p>
<p><span class="math display">\[ 
Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}
\]</span></p>
<p>where <span class="math inline">\(b_u\)</span> is a user-specific effect. Now if a cranky user (negative <span class="math inline">\(b_u\)</span>) rates a great movie (positive <span class="math inline">\(b_i\)</span>), the effects counter each other and we may be able to correctly predict that this user gave this great movie a 3 rather than a 5.</p>
<p>To fit this model, we could again use <code>lm</code> like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">lm</span>(rating <span class="op">~</span><span class="st"> </span><span class="kw">as.factor</span>(movieId) <span class="op">+</span><span class="st"> </span><span class="kw">as.factor</span>(userId))</code></pre></div>
<p>but, for the reasons described earlier, we won’t. Instead, we will compute an approximation by computing <span class="math inline">\(\hat{\mu}\)</span> and <span class="math inline">\(\hat{b}_i\)</span> and estimating <span class="math inline">\(\hat{b}_u\)</span> as the average of <span class="math inline">\(y_{u,i} - \hat{\mu} - \hat{b}_i\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">user_avgs &lt;-<span class="st"> </span>train_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">left_join</span>(movie_avgs, <span class="dt">by=</span><span class="st">&#39;movieId&#39;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(userId) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">b_u =</span> <span class="kw">mean</span>(rating <span class="op">-</span><span class="st"> </span>mu <span class="op">-</span><span class="st"> </span>b_i))</code></pre></div>
<p>We can now construct predictors and see how much the RMSE improves:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">predicted_ratings &lt;-<span class="st"> </span>test_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">left_join</span>(movie_avgs, <span class="dt">by=</span><span class="st">&#39;movieId&#39;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">left_join</span>(user_avgs, <span class="dt">by=</span><span class="st">&#39;userId&#39;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> mu <span class="op">+</span><span class="st"> </span>b_i <span class="op">+</span><span class="st"> </span>b_u) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">pull</span>(pred)
<span class="kw">RMSE</span>(predicted_ratings, test_set<span class="op">$</span>rating)
<span class="co">#&gt; [1] 0.905</span></code></pre></div>
</div>
</div>
<div id="exercises-59" class="section level2">
<h2><span class="header-section-number">33.8</span> Exercises</h2>
<p>1. Load the <code>movielens</code> data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;movielens&quot;</span>)</code></pre></div>
<p>Compute the number of ratings for each movie and then plot it against the year the movie came out. Use the square root transformation on the counts.</p>
<p>2. We see that, on average, movies that came out after 1993 get more ratings. We also see that with newer movies, starting in 1993, the number of ratings decreases with year: the more recent a movie is, the less time users have had to rate it.</p>
<p>Among movies that came out in 1993 or later, what are the 25 movies with the most ratings per year? Also report their average rating.</p>
<p>3. From the table constructed in the previous example, we see that the most rated movies tend to have above average ratings. This is not surprising: more people watch popular movies. To confirm this, stratify the post 1993 movies by ratings per year and compute their average ratings. Make a plot of average rating versus ratings per year and show an estimate of the trend.</p>
<p>4. In the previous exercise, we see that the more a movie is rated, the higher the rating. Suppose you are doing a predictive analysis in which you need to fill in the missing ratings with some value. Which of the following strategies would you use?</p>
<ol style="list-style-type: lower-alpha">
<li>Fill in the missing values with average rating of all movies.</li>
<li>Fill in the missing values with 0.</li>
<li>Fill in the value with a lower value than the average since lack of rating is associated with lower ratings. Try out different values and evaluate prediction in a test set.</li>
<li>None of the above.</li>
</ol>
<p>5. The <code>movielens</code> dataset also includes a time stamp. This variable represents the time and data in which the rating was provided. The units are seconds since January 1, 1970. Create a new column <code>date</code> with the date. Hint: use the <code>as_datetime</code> function in the <strong>lubridate</strong> package.</p>
<p>6. Compute the average rating for each week and plot this average against day. Hint: use the <code>round_date</code> function before you <code>group_by</code>.</p>
<p>7. The plot shows some evidence of a time effect. If we define <span class="math inline">\(d_{u,i}\)</span> as the day for user’s <span class="math inline">\(u\)</span> rating of movie <span class="math inline">\(i\)</span>, which of the following models is most appropriate:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(Y_{u,i} = \mu + b_i + b_u + d_{u,i} + \varepsilon_{u,i}\)</span>.</li>
<li><span class="math inline">\(Y_{u,i} = \mu + b_i + b_u + d_{u,i}\beta + \varepsilon_{u,i}\)</span>.</li>
<li><span class="math inline">\(Y_{u,i} = \mu + b_i + b_u + d_{u,i}\beta_i + \varepsilon_{u,i}\)</span>.</li>
<li><span class="math inline">\(Y_{u,i} = \mu + b_i + b_u + f(d_{u,i}) + \varepsilon_{u,i}\)</span>, with <span class="math inline">\(f\)</span> a smooth function of <span class="math inline">\(d_{u,i}\)</span>.</li>
</ol>
<p>8. The <code>movielens</code> data also has a <code>genres</code> column. This column includes every genre that applies to the movie. Some movies fall under several genres. Define a category as whatever combination appears in this column. Keep only categories with more than 1,000 ratings. Then compute the average and standard error for each category. Plot these as error bar plots.</p>
<p>9. The plot shows strong evidence of a genre effect. If we define <span class="math inline">\(g_{u,i}\)</span> as the genre for user’s <span class="math inline">\(u\)</span> rating of movie <span class="math inline">\(i\)</span>, which of the following models is most appropriate:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(Y_{u,i} = \mu + b_i + b_u + d_{u,i} + \varepsilon_{u,i}\)</span>.</li>
<li><span class="math inline">\(Y_{u,i} = \mu + b_i + b_u + d_{u,i}\beta + \varepsilon_{u,i}\)</span>.</li>
<li><span class="math inline">\(Y_{u,i} = \mu + b_i + b_u + \sum_{k=1}^K x_{u,i} \beta_k + \varepsilon_{u,i}\)</span>, with <span class="math inline">\(x^k_{u,i} = 1\)</span> if <span class="math inline">\(g_{u,i}\)</span> is genre <span class="math inline">\(k\)</span>.</li>
<li><span class="math inline">\(Y_{u,i} = \mu + b_i + b_u + f(d_{u,i}) + \varepsilon_{u,i}\)</span>, with <span class="math inline">\(f\)</span> a smooth function of <span class="math inline">\(d_{u,i}\)</span>.</li>
</ol>
</div>
<div id="regularization" class="section level2">
<h2><span class="header-section-number">33.9</span> Regularization</h2>
<div id="motivation" class="section level3">
<h3><span class="header-section-number">33.9.1</span> Motivation</h3>
<p>Despite the large movie to movie variation, our improvement in RMSE was only about 5%. Let’s explore where we made mistakes in our first model, using only movie effects <span class="math inline">\(b_i\)</span>. Here are the 10 largest mistakes:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">test_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">left_join</span>(movie_avgs, <span class="dt">by=</span><span class="st">&#39;movieId&#39;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">residual =</span> rating <span class="op">-</span><span class="st"> </span>(mu <span class="op">+</span><span class="st"> </span>b_i)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(<span class="kw">abs</span>(residual))) <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>(title)
<span class="co">#&gt;  [1] &quot;Kingdom, The (Riget)&quot;            &quot;Heaven Knows, Mr. Allison&quot;      </span>
<span class="co">#&gt;  [3] &quot;American Pimp&quot;                   &quot;Chinatown&quot;                      </span>
<span class="co">#&gt;  [5] &quot;American Beauty&quot;                 &quot;Apocalypse Now&quot;                 </span>
<span class="co">#&gt;  [7] &quot;Taxi Driver&quot;                     &quot;Wallace &amp; Gromit: A Close Shave&quot;</span>
<span class="co">#&gt;  [9] &quot;Down in the Delta&quot;               &quot;Stalag 17&quot;</span></code></pre></div>
<p>These all seem like obscure movies. Many of them have large predictions. Let’s look at the top 10 worst and best movies based on <span class="math inline">\(\hat{b}_i\)</span>. First, let’s create a database that connects <code>movieId</code> to movie title:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">movie_titles &lt;-<span class="st"> </span>movielens <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(movieId, title) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">distinct</span>()</code></pre></div>
<p>Here are the 10 best movies according to our estimate:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">movie_avgs <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">left_join</span>(movie_titles, <span class="dt">by=</span><span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(b_i)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)  <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>(title)
<span class="co">#&gt;  [1] &quot;When Night Is Falling&quot;                                  </span>
<span class="co">#&gt;  [2] &quot;Lamerica&quot;                                               </span>
<span class="co">#&gt;  [3] &quot;Mute Witness&quot;                                           </span>
<span class="co">#&gt;  [4] &quot;Picture Bride (Bijo photo)&quot;                             </span>
<span class="co">#&gt;  [5] &quot;Red Firecracker, Green Firecracker (Pao Da Shuang Deng)&quot;</span>
<span class="co">#&gt;  [6] &quot;Paris, France&quot;                                          </span>
<span class="co">#&gt;  [7] &quot;Faces&quot;                                                  </span>
<span class="co">#&gt;  [8] &quot;Maya Lin: A Strong Clear Vision&quot;                        </span>
<span class="co">#&gt;  [9] &quot;Heavy&quot;                                                  </span>
<span class="co">#&gt; [10] &quot;Gate of Heavenly Peace, The&quot;</span></code></pre></div>
<p>And here are the 10 worst:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">movie_avgs <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">left_join</span>(movie_titles, <span class="dt">by=</span><span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(b_i) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)  <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>(title)
<span class="co">#&gt;  [1] &quot;Children of the Corn IV: The Gathering&quot;           </span>
<span class="co">#&gt;  [2] &quot;Barney&#39;s Great Adventure&quot;                         </span>
<span class="co">#&gt;  [3] &quot;Merry War, A&quot;                                     </span>
<span class="co">#&gt;  [4] &quot;Whiteboyz&quot;                                        </span>
<span class="co">#&gt;  [5] &quot;Catfish in Black Bean Sauce&quot;                      </span>
<span class="co">#&gt;  [6] &quot;Killer Shrews, The&quot;                               </span>
<span class="co">#&gt;  [7] &quot;Horrors of Spider Island (Ein Toter Hing im Netz)&quot;</span>
<span class="co">#&gt;  [8] &quot;Monkeybone&quot;                                       </span>
<span class="co">#&gt;  [9] &quot;Arthur 2: On the Rocks&quot;                           </span>
<span class="co">#&gt; [10] &quot;Red Heat&quot;</span></code></pre></div>
<p>They all seem to be quite obscure. Let’s look at how often they are rated.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_set <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(movieId) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">left_join</span>(movie_avgs, <span class="dt">by=</span><span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">left_join</span>(movie_titles, <span class="dt">by=</span><span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(b_i)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>(n)
<span class="co">#&gt;  [1] 1 1 1 1 3 1 1 2 1 1</span>
 
train_set <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(movieId) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">left_join</span>(movie_avgs) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">left_join</span>(movie_titles, <span class="dt">by=</span><span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(b_i) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>(n)
<span class="co">#&gt; Joining, by = &quot;movieId&quot;</span>
<span class="co">#&gt;  [1] 1 1 1 1 1 1 1 1 1 1</span></code></pre></div>
<p>The supposed “best” and “worst” movies were rated by very few users, in most cases just 1. These movies were mostly obscure ones. This is because with just a few users, we have more uncertainty. Therefore, larger estimates of <span class="math inline">\(b_i\)</span>, negative or positive, are more likely.</p>
<p>These are noisy estimates that we should not trust, especially when it comes to prediction. Large errors can increase our RMSE, so we would rather be conservative when unsure.</p>
<p>In previous sections, we computed standard error and constructed confidence intervals to account for different levels of uncertainty. However, when making predictions, we need one number, one prediction, not an interval. For this, we introduce the concept of regularization.</p>
<p>Regularization permits us to penalize large estimates that are formed using small sample sizes. It has commonalities with the Bayesian approach that shrunk predictions described in Section <a href="models.html#bayesian-statistics">16.4</a>.</p>
</div>
<div id="penalized-least-squares" class="section level3">
<h3><span class="header-section-number">33.9.2</span> Penalized least squares</h3>
<p>The general idea behind regularization is to constrain the total variability of the effect sizes. Why does this help? Consider a case in which we have movie <span class="math inline">\(i=1\)</span> with 100 user ratings and 4 movies <span class="math inline">\(i=2,3,4,5\)</span> with just one user rating. We intend to fit the model</p>
<p><span class="math display">\[
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
\]</span></p>
<p>Suppose we know the average rating is, say, <span class="math inline">\(\mu = 3\)</span>. If we use least squares, the estimate for the first movie effect <span class="math inline">\(b_1\)</span> is the average of the 100 user ratings, <span class="math inline">\(1/100 \sum_{i=1}^{100} (Y_{i,1} - \mu)\)</span>, which we expect to be a quite precise. However, the estimate for movies 2, 3, 4, and 5 will simply be the observed deviation from the average rating <span class="math inline">\(\hat{b}_i = Y_{u,i} - \hat{\mu}\)</span> which is an estimate based on just one number so it won’t be precise at all. Note these estimates make the error <span class="math inline">\(Y_{u,i} - \mu + \hat{b}_i\)</span> equal to 0 for <span class="math inline">\(i=2,3,4,5\)</span>, but this is a case of over-training. In fact, ignoring the one user and guessing that movies 2,3,4, and 5 are just average movies (<span class="math inline">\(b_i = 0\)</span>) might provide a better prediction. The general idea of penalized regression is to control the total variability of the movie effects: <span class="math inline">\(\sum_{i=1}^5 b_i^2\)</span>. Specifically, instead of minimizing the least squares equation, we minimize an equation that adds a penalty:</p>
<p><span class="math display">\[\frac{1}{N} \sum_{u,i} \left(y_{u,i} - \mu - b_i\right)^2 + \lambda \sum_{i} b_i^2\]</span> The first term is just least squares and the second is a penalty that gets larger when many <span class="math inline">\(b_i\)</span> are large. Using calculus we can actually show that the values of <span class="math inline">\(b_i\)</span> that minimize this equation are:</p>
<p><span class="math display">\[
\hat{b}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)
\]</span></p>
<p>where <span class="math inline">\(n_i\)</span> is the number of ratings made for movie <span class="math inline">\(i\)</span>. This approach will have our desired effect: when our sample size <span class="math inline">\(n_i\)</span> is very large, a case which will give us a stable estimate, then the penalty <span class="math inline">\(\lambda\)</span> is effectively ignored since <span class="math inline">\(n_i+\lambda \approx n_i\)</span>. However, when the <span class="math inline">\(n_i\)</span> is small, then the estimate <span class="math inline">\(\hat{b}_i(\lambda)\)</span> is shrunken towards 0. The larger <span class="math inline">\(\lambda\)</span>, the more we shrink.</p>
<p>Let’s compute these regularized estimates of <span class="math inline">\(b_i\)</span> using <span class="math inline">\(\lambda=3\)</span>. Later, we will see why we picked 3.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lambda &lt;-<span class="st"> </span><span class="dv">3</span>
mu &lt;-<span class="st"> </span><span class="kw">mean</span>(train_set<span class="op">$</span>rating)
movie_reg_avgs &lt;-<span class="st"> </span>train_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(movieId) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">b_i =</span> <span class="kw">sum</span>(rating <span class="op">-</span><span class="st"> </span>mu)<span class="op">/</span>(<span class="kw">n</span>()<span class="op">+</span>lambda), <span class="dt">n_i =</span> <span class="kw">n</span>()) </code></pre></div>
<p>To see how the estimates shrink, let’s make a plot of the regularized estimates versus the least squares estimates.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tibble</span>(<span class="dt">original =</span> movie_avgs<span class="op">$</span>b_i, 
       <span class="dt">regularlized =</span> movie_reg_avgs<span class="op">$</span>b_i, 
       <span class="dt">n =</span> movie_reg_avgs<span class="op">$</span>n_i) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(original, regularlized, <span class="dt">size=</span><span class="kw">sqrt</span>(n))) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">shape=</span><span class="dv">1</span>, <span class="dt">alpha=</span><span class="fl">0.5</span>)</code></pre></div>
<p><img src="book_files/figure-html/regularization-shrinkage-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Now, let’s look at the top 10 best movies based on the penalized estimates <span class="math inline">\(\hat{b}_i(\lambda)\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_set <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">count</span>(movieId) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">left_join</span>(movie_reg_avgs, <span class="dt">by =</span> <span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">left_join</span>(movie_titles, <span class="dt">by =</span> <span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(b_i)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>(title)
<span class="co">#&gt;  [1] &quot;Paris Is Burning&quot;          &quot;Shawshank Redemption, The&quot;</span>
<span class="co">#&gt;  [3] &quot;Godfather, The&quot;            &quot;African Queen, The&quot;       </span>
<span class="co">#&gt;  [5] &quot;Band of Brothers&quot;          &quot;Paperman&quot;                 </span>
<span class="co">#&gt;  [7] &quot;On the Waterfront&quot;         &quot;All About Eve&quot;            </span>
<span class="co">#&gt;  [9] &quot;Usual Suspects, The&quot;       &quot;Ikiru&quot;</span></code></pre></div>
<p>These make much more sense! These movies are watched more and have more ratings. Here are the top 10 worst movies:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_set <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">count</span>(movieId) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">left_join</span>(movie_reg_avgs, <span class="dt">by =</span> <span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">left_join</span>(movie_titles, <span class="dt">by=</span><span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(b_i) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(title, b_i, n) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>(title)
<span class="co">#&gt;  [1] &quot;Battlefield Earth&quot;                      </span>
<span class="co">#&gt;  [2] &quot;Joe&#39;s Apartment&quot;                        </span>
<span class="co">#&gt;  [3] &quot;Super Mario Bros.&quot;                      </span>
<span class="co">#&gt;  [4] &quot;Speed 2: Cruise Control&quot;                </span>
<span class="co">#&gt;  [5] &quot;Dungeons &amp; Dragons&quot;                     </span>
<span class="co">#&gt;  [6] &quot;Batman &amp; Robin&quot;                         </span>
<span class="co">#&gt;  [7] &quot;Police Academy 6: City Under Siege&quot;     </span>
<span class="co">#&gt;  [8] &quot;Cats &amp; Dogs&quot;                            </span>
<span class="co">#&gt;  [9] &quot;Disaster Movie&quot;                         </span>
<span class="co">#&gt; [10] &quot;Mighty Morphin Power Rangers: The Movie&quot;</span></code></pre></div>
<p>Do we improve our results?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">predicted_ratings &lt;-<span class="st"> </span>test_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">left_join</span>(movie_reg_avgs, <span class="dt">by =</span> <span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> mu <span class="op">+</span><span class="st"> </span>b_i) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">pull</span>(pred)
<span class="kw">RMSE</span>(predicted_ratings, test_set<span class="op">$</span>rating)
<span class="co">#&gt; [1] 0.97</span></code></pre></div>
<pre><code>#&gt; # A tibble: 4 x 2
#&gt;   method                          RMSE
#&gt;   &lt;chr&gt;                          &lt;dbl&gt;
#&gt; 1 Just the average               1.05 
#&gt; 2 Movie Effect Model             0.989
#&gt; 3 Movie + User Effects Model     0.905
#&gt; 4 Regularized Movie Effect Model 0.970</code></pre>
<p>The penalized estimates provide a large improvement over the least squares estimates.</p>
</div>
<div id="choosing-the-penalty-terms" class="section level3">
<h3><span class="header-section-number">33.9.3</span> Choosing the penalty terms</h3>
<p>Note that <span class="math inline">\(\lambda\)</span> is a tuning parameter. We can use cross-validation to choose it.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lambdas &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="fl">0.25</span>)

mu &lt;-<span class="st"> </span><span class="kw">mean</span>(train_set<span class="op">$</span>rating)
just_the_sum &lt;-<span class="st"> </span>train_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(movieId) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">s =</span> <span class="kw">sum</span>(rating <span class="op">-</span><span class="st"> </span>mu), <span class="dt">n_i =</span> <span class="kw">n</span>())

rmses &lt;-<span class="st"> </span><span class="kw">sapply</span>(lambdas, <span class="cf">function</span>(l){
  predicted_ratings &lt;-<span class="st"> </span>test_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">left_join</span>(just_the_sum, <span class="dt">by=</span><span class="st">&#39;movieId&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">b_i =</span> s<span class="op">/</span>(n_i<span class="op">+</span>l)) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">pred =</span> mu <span class="op">+</span><span class="st"> </span>b_i) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">pull</span>(pred)
  <span class="kw">return</span>(<span class="kw">RMSE</span>(predicted_ratings, test_set<span class="op">$</span>rating))
})
<span class="kw">qplot</span>(lambdas, rmses)  
lambdas[<span class="kw">which.min</span>(rmses)]
<span class="co">#&gt; [1] 3</span></code></pre></div>
<p><img src="book_files/figure-html/best-penalty-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>However, while we show this as an illustration, in practice we should be using full cross-validation just on the train set, without using the test set until the final assessment. The test set should never be used for tuning.</p>
<p>We can use regularization for the estimate user effects as well. We are minimizing:</p>
<p><span class="math display">\[
\frac{1}{N} \sum_{u,i} \left(y_{u,i} - \mu - b_i - b_u \right)^2 + 
\lambda \left(\sum_{i} b_i^2 + \sum_{u} b_u^2\right)
\]</span></p>
<p>The estimates that minimize this can be found similarly to what we did above. Here we use cross-validation to pick a <span class="math inline">\(\lambda\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lambdas &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="fl">0.25</span>)

rmses &lt;-<span class="st"> </span><span class="kw">sapply</span>(lambdas, <span class="cf">function</span>(l){

  mu &lt;-<span class="st"> </span><span class="kw">mean</span>(train_set<span class="op">$</span>rating)
  
  b_i &lt;-<span class="st"> </span>train_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">group_by</span>(movieId) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">summarize</span>(<span class="dt">b_i =</span> <span class="kw">sum</span>(rating <span class="op">-</span><span class="st"> </span>mu)<span class="op">/</span>(<span class="kw">n</span>()<span class="op">+</span>l))
  
  b_u &lt;-<span class="st"> </span>train_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">left_join</span>(b_i, <span class="dt">by=</span><span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">group_by</span>(userId) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">summarize</span>(<span class="dt">b_u =</span> <span class="kw">sum</span>(rating <span class="op">-</span><span class="st"> </span>b_i <span class="op">-</span><span class="st"> </span>mu)<span class="op">/</span>(<span class="kw">n</span>()<span class="op">+</span>l))

  predicted_ratings &lt;-<span class="st"> </span>
<span class="st">    </span>test_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">left_join</span>(b_i, <span class="dt">by =</span> <span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">left_join</span>(b_u, <span class="dt">by =</span> <span class="st">&quot;userId&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">pred =</span> mu <span class="op">+</span><span class="st"> </span>b_i <span class="op">+</span><span class="st"> </span>b_u) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">pull</span>(pred)
  
    <span class="kw">return</span>(<span class="kw">RMSE</span>(predicted_ratings, test_set<span class="op">$</span>rating))
})

<span class="kw">qplot</span>(lambdas, rmses)  </code></pre></div>
<p><img src="book_files/figure-html/best-lambdas-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>For the full model, the optimal <span class="math inline">\(\lambda\)</span> is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lambda &lt;-<span class="st"> </span>lambdas[<span class="kw">which.min</span>(rmses)]
lambda
<span class="co">#&gt; [1] 3.25</span></code></pre></div>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
method
</th>
<th style="text-align:right;">
RMSE
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Just the average
</td>
<td style="text-align:right;">
1.053
</td>
</tr>
<tr>
<td style="text-align:left;">
Movie Effect Model
</td>
<td style="text-align:right;">
0.989
</td>
</tr>
<tr>
<td style="text-align:left;">
Movie + User Effects Model
</td>
<td style="text-align:right;">
0.905
</td>
</tr>
<tr>
<td style="text-align:left;">
Regularized Movie Effect Model
</td>
<td style="text-align:right;">
0.970
</td>
</tr>
<tr>
<td style="text-align:left;">
Regularized Movie + User Effect Model
</td>
<td style="text-align:right;">
0.881
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="exercises-60" class="section level2">
<h2><span class="header-section-number">33.10</span> Exercises</h2>
<p>An education expert is advocating for smaller schools. The expert bases this recommendation on the fact that among the best performing schools, many are small schools. Let’s simulate a dataset for 100 schools. First, let’s simulate the number of students in each school.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1986</span>)
n &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="dv">2</span><span class="op">^</span><span class="kw">rnorm</span>(<span class="dv">1000</span>, <span class="dv">8</span>, <span class="dv">1</span>))</code></pre></div>
<p>Now let’s assign a <em>true</em> quality for each school completely independent from size. This is the parameter we want to estimate.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="dv">80</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">rt</span>(<span class="dv">1000</span>, <span class="dv">5</span>))
<span class="kw">range</span>(mu)
schools &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">id =</span> <span class="kw">paste</span>(<span class="st">&quot;PS&quot;</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">100</span>), 
                      <span class="dt">size =</span> n, 
                      <span class="dt">quality =</span> mu,
                      <span class="dt">rank =</span> <span class="kw">rank</span>(<span class="op">-</span>mu))</code></pre></div>
<p>We can see that the top 10 schools are:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">schools <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">top_n</span>(<span class="dv">10</span>, quality) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(<span class="kw">desc</span>(quality))</code></pre></div>
<p>Now let’s have the students in the school take a test. There is random variability in test taking so we will simulate the test scores as normally distributed with the average determined by the school quality and standard deviations of 30 percentage points:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">scores &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(schools), <span class="cf">function</span>(i){
  scores &lt;-<span class="st"> </span><span class="kw">rnorm</span>(schools<span class="op">$</span>size[i], schools<span class="op">$</span>quality[i], <span class="dv">30</span>)
  scores
})
schools &lt;-<span class="st"> </span>schools <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">score =</span> <span class="kw">sapply</span>(scores, mean))</code></pre></div>
<p>1. What are the top schools based on the average score? Show just the ID, size, and the average score.</p>
<p>2. Compare the median school size to the median school size of the top 10 schools based on the score.</p>
<p>3. According to this test, it appears small schools are better than large schools. Five out of the top 10 schools have 100 or fewer students. But how can this be? We constructed the simulation so that quality and size are independent. Repeat the exercise for the worst 10 schools.</p>
<p>4. The same is true for the worst schools! They are small as well. Plot the average score versus school size to see what’s going on. Highlight the top 10 schools based on the <em>true</em> quality. Use the log scale transform for the size.</p>
<p>5. We can see that the standard error of the score has larger variability when the school is smaller. This is a basic statistical reality we learned in the probability and inference sections. In fact, note that 4 of the top 10 schools are in the top 10 schools based on the exam score.</p>
<p>Let’s use regularization to pick the best schools. Remember regularization <em>shrinks</em> deviations from the average towards 0. So to apply regularization here, we first need to define the overall average for all schools:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">overall &lt;-<span class="st"> </span><span class="kw">mean</span>(<span class="kw">sapply</span>(scores, mean))</code></pre></div>
<p>and then define, for each school, how it deviates from that average. Write code that estimates the score above average for each school but dividing by <span class="math inline">\(n + \lambda\)</span> instead of <span class="math inline">\(n\)</span>, with <span class="math inline">\(n\)</span> the school size and <span class="math inline">\(\lambda\)</span> a regularization parameter. Try <span class="math inline">\(\lambda = 3\)</span>.</p>
<p>6. Notice that this improves things a bit. The number of small schools that are not highly ranked is now 4. Is there a better <span class="math inline">\(\lambda\)</span>? Find the <span class="math inline">\(\lambda\)</span> that minimizes the RMSE = <span class="math inline">\(1/100 \sum_{i=1}^{100} (\mbox{quality} - \mbox{estimate})^2\)</span>.</p>
<p>7. Rank the schools based on the average obtained with the best <span class="math inline">\(\alpha\)</span>. Note that no small school is incorrectly included.</p>
<p>8. A common mistake to make when using regularization is shrinking values towards 0 that are not centered around 0. For example, if we don’t subtract the overall average before shrinking, we actually obtain a very similar result. Confirm this by re-running the code from exercise 6 but without removing the overall mean.</p>
</div>
<div id="matrix-factorization" class="section level2">
<h2><span class="header-section-number">33.11</span> Matrix factorization</h2>
<p>Matrix factorization is a widely used concept in machine learning. It is very much related to factor analysis, singular value decomposition (SVD), and principal component analysis (PCA). Here we describe the concept in the context of movie recommendation systems.</p>
<p>We have described how the model:</p>
<p><span class="math display">\[ 
Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}
\]</span></p>
<p>accounts for movie to movie differences through the <span class="math inline">\(b_i\)</span> and user to user differences through the <span class="math inline">\(b_u\)</span>. But this model leaves out an important source of variation related to the fact that groups of movies have similar rating patterns and groups of users have similar rating patterns as well. We will discover these patterns by studying the residuals:</p>
<p><span class="math display">\[
r_{u,i} = y_{u,i} - \hat{b}_i - \hat{b}_u
\]</span></p>
<p>To see this, we will convert the data into a matrix so that each user gets a row, each movie gets a column, and <span class="math inline">\(y_{u,i}\)</span> is the entry in row <span class="math inline">\(u\)</span> and column <span class="math inline">\(i\)</span>. For illustrative purposes, we will only consider a small subset of movies with many ratings and users that have rated many movies. We also keep Scent of a Woman (<code>movieId == 3252</code>) because we use it for a specific example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_small &lt;-<span class="st"> </span>movielens <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(movieId) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">n</span>() <span class="op">&gt;=</span><span class="st"> </span><span class="dv">50</span> <span class="op">|</span><span class="st"> </span>movieId <span class="op">==</span><span class="st"> </span><span class="dv">3252</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(userId) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">n</span>() <span class="op">&gt;=</span><span class="st"> </span><span class="dv">50</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ungroup</span>()

y &lt;-<span class="st"> </span>train_small <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(userId, movieId, rating) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spread</span>(movieId, rating) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as.matrix</span>()</code></pre></div>
<p>We add row names and column names:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rownames</span>(y)&lt;-<span class="st"> </span>y[,<span class="dv">1</span>]
y &lt;-<span class="st"> </span>y[,<span class="op">-</span><span class="dv">1</span>]

movie_titles &lt;-<span class="st"> </span>movielens <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(movieId, title) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">distinct</span>()

<span class="kw">colnames</span>(y) &lt;-<span class="st"> </span><span class="kw">with</span>(movie_titles, title[<span class="kw">match</span>(<span class="kw">colnames</span>(y), movieId)])</code></pre></div>
<p>and convert them to residuals by removing the column and row effects:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y &lt;-<span class="st"> </span><span class="kw">sweep</span>(y, <span class="dv">2</span>, <span class="kw">colMeans</span>(y, <span class="dt">na.rm=</span><span class="ot">TRUE</span>))
y &lt;-<span class="st"> </span><span class="kw">sweep</span>(y, <span class="dv">1</span>, <span class="kw">rowMeans</span>(y, <span class="dt">na.rm=</span><span class="ot">TRUE</span>))</code></pre></div>
<p>If the model above explains all the signals, and the <span class="math inline">\(\varepsilon\)</span> are just noise, then the residuals for different movies should be independent from each other. But they are not. Here are some examples:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m_<span class="dv">1</span> &lt;-<span class="st"> &quot;Godfather, The&quot;</span>
m_<span class="dv">2</span> &lt;-<span class="st"> &quot;Godfather: Part II, The&quot;</span>
p1 &lt;-<span class="st"> </span><span class="kw">qplot</span>(y[ ,m_<span class="dv">1</span>], y[,m_<span class="dv">2</span>], <span class="dt">xlab =</span> m_<span class="dv">1</span>, <span class="dt">ylab =</span> m_<span class="dv">2</span>)

m_<span class="dv">1</span> &lt;-<span class="st"> &quot;Godfather, The&quot;</span>
m_<span class="dv">3</span> &lt;-<span class="st"> &quot;Goodfellas&quot;</span>
p2 &lt;-<span class="st"> </span><span class="kw">qplot</span>(y[ ,m_<span class="dv">1</span>], y[,m_<span class="dv">3</span>], <span class="dt">xlab =</span> m_<span class="dv">1</span>, <span class="dt">ylab =</span> m_<span class="dv">3</span>)

m_<span class="dv">4</span> &lt;-<span class="st"> &quot;You&#39;ve Got Mail&quot;</span> 
m_<span class="dv">5</span> &lt;-<span class="st"> &quot;Sleepless in Seattle&quot;</span> 
p3 &lt;-<span class="st"> </span><span class="kw">qplot</span>(y[ ,m_<span class="dv">4</span>], y[,m_<span class="dv">5</span>], <span class="dt">xlab =</span> m_<span class="dv">4</span>, <span class="dt">ylab =</span> m_<span class="dv">5</span>)

gridExtra<span class="op">::</span><span class="kw">grid.arrange</span>(p1, p2 ,p3, <span class="dt">ncol =</span> <span class="dv">3</span>)</code></pre></div>
<p><img src="book_files/figure-html/movie-cor-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>This plot says that users that liked The Godfather more than what the model expects them to, based on the movie and user effects, also liked The Godfather II more than expected. A similar relationship is seen when comparing The Godfather and Goodfellas. Although not as strong, there is still correlation. We see correlations between You’ve Got Mail and Sleepless in Seattle as well</p>
<p>By looking at the correlation between movies, we can see a pattern (we rename the columns to save print space):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span>y[, <span class="kw">c</span>(m_<span class="dv">1</span>, m_<span class="dv">2</span>, m_<span class="dv">3</span>, m_<span class="dv">4</span>, m_<span class="dv">5</span>)]
short_names &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Godfather&quot;</span>, <span class="st">&quot;Godfather2&quot;</span>, <span class="st">&quot;Goodfellas&quot;</span>,
                 <span class="st">&quot;You&#39;ve Got&quot;</span>, <span class="st">&quot;Sleepless&quot;</span>)
<span class="kw">colnames</span>(x) &lt;-<span class="st"> </span>short_names
<span class="kw">cor</span>(x, <span class="dt">use=</span><span class="st">&quot;pairwise.complete&quot;</span>)
<span class="co">#&gt;            Godfather Godfather2 Goodfellas You&#39;ve Got Sleepless</span>
<span class="co">#&gt; Godfather      1.000      0.829      0.444     -0.440    -0.378</span>
<span class="co">#&gt; Godfather2     0.829      1.000      0.521     -0.331    -0.358</span>
<span class="co">#&gt; Goodfellas     0.444      0.521      1.000     -0.481    -0.402</span>
<span class="co">#&gt; You&#39;ve Got    -0.440     -0.331     -0.481      1.000     0.533</span>
<span class="co">#&gt; Sleepless     -0.378     -0.358     -0.402      0.533     1.000</span></code></pre></div>
<p>There seems to be people that like romantic comedies more than expected, while others that like gangster movies more than expected.</p>
<p>These results tell us that there is structure in the data. But how can we model this?</p>
<div id="factors-analysis" class="section level3">
<h3><span class="header-section-number">33.11.1</span> Factors analysis</h3>
<p>Here is an illustration, using a simulation, of how we can use some structure to predict the <span class="math inline">\(r_{u,i}\)</span>. Suppose our residuals <code>r</code> look like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(r, <span class="dv">1</span>)
<span class="co">#&gt;    Godfather Godfather2 Goodfellas You&#39;ve Got Sleepless</span>
<span class="co">#&gt; 1        2.0        2.3        2.2       -1.8      -1.9</span>
<span class="co">#&gt; 2        2.0        1.7        2.0       -1.9      -1.7</span>
<span class="co">#&gt; 3        1.9        2.4        2.1       -2.3      -2.0</span>
<span class="co">#&gt; 4       -0.3        0.3        0.3       -0.4      -0.3</span>
<span class="co">#&gt; 5       -0.3       -0.4        0.3        0.2       0.3</span>
<span class="co">#&gt; 6       -0.1        0.1        0.2       -0.3       0.2</span>
<span class="co">#&gt; 7       -0.1        0.0       -0.2       -0.2       0.3</span>
<span class="co">#&gt; 8        0.2        0.2        0.1        0.0       0.4</span>
<span class="co">#&gt; 9       -1.7       -2.1       -1.8        2.0       2.4</span>
<span class="co">#&gt; 10      -2.3       -1.8       -1.7        1.8       1.7</span>
<span class="co">#&gt; 11      -1.7       -2.0       -2.1        1.9       2.3</span>
<span class="co">#&gt; 12      -1.8       -1.7       -2.1        2.3       2.0</span></code></pre></div>
<p>There seems to be a pattern here. In fact, we can see very strong correlation patterns:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(r) 
<span class="co">#&gt;            Godfather Godfather2 Goodfellas You&#39;ve Got Sleepless</span>
<span class="co">#&gt; Godfather      1.000      0.980      0.978     -0.974    -0.966</span>
<span class="co">#&gt; Godfather2     0.980      1.000      0.983     -0.987    -0.992</span>
<span class="co">#&gt; Goodfellas     0.978      0.983      1.000     -0.986    -0.989</span>
<span class="co">#&gt; You&#39;ve Got    -0.974     -0.987     -0.986      1.000     0.986</span>
<span class="co">#&gt; Sleepless     -0.966     -0.992     -0.989      0.986     1.000</span></code></pre></div>
<p>We can create vectors <code>q</code> and <code>p</code>, that can explain much of the structure we see. The <code>q</code> would look like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t</span>(q) 
<span class="co">#&gt;      Godfather Godfather2 Goodfellas You&#39;ve Got Sleepless</span>
<span class="co">#&gt; [1,]         1          1          1         -1        -1</span></code></pre></div>
<p>and it narrows down movies to two groups: gangster (coded with 1) and romance (coded with -1). We can also reduce the users to three groups:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t</span>(p)
<span class="co">#&gt;      1 2 3 4 5 6 7 8  9 10 11 12</span>
<span class="co">#&gt; [1,] 2 2 2 0 0 0 0 0 -2 -2 -2 -2</span></code></pre></div>
<p>those that like gangster movies and dislike romance movies (coded as 2), those that like romance movies and dislike gangster movies (coded as -2), and those that don’t care (coded as 0). The main point here is that we can almost reconstruct <span class="math inline">\(r\)</span>, which has 60 values, with a couple of vectors totaling 17 values. If <span class="math inline">\(r\)</span> contains the residuals for users <span class="math inline">\(u=1,\dots,12\)</span> for movies <span class="math inline">\(i=1,\dots,5\)</span> we can write the following mathematical formula for our residuals <span class="math inline">\(r_{u,i}\)</span>.</p>
<p><span class="math display">\[
r_{u,i} \approx p_u q_i 
\]</span></p>
<p>This implies that we can explain more variability by modifying our previous model for movie recommendations to:</p>
<p><span class="math display">\[
Y_{u,i} = \mu + b_i + b_u + p_u q_i + \varepsilon_{u,i}
\]</span></p>
<p>However, we motivated the need for the <span class="math inline">\(p_u q_i\)</span> term with a simple simulation. The structure found in data is usually more complex. For example, in this first simulation we assumed there were was just one factor <span class="math inline">\(p_u\)</span> that determined which of the two genres movie <span class="math inline">\(u\)</span> belongs to. But the structure in our movie data seems to be much more complicated than gangster movie versus romance. We may have many other factors. Here we present a slightly more complex simulation. We now add a sixth movie.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(r, <span class="dv">1</span>)
<span class="co">#&gt;    Godfather Godfather2 Goodfellas You&#39;ve Got Sleepless Scent</span>
<span class="co">#&gt; 1        0.5        0.6        1.6       -0.5      -0.5  -1.6</span>
<span class="co">#&gt; 2        1.5        1.4        0.5       -1.5      -1.4  -0.4</span>
<span class="co">#&gt; 3        1.5        1.6        0.5       -1.6      -1.5  -0.5</span>
<span class="co">#&gt; 4       -0.1        0.1        0.1       -0.1      -0.1   0.1</span>
<span class="co">#&gt; 5       -0.1       -0.1        0.1        0.0       0.1  -0.1</span>
<span class="co">#&gt; 6        0.5        0.5       -0.4       -0.6      -0.5   0.5</span>
<span class="co">#&gt; 7        0.5        0.5       -0.5       -0.6      -0.4   0.4</span>
<span class="co">#&gt; 8        0.5        0.6       -0.5       -0.5      -0.4   0.4</span>
<span class="co">#&gt; 9       -0.9       -1.0       -0.9        1.0       1.1   0.9</span>
<span class="co">#&gt; 10      -1.6       -1.4       -0.4        1.5       1.4   0.5</span>
<span class="co">#&gt; 11      -1.4       -1.5       -0.5        1.5       1.6   0.6</span>
<span class="co">#&gt; 12      -1.4       -1.4       -0.5        1.6       1.5   0.6</span></code></pre></div>
<p>By exploring the correlation structure of this new dataset</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">colnames</span>(r)[<span class="dv">4</span><span class="op">:</span><span class="dv">6</span>] &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;YGM&quot;</span>, <span class="st">&quot;SS&quot;</span>, <span class="st">&quot;SW&quot;</span>)
<span class="kw">cor</span>(r)
<span class="co">#&gt;            Godfather Godfather2 Goodfellas    YGM     SS     SW</span>
<span class="co">#&gt; Godfather      1.000      0.997      0.562 -0.997 -0.996 -0.571</span>
<span class="co">#&gt; Godfather2     0.997      1.000      0.577 -0.998 -0.999 -0.583</span>
<span class="co">#&gt; Goodfellas     0.562      0.577      1.000 -0.552 -0.583 -0.994</span>
<span class="co">#&gt; YGM           -0.997     -0.998     -0.552  1.000  0.998  0.558</span>
<span class="co">#&gt; SS            -0.996     -0.999     -0.583  0.998  1.000  0.588</span>
<span class="co">#&gt; SW            -0.571     -0.583     -0.994  0.558  0.588  1.000</span></code></pre></div>
<p>We note that we perhaps need a second factor to account for the fact that some users like Al Pacino, while others dislike him or don’t care. Notice that the overall structure of the correlation obtained from the simulated data is not that far off the real correlation:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">six_movies &lt;-<span class="st"> </span><span class="kw">c</span>(m_<span class="dv">1</span>, m_<span class="dv">2</span>, m_<span class="dv">3</span>, m_<span class="dv">4</span>, m_<span class="dv">5</span>, m_<span class="dv">6</span>)
x &lt;-<span class="st"> </span>y[, six_movies]
<span class="kw">colnames</span>(x) &lt;-<span class="st"> </span><span class="kw">colnames</span>(r)
<span class="kw">cor</span>(x, <span class="dt">use=</span><span class="st">&quot;pairwise.complete&quot;</span>)
<span class="co">#&gt;            Godfather Godfather2 Goodfellas    YGM     SS      SW</span>
<span class="co">#&gt; Godfather     1.0000      0.829      0.444 -0.440 -0.378  0.0589</span>
<span class="co">#&gt; Godfather2    0.8285      1.000      0.521 -0.331 -0.358  0.1186</span>
<span class="co">#&gt; Goodfellas    0.4441      0.521      1.000 -0.481 -0.402 -0.1230</span>
<span class="co">#&gt; YGM          -0.4397     -0.331     -0.481  1.000  0.533 -0.1699</span>
<span class="co">#&gt; SS           -0.3781     -0.358     -0.402  0.533  1.000 -0.1822</span>
<span class="co">#&gt; SW            0.0589      0.119     -0.123 -0.170 -0.182  1.0000</span></code></pre></div>
<p>To explain this more complicated structure, we need two factors. For example something like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t</span>(q) 
<span class="co">#&gt;      Godfather Godfather2 Goodfellas You&#39;ve Got Sleepless Scent</span>
<span class="co">#&gt; [1,]         1          1          1         -1        -1    -1</span>
<span class="co">#&gt; [2,]         1          1         -1         -1        -1     1</span></code></pre></div>
<p>With the first factor (the first row) used to code the gangster versus romance groups and a second factor (the second row) to explain the Al Pacino versus no Al Pacino groups. We will also need two sets of coefficients to explain the variability introduced by the <span class="math inline">\(3\times 3\)</span> types of groups:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t</span>(p)
<span class="co">#&gt;         1   2   3 4 5   6   7   8  9   10   11   12</span>
<span class="co">#&gt; [1,]  1.0 1.0 1.0 0 0 0.0 0.0 0.0 -1 -1.0 -1.0 -1.0</span>
<span class="co">#&gt; [2,] -0.5 0.5 0.5 0 0 0.5 0.5 0.5  0 -0.5 -0.5 -0.5</span></code></pre></div>
<p>The model with two factors has 36 parameters that can be used to explain much of the variability in the 72 ratings:</p>
<p><span class="math display">\[
Y_{u,i} = \mu + b_i + b_u + p_{u,1} q_{1,i} + p_{u,2} q_{2,i} + \varepsilon_{u,i}
\]</span></p>
<p>Note that in an actual data application, we need to fit this model to data. To explain the complex correlation we observe in real data, we usually permit the entries of <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> to be continuous values, rather than discrete ones as we used in the simulation. For example, rather than dividing movies into gangster or romance, we define a continuum. Also note that this is not a linear model and to fit it we need to use an algorithm other than the one used by <code>lm</code> to find the parameters that minimize the least squares. The winning algorithms for the Netflix challenge fit a model similar to the above and used regularization to penalize for large values of <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>, rather than using least squares. Implementing this approach is beyond the scope of this book.</p>
</div>
<div id="connection-to-svd-and-pca" class="section level3">
<h3><span class="header-section-number">33.11.2</span> Connection to SVD and PCA</h3>
<p>The decomposition:</p>
<p><span class="math display">\[
r_{u,i} \approx p_{u,1} q_{1,i} + p_{u,2} q_{2,i}
\]</span></p>
<p>is very much related to SVD and PCA. SVD and PCA are complicated concepts, but one way to understand them is that SVD is an algorithm that finds the vectors <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> that permit us to rewrite the matrix <span class="math inline">\(\mbox{r}\)</span> with <span class="math inline">\(m\)</span> rows and <span class="math inline">\(n\)</span> columns as:</p>
<p><span class="math display">\[
r_{u,i} = p_{u,1} q_{1,i} + p_{u,2} q_{2,i} + \dots + p_{u,m} q_{m,i} 
\]</span></p>
<p>with the variability of each term decreasing and with the <span class="math inline">\(p\)</span>s uncorrelated. The algorithm also computes this variability so that we can know how much of the matrices, total variability is explained as we add new terms. This may permit us to see that, with just a few terms, we can explain most of the variability.</p>
<p>Let’s see an example with the movie data. To compute the decomposition, we will make the residuals with NAs equal to 0:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y[<span class="kw">is.na</span>(y)] &lt;-<span class="st"> </span><span class="dv">0</span>
pca &lt;-<span class="st"> </span><span class="kw">prcomp</span>(y)</code></pre></div>
<p>The <span class="math inline">\(q\)</span> vectors are called the principal components and they are stored in this matrix:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(pca<span class="op">$</span>rotation)
<span class="co">#&gt; [1] 454 292</span></code></pre></div>
<p>While the <span class="math inline">\(p\)</span>, or the user effects, are here:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(pca<span class="op">$</span>x)
<span class="co">#&gt; [1] 292 292</span></code></pre></div>
<p>We can see the variability of each of the vectors:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qplot</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(x), pca<span class="op">$</span>sdev, <span class="dt">xlab =</span> <span class="st">&quot;PC&quot;</span>)</code></pre></div>
<p><img src="book_files/figure-html/pca-sds-1.png" width="70%" style="display: block; margin: auto;" /></p>
<!--
and see that just the first few already explain a large percent:


```r
var_explained <- cumsum(pca$sdev^2 / sum(pca$sdev^2))
qplot(1:nrow(x), var_explained, xlab = "PC")
```

<img src="book_files/figure-html/var-expained-pca-1.png" width="70%" style="display: block; margin: auto;" />
-->
<p>We also notice that the first two principal components are related to the structure in opinions about movies:</p>
<p><img src="book_files/figure-html/movies-pca-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Just by looking at the top 10 in each direction, we see a meaningful pattern. The first PC shows the difference between critically acclaimed movies on one side:</p>
<pre><code>#&gt;  [1] &quot;Pulp Fiction&quot;              &quot;Seven (a.k.a. Se7en)&quot;     
#&gt;  [3] &quot;Fargo&quot;                     &quot;2001: A Space Odyssey&quot;    
#&gt;  [5] &quot;Silence of the Lambs, The&quot; &quot;Clockwork Orange, A&quot;      
#&gt;  [7] &quot;Taxi Driver&quot;               &quot;Being John Malkovich&quot;     
#&gt;  [9] &quot;Royal Tenenbaums, The&quot;     &quot;Shining, The&quot;</code></pre>
<p>and Hollywood blockbusters on the other:</p>
<pre><code>#&gt;  [1] &quot;Independence Day (a.k.a. ID4)&quot;  &quot;Shrek&quot;                         
#&gt;  [3] &quot;Spider-Man&quot;                     &quot;Titanic&quot;                       
#&gt;  [5] &quot;Twister&quot;                        &quot;Armageddon&quot;                    
#&gt;  [7] &quot;Harry Potter and the Sorcer...&quot; &quot;Forrest Gump&quot;                  
#&gt;  [9] &quot;Lord of the Rings: The Retu...&quot; &quot;Enemy of the State&quot;</code></pre>
<p>While the second PC seems to go from artsy, independent films:</p>
<pre><code>#&gt;  [1] &quot;Shawshank Redemption, The&quot;      &quot;Truman Show, The&quot;              
#&gt;  [3] &quot;Little Miss Sunshine&quot;           &quot;Slumdog Millionaire&quot;           
#&gt;  [5] &quot;Amelie (Fabuleux destin d&#39;A...&quot; &quot;Kill Bill: Vol. 1&quot;             
#&gt;  [7] &quot;American Beauty&quot;                &quot;City of God (Cidade de Deus)&quot;  
#&gt;  [9] &quot;Mars Attacks!&quot;                  &quot;Beautiful Mind, A&quot;</code></pre>
<p>to nerd favorites:</p>
<pre><code>#&gt;  [1] &quot;Lord of the Rings: The Two ...&quot; &quot;Lord of the Rings: The Fell...&quot;
#&gt;  [3] &quot;Lord of the Rings: The Retu...&quot; &quot;Matrix, The&quot;                   
#&gt;  [5] &quot;Star Wars: Episode IV - A N...&quot; &quot;Star Wars: Episode VI - Ret...&quot;
#&gt;  [7] &quot;Star Wars: Episode V - The ...&quot; &quot;Spider-Man 2&quot;                  
#&gt;  [9] &quot;Dark Knight, The&quot;               &quot;Speed&quot;</code></pre>
<p>Fitting a model that incorporates these estimates is complicated. For those interested in implementing an approach that incorporates these ideas, we recommend trying the <strong>recommenderlab</strong> package. The details are beyond the scope of this book.</p>
</div>
</div>
<div id="exercises-61" class="section level2">
<h2><span class="header-section-number">33.12</span> Exercises</h2>
<p>In this exercise set, we will be covering a topic useful for understanding matrix factorization: the singular value decomposition (SVD). SVD is a mathematical result that is widely used in machine learning, both in practice and to understand the mathematical properties of some algorithms. This is a rather advanced topic and to complete this exercise set you will have to be familiar with linear algebra concepts such as matrix multiplication, orthogonal matrices, and diagonal matrices.</p>
<p>The SVD tells us that we can <em>decompose</em> an <span class="math inline">\(N\times p\)</span> matrix <span class="math inline">\(Y\)</span> with <span class="math inline">\(p &lt; N\)</span> as</p>
<p><span class="math display">\[ Y = U D V^{\top} \]</span></p>
<p>With <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> <em>orthogonal</em> of dimensions <span class="math inline">\(N\times p\)</span> and <span class="math inline">\(p\times p\)</span>, respectively, and <span class="math inline">\(D\)</span> a <span class="math inline">\(p \times p\)</span> <em>diagonal</em> matrix with the values of the diagonal decreasing:</p>
<p><span class="math display">\[d_{1,1} \geq d_{2,2} \geq \dots d_{p,p}.\]</span></p>
<p>In this exercise, we will see one of the ways that this decomposition can be useful. To do this, we will construct a dataset that represents grade scores for 100 students in 24 different subjects. The overall average has been removed so this data represents the percentage point each student received above or below the average test score. So a 0 represents an average grade (C), a 25 is a high grade (A+), and a -25 represents a low grade (F). You can simulate the data like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1987</span>)
n &lt;-<span class="st"> </span><span class="dv">100</span>
k &lt;-<span class="st"> </span><span class="dv">8</span>
Sigma &lt;-<span class="st"> </span><span class="dv">64</span>  <span class="op">*</span><span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, .<span class="dv">75</span>, .<span class="dv">5</span>, .<span class="dv">75</span>, <span class="dv">1</span>, .<span class="dv">5</span>, .<span class="dv">5</span>, .<span class="dv">5</span>, <span class="dv">1</span>), <span class="dv">3</span>, <span class="dv">3</span>) 
m &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">mvrnorm</span>(n, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>), Sigma)
m &lt;-<span class="st"> </span>m[<span class="kw">order</span>(<span class="kw">rowMeans</span>(m), <span class="dt">decreasing =</span> <span class="ot">TRUE</span>),]
y &lt;-<span class="st"> </span>m <span class="op">%x%</span><span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">1</span>, k), <span class="dt">nrow =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="kw">matrix</span>(n <span class="op">*</span><span class="st"> </span>k <span class="op">*</span><span class="st"> </span><span class="dv">3</span>)), n, k <span class="op">*</span><span class="st"> </span><span class="dv">3</span>)
<span class="kw">colnames</span>(y) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">paste</span>(<span class="kw">rep</span>(<span class="st">&quot;Math&quot;</span>,k), <span class="dv">1</span><span class="op">:</span>k, <span class="dt">sep=</span><span class="st">&quot;_&quot;</span>),
                 <span class="kw">paste</span>(<span class="kw">rep</span>(<span class="st">&quot;Science&quot;</span>,k), <span class="dv">1</span><span class="op">:</span>k, <span class="dt">sep=</span><span class="st">&quot;_&quot;</span>),
                 <span class="kw">paste</span>(<span class="kw">rep</span>(<span class="st">&quot;Arts&quot;</span>,k), <span class="dv">1</span><span class="op">:</span>k, <span class="dt">sep=</span><span class="st">&quot;_&quot;</span>))</code></pre></div>
<p>Our goal is to describe the student performances as succinctly as possible. For example, we want to know if these test results are all just random independent numbers. Are all students just about as good? Does being good in one subject imply you will be good in another? How does the SVD help with all this? We will go step by step to show that with just three relatively small pairs of vectors we can explain much of the variability in this <span class="math inline">\(100 \times 24\)</span> dataset.</p>
<p>You can visualize the 24 test scores for the 100 students by plotting an image:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">my_image &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">zlim =</span> <span class="kw">range</span>(x), ...){
  colors =<span class="st"> </span><span class="kw">rev</span>(RColorBrewer<span class="op">::</span><span class="kw">brewer.pal</span>(<span class="dv">9</span>, <span class="st">&quot;RdBu&quot;</span>))
  cols &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(x)
  rows &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(x)
  <span class="kw">image</span>(cols, rows, <span class="kw">t</span>(x[<span class="kw">rev</span>(rows),,<span class="dt">drop=</span><span class="ot">FALSE</span>]), <span class="dt">xaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="dt">yaxt =</span> <span class="st">&quot;n&quot;</span>,
        <span class="dt">xlab=</span><span class="st">&quot;&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;&quot;</span>,  <span class="dt">col =</span> colors, <span class="dt">zlim =</span> zlim, ...)
  <span class="kw">abline</span>(<span class="dt">h=</span>rows <span class="op">+</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dt">v =</span> cols <span class="op">+</span><span class="st"> </span><span class="fl">0.5</span>)
  <span class="kw">axis</span>(<span class="dt">side =</span> <span class="dv">1</span>, cols, <span class="kw">colnames</span>(x), <span class="dt">las =</span> <span class="dv">2</span>)
}

<span class="kw">my_image</span>(y)</code></pre></div>
<p>1. How would you describe the data based on this figure?</p>
<ol style="list-style-type: lower-alpha">
<li>The test scores are all independent of each other.</li>
<li>The students that test well are at the top of the image and there seem to be three groupings by subject.</li>
<li>The students that are good at math are not good at science.</li>
<li>The students that are good at math are not good at humanities.</li>
</ol>
<p>2. You can examine the correlation between the test scores directly like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">my_image</span>(<span class="kw">cor</span>(y), <span class="dt">zlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))
<span class="kw">range</span>(<span class="kw">cor</span>(y))
<span class="kw">axis</span>(<span class="dt">side =</span> <span class="dv">2</span>, <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(y), <span class="kw">rev</span>(<span class="kw">colnames</span>(y)), <span class="dt">las =</span> <span class="dv">2</span>)</code></pre></div>
<p>Which of the following best describes what you see?</p>
<ol style="list-style-type: lower-alpha">
<li>The test scores are independent.</li>
<li>Math and science are highly correlated but the humanities are not.</li>
<li>There is high correlation between tests in the same subject but no correlation across subjects.</li>
<li>There is a correlation among all tests, but higher if the tests are in science and math and even higher within each subject.</li>
</ol>
<p>3. Remember that orthogonality means that <span class="math inline">\(U^{\top}U\)</span> and <span class="math inline">\(V^{\top}V\)</span> are equal to the identity matrix. This implies that we can also rewrite the decomposition as</p>
<p><span class="math display">\[ Y V = U D \mbox{ or } U^{\top}Y = D V^{\top}\]</span></p>
<p>We can think of <span class="math inline">\(YV\)</span> and <span class="math inline">\(U^{\top}V\)</span> as two transformations of Y that preserve the total variability of <span class="math inline">\(Y\)</span> since <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are orthogonal.</p>
<p>Use the function <code>svd</code> to compute the SVD of <code>y</code>. This function will return <span class="math inline">\(U\)</span>, <span class="math inline">\(V\)</span> and the diagonal entries of <span class="math inline">\(D\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">s &lt;-<span class="st"> </span><span class="kw">svd</span>(y)
<span class="kw">names</span>(s)</code></pre></div>
<p>You can check that the SVD works by typing:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y_svd &lt;-<span class="st"> </span>s<span class="op">$</span>u <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(s<span class="op">$</span>d) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(s<span class="op">$</span>v)
<span class="kw">max</span>(<span class="kw">abs</span>(y <span class="op">-</span><span class="st"> </span>y_svd))</code></pre></div>
<p>Compute the sum of squares of the columns of <span class="math inline">\(Y\)</span> and store them in <code>ss_y</code>. Then compute the sum of squares of columns of the transformed <span class="math inline">\(YV\)</span> and store them in <code>ss_yv</code>. Confirm that <code>sum(ss_y)</code> is equal to <code>sum(ss_yv)</code>.</p>
<p>4. We see that the total sum of squares is preserved. This is because <span class="math inline">\(V\)</span> is orthogonal. Now to start understanding how <span class="math inline">\(YV\)</span> is useful, plot <code>ss_y</code> against the column number and then do the same for <code>ss_yv</code>. What do you observe?</p>
<p>5. We see that the variability of the columns of <span class="math inline">\(YV\)</span> is decreasing. Furthermore, we see that, relative to the first three, the variability of the columns beyond the third is almost 0. Now notice that we didn’t have to compute <code>ss_yv</code> because we already have the answer. How? Remember that <span class="math inline">\(YV = UD\)</span> and because <span class="math inline">\(U\)</span> is orthogonal, we know that the sum of squares of the columns of <span class="math inline">\(UD\)</span> are the diagonal entries of <span class="math inline">\(D\)</span> squared. Confirm this by plotting the square root of <code>ss_yv</code> versus the diagonal entries of <span class="math inline">\(D\)</span>.</p>
<p>6. From the above we know that the sum of squares of the columns of <span class="math inline">\(Y\)</span> (the total sum of squares) add up to the sum of <code>s$d^2</code> and that the transformation <span class="math inline">\(YV\)</span> gives us columns with sums of squares equal to <code>s$d^2</code>. Now compute what percent of the total variability is explained by just the first three columns of <span class="math inline">\(YV\)</span>.</p>
<p>7. We see that almost 99% of the variability is explained by the first three columns of <span class="math inline">\(YV = UD\)</span>. So we get the sense that we should be able to explain much of the variability and structure we found while exploring the data with a few columns. Before we continue, let’s show a useful computational trick to avoid creating the matrix <code>diag(s$d)</code>. To motivate this, we note that if we write <span class="math inline">\(U\)</span> out in its columns <span class="math inline">\([U_1, U_2, \dots, U_p]\)</span> then <span class="math inline">\(UD\)</span> is equal to</p>
<p><span class="math display">\[UD = [U_1 d_{1,1}, U_2 d_{2,2}, \dots, U_p d_{p,p}]\]</span></p>
<p>Use the <code>sweep</code> function to compute <span class="math inline">\(UD\)</span> without constructing <code>diag(s$d)</code> nor matrix multiplication.</p>
<p>8. We know that <span class="math inline">\(U_1 d_{1,1}\)</span>, the first column of <span class="math inline">\(UD\)</span>, has the most variability of all the columns of <span class="math inline">\(UD\)</span>. Earlier we saw an image of <span class="math inline">\(Y\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">my_image</span>(y)</code></pre></div>
<p>in which we can see that the student to student variability is quite large and that it appears that students that are good in one subject are good in all. This implies that the average (across all subjects) for each student should explain a lot of the variability. Compute the average score for each student and plot it against <span class="math inline">\(U_1 d_{1,1}\)</span> and describe what you find.</p>
<p>9. We note that the signs in SVD are arbitrary because:</p>
<p><span class="math display">\[ U D V^{\top} = (-U) D (-V)^{\top} \]</span></p>
<p>With this in mind we see that the first column of <span class="math inline">\(UD\)</span> is almost identical to the average score for each student except for the sign.</p>
<p>This implies that multiplying <span class="math inline">\(Y\)</span> by the first column of <span class="math inline">\(V\)</span> must be performing a similar operation to taking the average. Make an image plot of <span class="math inline">\(V\)</span> and describe the first column relative to others and how this relates to taking an average.</p>
<p>10. We already saw that we can rewrite <span class="math inline">\(UD\)</span> as</p>
<p><span class="math display">\[U_1 d_{1,1} + U_2 d_{2,2} + \dots + U_p d_{p,p}\]</span></p>
<p>with <span class="math inline">\(U_j\)</span> the j-th column of <span class="math inline">\(U\)</span>. This implies that we can rewrite the entire SVD as:</p>
<p><span class="math display">\[Y = U_1 d_{1,1} V_1 ^{\top} + U_2 d_{2,2} V_2 ^{\top} + \dots + U_p d_{p,p} V_p ^{\top}\]</span></p>
<p>with <span class="math inline">\(V_j\)</span> the jth column of <span class="math inline">\(V\)</span>. Plot <span class="math inline">\(U_1\)</span>, then plot <span class="math inline">\(V_1^{\top}\)</span> using the same range for the y-axis limits, then make an image of <span class="math inline">\(U_1 d_{1,1} V_1 ^{\top}\)</span> and compare it to the image of <span class="math inline">\(Y\)</span>. Hint: use the <code>my_image</code> function defined above and use the <code>drop=FALSE</code> argument to assure the subsets of matrices are matrices.</p>
<p>11. We see that with just a vector of length 100, a scalar, and a vector of length 24, we actually come close to reconstructing the original <span class="math inline">\(100 \times 24\)</span> matrix. This is our first matrix factorization:</p>
<p><span class="math display">\[ Y \approx d_{1,1} U_1 V_1^{\top}\]</span></p>
<p>We know it explains <code>s$d[1]^2/sum(s$d^2) * 100</code> percent of the total variability. Our approximation only explains the observation that good students tend to be good in all subjects. But another aspect of the original data that our approximation does not explain was the higher similarity we observed within subjects. We can see this by computing the difference between our approximation and original data and then computing the correlations. You can see this by running this code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">resid &lt;-<span class="st"> </span>y <span class="op">-</span><span class="st"> </span><span class="kw">with</span>(s,(u[,<span class="dv">1</span>, <span class="dt">drop=</span><span class="ot">FALSE</span>]<span class="op">*</span>d[<span class="dv">1</span>]) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(v[,<span class="dv">1</span>, <span class="dt">drop=</span><span class="ot">FALSE</span>]))
<span class="kw">my_image</span>(<span class="kw">cor</span>(resid), <span class="dt">zlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))
<span class="kw">axis</span>(<span class="dt">side =</span> <span class="dv">2</span>, <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(y), <span class="kw">rev</span>(<span class="kw">colnames</span>(y)), <span class="dt">las =</span> <span class="dv">2</span>)</code></pre></div>
<p>Now that we have removed the overall student effect, the correlation plot reveals that we have not yet explained the within subject correlation nor the fact that math and science are closer to each other than to the arts. So let’s explore the second column of the SVD. Repeat the previous exercise but for the second column: Plot <span class="math inline">\(U_2\)</span>, then plot <span class="math inline">\(V_2^{\top}\)</span> using the same range for the y-axis limits, then make an image of <span class="math inline">\(U_2 d_{2,2} V_2 ^{\top}\)</span> and compare it to the image of <code>resid</code>.</p>
<p>12. The second column clearly relates to a student’s difference in ability in math/science versus the arts. We can see this most clearly from the plot of <code>s$v[,2]</code>. Adding the matrix we obtain with these two columns will help with our approximation:</p>
<p><span class="math display">\[ Y \approx d_{1,1} U_1 V_1^{\top} + d_{2,2} U_2 V_2^{\top} \]</span></p>
<p>We know it will explain</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(s<span class="op">$</span>d[<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">sum</span>(s<span class="op">$</span>d<span class="op">^</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="dv">100</span></code></pre></div>
<p>percent of the total variability. We can compute new residuals like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">resid &lt;-<span class="st"> </span>y <span class="op">-</span><span class="st"> </span><span class="kw">with</span>(s,<span class="kw">sweep</span>(u[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>], <span class="dv">2</span>, d[<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>], <span class="dt">FUN=</span><span class="st">&quot;*&quot;</span>) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(v[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]))
<span class="kw">my_image</span>(<span class="kw">cor</span>(resid), <span class="dt">zlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))
<span class="kw">axis</span>(<span class="dt">side =</span> <span class="dv">2</span>, <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(y), <span class="kw">rev</span>(<span class="kw">colnames</span>(y)), <span class="dt">las =</span> <span class="dv">2</span>)</code></pre></div>
<p>and see that the structure that is left is driven by the differences between math and science. Confirm this by plotting <span class="math inline">\(U_3\)</span>, then plot <span class="math inline">\(V_3^{\top}\)</span> using the same range for the y-axis limits, then make an image of <span class="math inline">\(U_3 d_{3,3} V_3 ^{\top}\)</span> and compare it to the image of <code>resid</code>.</p>
<p>13. The third column clearly relates to a student’s difference in ability in math and science. We can see this most clearly from the plot of <code>s$v[,3]</code>. Adding the matrix we obtain with these two columns will help with our approximation:</p>
<p><span class="math display">\[ Y \approx d_{1,1} U_1 V_1^{\top} + d_{2,2} U_2 V_2^{\top} + d_{3,3} U_3 V_3^{\top}\]</span></p>
<p>We know it will explain:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(s<span class="op">$</span>d[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">sum</span>(s<span class="op">$</span>d<span class="op">^</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="dv">100</span></code></pre></div>
<p>percent of the total variability. We can compute new residuals like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">resid &lt;-<span class="st"> </span>y <span class="op">-</span><span class="st"> </span><span class="kw">with</span>(s,<span class="kw">sweep</span>(u[,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>], <span class="dv">2</span>, d[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>], <span class="dt">FUN=</span><span class="st">&quot;*&quot;</span>) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(v[,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]))
<span class="kw">my_image</span>(<span class="kw">cor</span>(resid), <span class="dt">zlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))
<span class="kw">axis</span>(<span class="dt">side =</span> <span class="dv">2</span>, <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(y), <span class="kw">rev</span>(<span class="kw">colnames</span>(y)), <span class="dt">las =</span> <span class="dv">2</span>)</code></pre></div>
<p>We no longer see structure in the residuals: they seem to be independent of each other. This implies that we can describe the data with the following model:</p>
<p><span class="math display">\[ Y =  d_{1,1} U_1 V_1^{\top} + d_{2,2} U_2 V_2^{\top} + d_{3,3} U_3 V_3^{\top} + \varepsilon\]</span></p>
<p>with <span class="math inline">\(\varepsilon\)</span> a matrix of independent identically distributed errors. This model is useful because we summarize of <span class="math inline">\(100 \times 24\)</span> observations with <span class="math inline">\(3 \times (100+24+1) = 375\)</span> numbers. Furthermore, the three components of the model have useful interpretations: 1) the overall ability of a student, 2) the difference in ability between the math/sciences and arts, and 3) the remaining differences between the three subjects. The sizes <span class="math inline">\(d_{1,1}, d_{2,2}\)</span> and <span class="math inline">\(d_{3,3}\)</span> tell us the variability explained by each component. Finally, note that the components <span class="math inline">\(d_{j,j} U_j V_j^{\top}\)</span> are equivalent to the jth principal component.</p>
<p>Finish the exercise by plotting an image of <span class="math inline">\(Y\)</span>, an image of <span class="math inline">\(d_{1,1} U_1 V_1^{\top} + d_{2,2} U_2 V_2^{\top} + d_{3,3} U_3 V_3^{\top}\)</span> and an image of the residuals, all with the same <code>zlim</code>.</p>
<p>14. Advanced. The <code>movielens</code> dataset included in the <strong>dslabs</strong> package is a small subset of a larger dataset with millions of ratings. You can find the entire latest dataset here <a href="https://grouplens.org/datasets/movielens/20m/" class="uri">https://grouplens.org/datasets/movielens/20m/</a>. Create your own recommendation system using all the tools we have shown you.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="109">
<li id="fn109"><p><a href="http://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/" class="uri">http://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/</a><a href="large-datasets.html#fnref109">↩</a></p></li>
<li id="fn110"><p><a href="https://grouplens.org/" class="uri">https://grouplens.org/</a><a href="large-datasets.html#fnref110">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="machine-learning-in-practice.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="clustering.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rafalab/dsbook/edit/master/ml/matrix.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
