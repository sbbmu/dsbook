<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 15 Statistical inference | Introduction to Data Science</title>
  <meta name="description" content="This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 15 Statistical inference | Introduction to Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 15 Statistical inference | Introduction to Data Science" />
  
  <meta name="twitter:description" content="This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  

<meta name="author" content="Rafael A. Irizarry" />


<meta name="date" content="2019-11-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="random-variables.html">
<link rel="next" href="models.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Data Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#case-studies"><i class="fa fa-check"></i>Case studies</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#who-will-find-this-book-useful"><i class="fa fa-check"></i>Who will find this book useful?</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#what-does-this-book-cover"><i class="fa fa-check"></i>What does this book cover?</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#what-is-not-covered-by-this-book"><i class="fa fa-check"></i>What is not covered by this book?</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>1</b> Getting started with R and RStudio</a><ul>
<li class="chapter" data-level="1.1" data-path="getting-started.html"><a href="getting-started.html#why-r"><i class="fa fa-check"></i><b>1.1</b> Why R?</a></li>
<li class="chapter" data-level="1.2" data-path="getting-started.html"><a href="getting-started.html#the-r-console"><i class="fa fa-check"></i><b>1.2</b> The R console</a></li>
<li class="chapter" data-level="1.3" data-path="getting-started.html"><a href="getting-started.html#scripts"><i class="fa fa-check"></i><b>1.3</b> Scripts</a></li>
<li class="chapter" data-level="1.4" data-path="getting-started.html"><a href="getting-started.html#rstudio"><i class="fa fa-check"></i><b>1.4</b> RStudio</a><ul>
<li class="chapter" data-level="1.4.1" data-path="getting-started.html"><a href="getting-started.html#the-panes"><i class="fa fa-check"></i><b>1.4.1</b> The panes</a></li>
<li class="chapter" data-level="1.4.2" data-path="getting-started.html"><a href="getting-started.html#key-bindings"><i class="fa fa-check"></i><b>1.4.2</b> Key bindings</a></li>
<li class="chapter" data-level="1.4.3" data-path="getting-started.html"><a href="getting-started.html#running-commands-while-editing-scripts"><i class="fa fa-check"></i><b>1.4.3</b> Running commands while editing scripts</a></li>
<li class="chapter" data-level="1.4.4" data-path="getting-started.html"><a href="getting-started.html#changing-global-options"><i class="fa fa-check"></i><b>1.4.4</b> Changing global options</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="getting-started.html"><a href="getting-started.html#installing-r-packages"><i class="fa fa-check"></i><b>1.5</b> Installing R packages</a></li>
</ul></li>
<li class="part"><span><b>I R</b></span></li>
<li class="chapter" data-level="2" data-path="r-basics.html"><a href="r-basics.html"><i class="fa fa-check"></i><b>2</b> R basics</a><ul>
<li class="chapter" data-level="2.1" data-path="r-basics.html"><a href="r-basics.html#case-study-us-gun-murders"><i class="fa fa-check"></i><b>2.1</b> Case study: US Gun Murders</a></li>
<li class="chapter" data-level="2.2" data-path="r-basics.html"><a href="r-basics.html#the-very-basics"><i class="fa fa-check"></i><b>2.2</b> The very basics</a><ul>
<li class="chapter" data-level="2.2.1" data-path="r-basics.html"><a href="r-basics.html#objects"><i class="fa fa-check"></i><b>2.2.1</b> Objects</a></li>
<li class="chapter" data-level="2.2.2" data-path="r-basics.html"><a href="r-basics.html#the-workspace"><i class="fa fa-check"></i><b>2.2.2</b> The workspace</a></li>
<li class="chapter" data-level="2.2.3" data-path="r-basics.html"><a href="r-basics.html#functions"><i class="fa fa-check"></i><b>2.2.3</b> Functions</a></li>
<li class="chapter" data-level="2.2.4" data-path="r-basics.html"><a href="r-basics.html#other-prebuilt-objects"><i class="fa fa-check"></i><b>2.2.4</b> Other prebuilt objects</a></li>
<li class="chapter" data-level="2.2.5" data-path="r-basics.html"><a href="r-basics.html#variable-names"><i class="fa fa-check"></i><b>2.2.5</b> Variable names</a></li>
<li class="chapter" data-level="2.2.6" data-path="r-basics.html"><a href="r-basics.html#saving-your-workspace"><i class="fa fa-check"></i><b>2.2.6</b> Saving your workspace</a></li>
<li class="chapter" data-level="2.2.7" data-path="r-basics.html"><a href="r-basics.html#motivating-scripts"><i class="fa fa-check"></i><b>2.2.7</b> Motivating scripts</a></li>
<li class="chapter" data-level="2.2.8" data-path="r-basics.html"><a href="r-basics.html#commenting-your-code"><i class="fa fa-check"></i><b>2.2.8</b> Commenting your code</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="r-basics.html"><a href="r-basics.html#exercises"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
<li class="chapter" data-level="2.4" data-path="r-basics.html"><a href="r-basics.html#data-types"><i class="fa fa-check"></i><b>2.4</b> Data types</a><ul>
<li class="chapter" data-level="2.4.1" data-path="r-basics.html"><a href="r-basics.html#data-frames"><i class="fa fa-check"></i><b>2.4.1</b> Data frames</a></li>
<li class="chapter" data-level="2.4.2" data-path="r-basics.html"><a href="r-basics.html#examining-an-object"><i class="fa fa-check"></i><b>2.4.2</b> Examining an object</a></li>
<li class="chapter" data-level="2.4.3" data-path="r-basics.html"><a href="r-basics.html#the-accessor"><i class="fa fa-check"></i><b>2.4.3</b> The accessor: <code>$</code></a></li>
<li class="chapter" data-level="2.4.4" data-path="r-basics.html"><a href="r-basics.html#vectors-numerics-characters-and-logical"><i class="fa fa-check"></i><b>2.4.4</b> Vectors: numerics, characters, and logical</a></li>
<li class="chapter" data-level="2.4.5" data-path="r-basics.html"><a href="r-basics.html#factors"><i class="fa fa-check"></i><b>2.4.5</b> Factors</a></li>
<li class="chapter" data-level="2.4.6" data-path="r-basics.html"><a href="r-basics.html#lists"><i class="fa fa-check"></i><b>2.4.6</b> Lists</a></li>
<li class="chapter" data-level="2.4.7" data-path="r-basics.html"><a href="r-basics.html#matrices"><i class="fa fa-check"></i><b>2.4.7</b> Matrices</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="r-basics.html"><a href="r-basics.html#exercises-1"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
<li class="chapter" data-level="2.6" data-path="r-basics.html"><a href="r-basics.html#vectors"><i class="fa fa-check"></i><b>2.6</b> Vectors</a><ul>
<li class="chapter" data-level="2.6.1" data-path="r-basics.html"><a href="r-basics.html#creating-vectors"><i class="fa fa-check"></i><b>2.6.1</b> Creating vectors</a></li>
<li class="chapter" data-level="2.6.2" data-path="r-basics.html"><a href="r-basics.html#names"><i class="fa fa-check"></i><b>2.6.2</b> Names</a></li>
<li class="chapter" data-level="2.6.3" data-path="r-basics.html"><a href="r-basics.html#sequences"><i class="fa fa-check"></i><b>2.6.3</b> Sequences</a></li>
<li class="chapter" data-level="2.6.4" data-path="r-basics.html"><a href="r-basics.html#subsetting"><i class="fa fa-check"></i><b>2.6.4</b> Subsetting</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="r-basics.html"><a href="r-basics.html#coercion"><i class="fa fa-check"></i><b>2.7</b> Coercion</a><ul>
<li class="chapter" data-level="2.7.1" data-path="r-basics.html"><a href="r-basics.html#not-availables-na"><i class="fa fa-check"></i><b>2.7.1</b> Not availables (NA)</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="r-basics.html"><a href="r-basics.html#exercises-2"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
<li class="chapter" data-level="2.9" data-path="r-basics.html"><a href="r-basics.html#sorting"><i class="fa fa-check"></i><b>2.9</b> Sorting</a><ul>
<li class="chapter" data-level="2.9.1" data-path="r-basics.html"><a href="r-basics.html#sort"><i class="fa fa-check"></i><b>2.9.1</b> <code>sort</code></a></li>
<li class="chapter" data-level="2.9.2" data-path="r-basics.html"><a href="r-basics.html#order"><i class="fa fa-check"></i><b>2.9.2</b> <code>order</code></a></li>
<li class="chapter" data-level="2.9.3" data-path="r-basics.html"><a href="r-basics.html#max-and-which.max"><i class="fa fa-check"></i><b>2.9.3</b> <code>max</code> and <code>which.max</code></a></li>
<li class="chapter" data-level="2.9.4" data-path="r-basics.html"><a href="r-basics.html#rank"><i class="fa fa-check"></i><b>2.9.4</b> <code>rank</code></a></li>
<li class="chapter" data-level="2.9.5" data-path="r-basics.html"><a href="r-basics.html#beware-of-recycling"><i class="fa fa-check"></i><b>2.9.5</b> Beware of recycling</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="r-basics.html"><a href="r-basics.html#exercises-3"><i class="fa fa-check"></i><b>2.10</b> Exercises</a></li>
<li class="chapter" data-level="2.11" data-path="r-basics.html"><a href="r-basics.html#vector-arithmetics"><i class="fa fa-check"></i><b>2.11</b> Vector arithmetics</a><ul>
<li class="chapter" data-level="2.11.1" data-path="r-basics.html"><a href="r-basics.html#rescaling-a-vector"><i class="fa fa-check"></i><b>2.11.1</b> Rescaling a vector</a></li>
<li class="chapter" data-level="2.11.2" data-path="r-basics.html"><a href="r-basics.html#two-vectors"><i class="fa fa-check"></i><b>2.11.2</b> Two vectors</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="r-basics.html"><a href="r-basics.html#exercises-4"><i class="fa fa-check"></i><b>2.12</b> Exercises</a></li>
<li class="chapter" data-level="2.13" data-path="r-basics.html"><a href="r-basics.html#indexing"><i class="fa fa-check"></i><b>2.13</b> Indexing</a><ul>
<li class="chapter" data-level="2.13.1" data-path="r-basics.html"><a href="r-basics.html#subsetting-with-logicals"><i class="fa fa-check"></i><b>2.13.1</b> Subsetting with logicals</a></li>
<li class="chapter" data-level="2.13.2" data-path="r-basics.html"><a href="r-basics.html#logical-operators"><i class="fa fa-check"></i><b>2.13.2</b> Logical operators</a></li>
<li class="chapter" data-level="2.13.3" data-path="r-basics.html"><a href="r-basics.html#which"><i class="fa fa-check"></i><b>2.13.3</b> <code>which</code></a></li>
<li class="chapter" data-level="2.13.4" data-path="r-basics.html"><a href="r-basics.html#match"><i class="fa fa-check"></i><b>2.13.4</b> <code>match</code></a></li>
<li class="chapter" data-level="2.13.5" data-path="r-basics.html"><a href="r-basics.html#in"><i class="fa fa-check"></i><b>2.13.5</b> <code>%in%</code></a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="r-basics.html"><a href="r-basics.html#exercises-5"><i class="fa fa-check"></i><b>2.14</b> Exercises</a></li>
<li class="chapter" data-level="2.15" data-path="r-basics.html"><a href="r-basics.html#basic-plots"><i class="fa fa-check"></i><b>2.15</b> Basic plots</a><ul>
<li class="chapter" data-level="2.15.1" data-path="r-basics.html"><a href="r-basics.html#plot"><i class="fa fa-check"></i><b>2.15.1</b> <code>plot</code></a></li>
<li class="chapter" data-level="2.15.2" data-path="r-basics.html"><a href="r-basics.html#hist"><i class="fa fa-check"></i><b>2.15.2</b> <code>hist</code></a></li>
<li class="chapter" data-level="2.15.3" data-path="r-basics.html"><a href="r-basics.html#boxplot"><i class="fa fa-check"></i><b>2.15.3</b> <code>boxplot</code></a></li>
<li class="chapter" data-level="2.15.4" data-path="r-basics.html"><a href="r-basics.html#image"><i class="fa fa-check"></i><b>2.15.4</b> <code>image</code></a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="r-basics.html"><a href="r-basics.html#exercises-6"><i class="fa fa-check"></i><b>2.16</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="programming-basics.html"><a href="programming-basics.html"><i class="fa fa-check"></i><b>3</b> Programming basics</a><ul>
<li class="chapter" data-level="3.1" data-path="programming-basics.html"><a href="programming-basics.html#conditionals"><i class="fa fa-check"></i><b>3.1</b> Conditional expressions</a></li>
<li class="chapter" data-level="3.2" data-path="programming-basics.html"><a href="programming-basics.html#defining-functions"><i class="fa fa-check"></i><b>3.2</b> Defining functions</a></li>
<li class="chapter" data-level="3.3" data-path="programming-basics.html"><a href="programming-basics.html#namespaces"><i class="fa fa-check"></i><b>3.3</b> Namespaces</a></li>
<li class="chapter" data-level="3.4" data-path="programming-basics.html"><a href="programming-basics.html#for-loops"><i class="fa fa-check"></i><b>3.4</b> For-loops</a></li>
<li class="chapter" data-level="3.5" data-path="programming-basics.html"><a href="programming-basics.html#vectorization"><i class="fa fa-check"></i><b>3.5</b> Vectorization and functionals</a></li>
<li class="chapter" data-level="3.6" data-path="programming-basics.html"><a href="programming-basics.html#exercises-7"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="tidyverse.html"><a href="tidyverse.html"><i class="fa fa-check"></i><b>4</b> The tidyverse</a><ul>
<li class="chapter" data-level="4.1" data-path="tidyverse.html"><a href="tidyverse.html#tidy-data"><i class="fa fa-check"></i><b>4.1</b> Tidy data</a></li>
<li class="chapter" data-level="4.2" data-path="tidyverse.html"><a href="tidyverse.html#exercises-8"><i class="fa fa-check"></i><b>4.2</b> Exercises</a></li>
<li class="chapter" data-level="4.3" data-path="tidyverse.html"><a href="tidyverse.html#manipulating-data-frames"><i class="fa fa-check"></i><b>4.3</b> Manipulating data frames</a><ul>
<li class="chapter" data-level="4.3.1" data-path="tidyverse.html"><a href="tidyverse.html#adding-a-column-with-mutate"><i class="fa fa-check"></i><b>4.3.1</b> Adding a column with <code>mutate</code></a></li>
<li class="chapter" data-level="4.3.2" data-path="tidyverse.html"><a href="tidyverse.html#subsetting-with-filter"><i class="fa fa-check"></i><b>4.3.2</b> Subsetting with <code>filter</code></a></li>
<li class="chapter" data-level="4.3.3" data-path="tidyverse.html"><a href="tidyverse.html#selecting-columns-with-select"><i class="fa fa-check"></i><b>4.3.3</b> Selecting columns with <code>select</code></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tidyverse.html"><a href="tidyverse.html#exercises-9"><i class="fa fa-check"></i><b>4.4</b> Exercises</a></li>
<li class="chapter" data-level="4.5" data-path="tidyverse.html"><a href="tidyverse.html#the-pipe"><i class="fa fa-check"></i><b>4.5</b> The pipe: <code>%&gt;%</code></a></li>
<li class="chapter" data-level="4.6" data-path="tidyverse.html"><a href="tidyverse.html#exercises-10"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
<li class="chapter" data-level="4.7" data-path="tidyverse.html"><a href="tidyverse.html#summarizing-data"><i class="fa fa-check"></i><b>4.7</b> Summarizing data</a><ul>
<li class="chapter" data-level="4.7.1" data-path="tidyverse.html"><a href="tidyverse.html#summarize"><i class="fa fa-check"></i><b>4.7.1</b> <code>summarize</code></a></li>
<li class="chapter" data-level="4.7.2" data-path="tidyverse.html"><a href="tidyverse.html#pull"><i class="fa fa-check"></i><b>4.7.2</b> <code>pull</code></a></li>
<li class="chapter" data-level="4.7.3" data-path="tidyverse.html"><a href="tidyverse.html#group-by"><i class="fa fa-check"></i><b>4.7.3</b> Group then summarize with <code>group_by</code></a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="tidyverse.html"><a href="tidyverse.html#sorting-data-frames"><i class="fa fa-check"></i><b>4.8</b> Sorting data frames</a><ul>
<li class="chapter" data-level="4.8.1" data-path="tidyverse.html"><a href="tidyverse.html#nested-sorting"><i class="fa fa-check"></i><b>4.8.1</b> Nested sorting</a></li>
<li class="chapter" data-level="4.8.2" data-path="tidyverse.html"><a href="tidyverse.html#the-top-n"><i class="fa fa-check"></i><b>4.8.2</b> The top <span class="math inline">\(n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="tidyverse.html"><a href="tidyverse.html#exercises-11"><i class="fa fa-check"></i><b>4.9</b> Exercises</a></li>
<li class="chapter" data-level="4.10" data-path="tidyverse.html"><a href="tidyverse.html#tibbles"><i class="fa fa-check"></i><b>4.10</b> Tibbles</a><ul>
<li class="chapter" data-level="4.10.1" data-path="tidyverse.html"><a href="tidyverse.html#tibbles-display-better"><i class="fa fa-check"></i><b>4.10.1</b> Tibbles display better</a></li>
<li class="chapter" data-level="4.10.2" data-path="tidyverse.html"><a href="tidyverse.html#subsets-of-tibbles-are-tibbles"><i class="fa fa-check"></i><b>4.10.2</b> Subsets of tibbles are tibbles</a></li>
<li class="chapter" data-level="4.10.3" data-path="tidyverse.html"><a href="tidyverse.html#tibbles-can-have-complex-entries"><i class="fa fa-check"></i><b>4.10.3</b> Tibbles can have complex entries</a></li>
<li class="chapter" data-level="4.10.4" data-path="tidyverse.html"><a href="tidyverse.html#tibbles-can-be-grouped"><i class="fa fa-check"></i><b>4.10.4</b> Tibbles can be grouped</a></li>
<li class="chapter" data-level="4.10.5" data-path="tidyverse.html"><a href="tidyverse.html#create-a-tibble-using-tibble-instead-of-data.frame"><i class="fa fa-check"></i><b>4.10.5</b> Create a tibble using <code>tibble</code> instead of <code>data.frame</code></a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="tidyverse.html"><a href="tidyverse.html#the-dot-operator"><i class="fa fa-check"></i><b>4.11</b> The dot operator</a></li>
<li class="chapter" data-level="4.12" data-path="tidyverse.html"><a href="tidyverse.html#do"><i class="fa fa-check"></i><b>4.12</b> <code>do</code></a></li>
<li class="chapter" data-level="4.13" data-path="tidyverse.html"><a href="tidyverse.html#the-purrr-package"><i class="fa fa-check"></i><b>4.13</b> The <strong>purrr</strong> package</a></li>
<li class="chapter" data-level="4.14" data-path="tidyverse.html"><a href="tidyverse.html#tidyverse-conditionals"><i class="fa fa-check"></i><b>4.14</b> Tidyverse conditionals</a><ul>
<li class="chapter" data-level="4.14.1" data-path="tidyverse.html"><a href="tidyverse.html#case_when"><i class="fa fa-check"></i><b>4.14.1</b> <code>case_when</code></a></li>
<li class="chapter" data-level="4.14.2" data-path="tidyverse.html"><a href="tidyverse.html#between"><i class="fa fa-check"></i><b>4.14.2</b> <code>between</code></a></li>
</ul></li>
<li class="chapter" data-level="4.15" data-path="tidyverse.html"><a href="tidyverse.html#exercises-12"><i class="fa fa-check"></i><b>4.15</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="importing-data.html"><a href="importing-data.html"><i class="fa fa-check"></i><b>5</b> Importing data</a><ul>
<li class="chapter" data-level="5.1" data-path="importing-data.html"><a href="importing-data.html#paths-and-the-working-directory"><i class="fa fa-check"></i><b>5.1</b> Paths and the working directory</a><ul>
<li class="chapter" data-level="5.1.1" data-path="importing-data.html"><a href="importing-data.html#the-filesystem"><i class="fa fa-check"></i><b>5.1.1</b> The filesystem</a></li>
<li class="chapter" data-level="5.1.2" data-path="importing-data.html"><a href="importing-data.html#relative-and-full-paths"><i class="fa fa-check"></i><b>5.1.2</b> Relative and full paths</a></li>
<li class="chapter" data-level="5.1.3" data-path="importing-data.html"><a href="importing-data.html#the-working-directory"><i class="fa fa-check"></i><b>5.1.3</b> The working directory</a></li>
<li class="chapter" data-level="5.1.4" data-path="importing-data.html"><a href="importing-data.html#generating-path-names"><i class="fa fa-check"></i><b>5.1.4</b> Generating path names</a></li>
<li class="chapter" data-level="5.1.5" data-path="importing-data.html"><a href="importing-data.html#copying-files-using-paths"><i class="fa fa-check"></i><b>5.1.5</b> Copying files using paths</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="importing-data.html"><a href="importing-data.html#the-readr-and-readxl-packages"><i class="fa fa-check"></i><b>5.2</b> The readr and readxl packages</a><ul>
<li class="chapter" data-level="5.2.1" data-path="importing-data.html"><a href="importing-data.html#readr"><i class="fa fa-check"></i><b>5.2.1</b> readr</a></li>
<li class="chapter" data-level="5.2.2" data-path="importing-data.html"><a href="importing-data.html#readxl"><i class="fa fa-check"></i><b>5.2.2</b> readxl</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="importing-data.html"><a href="importing-data.html#exercises-13"><i class="fa fa-check"></i><b>5.3</b> Exercises</a></li>
<li class="chapter" data-level="5.4" data-path="importing-data.html"><a href="importing-data.html#downloading-files"><i class="fa fa-check"></i><b>5.4</b> Downloading files</a></li>
<li class="chapter" data-level="5.5" data-path="importing-data.html"><a href="importing-data.html#r-base-importing-functions"><i class="fa fa-check"></i><b>5.5</b> R-base importing functions</a><ul>
<li class="chapter" data-level="5.5.1" data-path="importing-data.html"><a href="importing-data.html#scan"><i class="fa fa-check"></i><b>5.5.1</b> <code>scan</code></a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="importing-data.html"><a href="importing-data.html#text-versus-binary-files"><i class="fa fa-check"></i><b>5.6</b> Text versus binary files</a></li>
<li class="chapter" data-level="5.7" data-path="importing-data.html"><a href="importing-data.html#unicode-versus-ascii"><i class="fa fa-check"></i><b>5.7</b> Unicode versus ASCII</a></li>
<li class="chapter" data-level="5.8" data-path="importing-data.html"><a href="importing-data.html#organizing-data-with-spreadsheets"><i class="fa fa-check"></i><b>5.8</b> Organizing data with spreadsheets</a></li>
<li class="chapter" data-level="5.9" data-path="importing-data.html"><a href="importing-data.html#exercises-14"><i class="fa fa-check"></i><b>5.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Data Visualization</b></span></li>
<li class="chapter" data-level="6" data-path="introduction-to-data-visualization.html"><a href="introduction-to-data-visualization.html"><i class="fa fa-check"></i><b>6</b> Introduction to data visualization</a></li>
<li class="chapter" data-level="7" data-path="ggplot2.html"><a href="ggplot2.html"><i class="fa fa-check"></i><b>7</b> ggplot2</a><ul>
<li class="chapter" data-level="7.1" data-path="ggplot2.html"><a href="ggplot2.html#the-components-of-a-graph"><i class="fa fa-check"></i><b>7.1</b> The components of a graph</a></li>
<li class="chapter" data-level="7.2" data-path="ggplot2.html"><a href="ggplot2.html#ggplot-objects"><i class="fa fa-check"></i><b>7.2</b> <code>ggplot</code> objects</a></li>
<li class="chapter" data-level="7.3" data-path="ggplot2.html"><a href="ggplot2.html#geometries"><i class="fa fa-check"></i><b>7.3</b> Geometries</a></li>
<li class="chapter" data-level="7.4" data-path="ggplot2.html"><a href="ggplot2.html#aesthetic-mappings"><i class="fa fa-check"></i><b>7.4</b> Aesthetic mappings</a></li>
<li class="chapter" data-level="7.5" data-path="ggplot2.html"><a href="ggplot2.html#layers"><i class="fa fa-check"></i><b>7.5</b> Layers</a><ul>
<li class="chapter" data-level="7.5.1" data-path="ggplot2.html"><a href="ggplot2.html#tinkering-with-arguments"><i class="fa fa-check"></i><b>7.5.1</b> Tinkering with arguments</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="ggplot2.html"><a href="ggplot2.html#global-versus-local-aesthetic-mappings"><i class="fa fa-check"></i><b>7.6</b> Global versus local aesthetic mappings</a></li>
<li class="chapter" data-level="7.7" data-path="ggplot2.html"><a href="ggplot2.html#scales"><i class="fa fa-check"></i><b>7.7</b> Scales</a></li>
<li class="chapter" data-level="7.8" data-path="ggplot2.html"><a href="ggplot2.html#labels-and-titles"><i class="fa fa-check"></i><b>7.8</b> Labels and titles</a></li>
<li class="chapter" data-level="7.9" data-path="ggplot2.html"><a href="ggplot2.html#categories-as-colors"><i class="fa fa-check"></i><b>7.9</b> Categories as colors</a></li>
<li class="chapter" data-level="7.10" data-path="ggplot2.html"><a href="ggplot2.html#annotation-shapes-and-adjustments"><i class="fa fa-check"></i><b>7.10</b> Annotation, shapes, and adjustments</a></li>
<li class="chapter" data-level="7.11" data-path="ggplot2.html"><a href="ggplot2.html#add-on-packages"><i class="fa fa-check"></i><b>7.11</b> Add-on packages</a></li>
<li class="chapter" data-level="7.12" data-path="ggplot2.html"><a href="ggplot2.html#putting-it-all-together"><i class="fa fa-check"></i><b>7.12</b> Putting it all together</a></li>
<li class="chapter" data-level="7.13" data-path="ggplot2.html"><a href="ggplot2.html#qplot"><i class="fa fa-check"></i><b>7.13</b> Quick plots with <code>qplot</code></a></li>
<li class="chapter" data-level="7.14" data-path="ggplot2.html"><a href="ggplot2.html#grids-of-plots"><i class="fa fa-check"></i><b>7.14</b> Grids of plots</a></li>
<li class="chapter" data-level="7.15" data-path="ggplot2.html"><a href="ggplot2.html#exercises-15"><i class="fa fa-check"></i><b>7.15</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>8</b> Visualizing data distributions</a><ul>
<li class="chapter" data-level="8.1" data-path="distributions.html"><a href="distributions.html#variable-types"><i class="fa fa-check"></i><b>8.1</b> Variable types</a></li>
<li class="chapter" data-level="8.2" data-path="distributions.html"><a href="distributions.html#case-study-describing-student-heights"><i class="fa fa-check"></i><b>8.2</b> Case study: describing student heights</a></li>
<li class="chapter" data-level="8.3" data-path="distributions.html"><a href="distributions.html#distribution-function"><i class="fa fa-check"></i><b>8.3</b> Distribution function</a></li>
<li class="chapter" data-level="8.4" data-path="distributions.html"><a href="distributions.html#cdf-intro"><i class="fa fa-check"></i><b>8.4</b> Cumulative distribution functions</a></li>
<li class="chapter" data-level="8.5" data-path="distributions.html"><a href="distributions.html#histograms"><i class="fa fa-check"></i><b>8.5</b> Histograms</a></li>
<li class="chapter" data-level="8.6" data-path="distributions.html"><a href="distributions.html#smoothed-density"><i class="fa fa-check"></i><b>8.6</b> Smoothed density</a><ul>
<li class="chapter" data-level="8.6.1" data-path="distributions.html"><a href="distributions.html#interpreting-the-y-axis"><i class="fa fa-check"></i><b>8.6.1</b> Interpreting the y-axis</a></li>
<li class="chapter" data-level="8.6.2" data-path="distributions.html"><a href="distributions.html#densities-permit-stratification"><i class="fa fa-check"></i><b>8.6.2</b> Densities permit stratification</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="distributions.html"><a href="distributions.html#exercises-16"><i class="fa fa-check"></i><b>8.7</b> Exercises</a></li>
<li class="chapter" data-level="8.8" data-path="distributions.html"><a href="distributions.html#normal-distribution"><i class="fa fa-check"></i><b>8.8</b> The normal distribution</a></li>
<li class="chapter" data-level="8.9" data-path="distributions.html"><a href="distributions.html#standard-units"><i class="fa fa-check"></i><b>8.9</b> Standard units</a></li>
<li class="chapter" data-level="8.10" data-path="distributions.html"><a href="distributions.html#quantile-quantile-plots"><i class="fa fa-check"></i><b>8.10</b> Quantile-quantile plots</a></li>
<li class="chapter" data-level="8.11" data-path="distributions.html"><a href="distributions.html#percentiles"><i class="fa fa-check"></i><b>8.11</b> Percentiles</a></li>
<li class="chapter" data-level="8.12" data-path="distributions.html"><a href="distributions.html#boxplots"><i class="fa fa-check"></i><b>8.12</b> Boxplots</a></li>
<li class="chapter" data-level="8.13" data-path="distributions.html"><a href="distributions.html#stratification"><i class="fa fa-check"></i><b>8.13</b> Stratification</a></li>
<li class="chapter" data-level="8.14" data-path="distributions.html"><a href="distributions.html#student-height-cont"><i class="fa fa-check"></i><b>8.14</b> Case study: describing student heights (continued)</a></li>
<li class="chapter" data-level="8.15" data-path="distributions.html"><a href="distributions.html#exercises-17"><i class="fa fa-check"></i><b>8.15</b> Exercises</a></li>
<li class="chapter" data-level="8.16" data-path="distributions.html"><a href="distributions.html#other-geometries"><i class="fa fa-check"></i><b>8.16</b> ggplot2 geometries</a><ul>
<li class="chapter" data-level="8.16.1" data-path="distributions.html"><a href="distributions.html#barplots"><i class="fa fa-check"></i><b>8.16.1</b> Barplots</a></li>
<li class="chapter" data-level="8.16.2" data-path="distributions.html"><a href="distributions.html#histograms-1"><i class="fa fa-check"></i><b>8.16.2</b> Histograms</a></li>
<li class="chapter" data-level="8.16.3" data-path="distributions.html"><a href="distributions.html#density-plots"><i class="fa fa-check"></i><b>8.16.3</b> Density plots</a></li>
<li class="chapter" data-level="8.16.4" data-path="distributions.html"><a href="distributions.html#boxplots-1"><i class="fa fa-check"></i><b>8.16.4</b> Boxplots</a></li>
<li class="chapter" data-level="8.16.5" data-path="distributions.html"><a href="distributions.html#qq-plots"><i class="fa fa-check"></i><b>8.16.5</b> QQ-plots</a></li>
<li class="chapter" data-level="8.16.6" data-path="distributions.html"><a href="distributions.html#images"><i class="fa fa-check"></i><b>8.16.6</b> Images</a></li>
<li class="chapter" data-level="8.16.7" data-path="distributions.html"><a href="distributions.html#quick-plots"><i class="fa fa-check"></i><b>8.16.7</b> Quick plots</a></li>
</ul></li>
<li class="chapter" data-level="8.17" data-path="distributions.html"><a href="distributions.html#exercises-18"><i class="fa fa-check"></i><b>8.17</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="gapminder.html"><a href="gapminder.html"><i class="fa fa-check"></i><b>9</b> Data visualization in practice</a><ul>
<li class="chapter" data-level="9.1" data-path="gapminder.html"><a href="gapminder.html#case-study-new-insights-on-poverty"><i class="fa fa-check"></i><b>9.1</b> Case study: new insights on poverty</a><ul>
<li class="chapter" data-level="9.1.1" data-path="gapminder.html"><a href="gapminder.html#hans-roslings-quiz"><i class="fa fa-check"></i><b>9.1.1</b> Hans Rosling’s quiz</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="gapminder.html"><a href="gapminder.html#scatterplots"><i class="fa fa-check"></i><b>9.2</b> Scatterplots</a></li>
<li class="chapter" data-level="9.3" data-path="gapminder.html"><a href="gapminder.html#faceting"><i class="fa fa-check"></i><b>9.3</b> Faceting</a><ul>
<li class="chapter" data-level="9.3.1" data-path="gapminder.html"><a href="gapminder.html#facet_wrap"><i class="fa fa-check"></i><b>9.3.1</b> <code>facet_wrap</code></a></li>
<li class="chapter" data-level="9.3.2" data-path="gapminder.html"><a href="gapminder.html#fixed-scales-for-better-comparisons"><i class="fa fa-check"></i><b>9.3.2</b> Fixed scales for better comparisons</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="gapminder.html"><a href="gapminder.html#time-series-plots"><i class="fa fa-check"></i><b>9.4</b> Time series plots</a><ul>
<li class="chapter" data-level="9.4.1" data-path="gapminder.html"><a href="gapminder.html#labels-instead-of-legends"><i class="fa fa-check"></i><b>9.4.1</b> Labels instead of legends</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="gapminder.html"><a href="gapminder.html#data-transformations"><i class="fa fa-check"></i><b>9.5</b> Data transformations</a><ul>
<li class="chapter" data-level="9.5.1" data-path="gapminder.html"><a href="gapminder.html#log-transformation"><i class="fa fa-check"></i><b>9.5.1</b> Log transformation</a></li>
<li class="chapter" data-level="9.5.2" data-path="gapminder.html"><a href="gapminder.html#which-base"><i class="fa fa-check"></i><b>9.5.2</b> Which base?</a></li>
<li class="chapter" data-level="9.5.3" data-path="gapminder.html"><a href="gapminder.html#transform-the-values-or-the-scale"><i class="fa fa-check"></i><b>9.5.3</b> Transform the values or the scale?</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="gapminder.html"><a href="gapminder.html#visualizing-multimodal-distributions"><i class="fa fa-check"></i><b>9.6</b> Visualizing multimodal distributions</a></li>
<li class="chapter" data-level="9.7" data-path="gapminder.html"><a href="gapminder.html#comparing-multiple-distributions-with-boxplots-and-ridge-plots"><i class="fa fa-check"></i><b>9.7</b> Comparing multiple distributions with boxplots and ridge plots</a><ul>
<li class="chapter" data-level="9.7.1" data-path="gapminder.html"><a href="gapminder.html#boxplots-2"><i class="fa fa-check"></i><b>9.7.1</b> Boxplots</a></li>
<li class="chapter" data-level="9.7.2" data-path="gapminder.html"><a href="gapminder.html#ridge-plots"><i class="fa fa-check"></i><b>9.7.2</b> Ridge plots</a></li>
<li class="chapter" data-level="9.7.3" data-path="gapminder.html"><a href="gapminder.html#example-1970-versus-2010-income-distributions"><i class="fa fa-check"></i><b>9.7.3</b> Example: 1970 versus 2010 income distributions</a></li>
<li class="chapter" data-level="9.7.4" data-path="gapminder.html"><a href="gapminder.html#accessing-computed-variables"><i class="fa fa-check"></i><b>9.7.4</b> Accessing computed variables</a></li>
<li class="chapter" data-level="9.7.5" data-path="gapminder.html"><a href="gapminder.html#weighted-densities"><i class="fa fa-check"></i><b>9.7.5</b> Weighted densities</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="gapminder.html"><a href="gapminder.html#the-ecological-fallacy-and-importance-of-showing-the-data"><i class="fa fa-check"></i><b>9.8</b> The ecological fallacy and importance of showing the data</a><ul>
<li class="chapter" data-level="9.8.1" data-path="gapminder.html"><a href="gapminder.html#logit"><i class="fa fa-check"></i><b>9.8.1</b> Logistic transformation</a></li>
<li class="chapter" data-level="9.8.2" data-path="gapminder.html"><a href="gapminder.html#show-the-data"><i class="fa fa-check"></i><b>9.8.2</b> Show the data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html"><i class="fa fa-check"></i><b>10</b> Data visualization principles</a><ul>
<li class="chapter" data-level="10.1" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#encoding-data-using-visual-cues"><i class="fa fa-check"></i><b>10.1</b> Encoding data using visual cues</a></li>
<li class="chapter" data-level="10.2" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#know-when-to-include-0"><i class="fa fa-check"></i><b>10.2</b> Know when to include 0</a></li>
<li class="chapter" data-level="10.3" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#do-not-distort-quantities"><i class="fa fa-check"></i><b>10.3</b> Do not distort quantities</a></li>
<li class="chapter" data-level="10.4" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#order-categories-by-a-meaningful-value"><i class="fa fa-check"></i><b>10.4</b> Order categories by a meaningful value</a></li>
<li class="chapter" data-level="10.5" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#show-the-data-1"><i class="fa fa-check"></i><b>10.5</b> Show the data</a></li>
<li class="chapter" data-level="10.6" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#ease-comparisons"><i class="fa fa-check"></i><b>10.6</b> Ease comparisons</a><ul>
<li class="chapter" data-level="10.6.1" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#use-common-axes"><i class="fa fa-check"></i><b>10.6.1</b> Use common axes</a></li>
<li class="chapter" data-level="10.6.2" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#align-plots-vertically-to-see-horizontal-changes-and-horizontally-to-see-vertical-changes"><i class="fa fa-check"></i><b>10.6.2</b> Align plots vertically to see horizontal changes and horizontally to see vertical changes</a></li>
<li class="chapter" data-level="10.6.3" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#consider-transformations"><i class="fa fa-check"></i><b>10.6.3</b> Consider transformations</a></li>
<li class="chapter" data-level="10.6.4" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#visual-cues-to-be-compared-should-be-adjacent"><i class="fa fa-check"></i><b>10.6.4</b> Visual cues to be compared should be adjacent</a></li>
<li class="chapter" data-level="10.6.5" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#use-color"><i class="fa fa-check"></i><b>10.6.5</b> Use color</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#think-of-the-color-blind"><i class="fa fa-check"></i><b>10.7</b> Think of the color blind</a></li>
<li class="chapter" data-level="10.8" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#plots-for-two-variables"><i class="fa fa-check"></i><b>10.8</b> Plots for two variables</a><ul>
<li class="chapter" data-level="10.8.1" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#slope-charts"><i class="fa fa-check"></i><b>10.8.1</b> Slope charts</a></li>
<li class="chapter" data-level="10.8.2" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#bland-altman-plot"><i class="fa fa-check"></i><b>10.8.2</b> Bland-Altman plot</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#encoding-a-third-variable"><i class="fa fa-check"></i><b>10.9</b> Encoding a third variable</a></li>
<li class="chapter" data-level="10.10" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#avoid-pseudo-three-dimensional-plots"><i class="fa fa-check"></i><b>10.10</b> Avoid pseudo-three-dimensional plots</a></li>
<li class="chapter" data-level="10.11" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#avoid-too-many-significant-digits"><i class="fa fa-check"></i><b>10.11</b> Avoid too many significant digits</a></li>
<li class="chapter" data-level="10.12" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#know-your-audience"><i class="fa fa-check"></i><b>10.12</b> Know your audience</a></li>
<li class="chapter" data-level="10.13" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#exercises-19"><i class="fa fa-check"></i><b>10.13</b> Exercises</a></li>
<li class="chapter" data-level="10.14" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#vaccines"><i class="fa fa-check"></i><b>10.14</b> Case study: vaccines and infectious diseases</a></li>
<li class="chapter" data-level="10.15" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#exercises-20"><i class="fa fa-check"></i><b>10.15</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="robust-summaries.html"><a href="robust-summaries.html"><i class="fa fa-check"></i><b>11</b> Robust summaries</a><ul>
<li class="chapter" data-level="11.1" data-path="robust-summaries.html"><a href="robust-summaries.html#outliers"><i class="fa fa-check"></i><b>11.1</b> Outliers</a></li>
<li class="chapter" data-level="11.2" data-path="robust-summaries.html"><a href="robust-summaries.html#median"><i class="fa fa-check"></i><b>11.2</b> Median</a></li>
<li class="chapter" data-level="11.3" data-path="robust-summaries.html"><a href="robust-summaries.html#the-inter-quartile-range-iqr"><i class="fa fa-check"></i><b>11.3</b> The inter quartile range (IQR)</a></li>
<li class="chapter" data-level="11.4" data-path="robust-summaries.html"><a href="robust-summaries.html#tukeys-definition-of-an-outlier"><i class="fa fa-check"></i><b>11.4</b> Tukey’s definition of an outlier</a></li>
<li class="chapter" data-level="11.5" data-path="robust-summaries.html"><a href="robust-summaries.html#median-absolute-deviation"><i class="fa fa-check"></i><b>11.5</b> Median absolute deviation</a></li>
<li class="chapter" data-level="11.6" data-path="robust-summaries.html"><a href="robust-summaries.html#exercises-21"><i class="fa fa-check"></i><b>11.6</b> Exercises</a></li>
<li class="chapter" data-level="11.7" data-path="robust-summaries.html"><a href="robust-summaries.html#case-study-self-reported-student-heights"><i class="fa fa-check"></i><b>11.7</b> Case study: self-reported student heights</a></li>
</ul></li>
<li class="part"><span><b>III Statistics with R</b></span></li>
<li class="chapter" data-level="12" data-path="introduction-to-statistics-with-r.html"><a href="introduction-to-statistics-with-r.html"><i class="fa fa-check"></i><b>12</b> Introduction to statistics with R</a></li>
<li class="chapter" data-level="13" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>13</b> Probability</a><ul>
<li class="chapter" data-level="13.1" data-path="probability.html"><a href="probability.html#discrete-probability"><i class="fa fa-check"></i><b>13.1</b> Discrete probability</a><ul>
<li class="chapter" data-level="13.1.1" data-path="probability.html"><a href="probability.html#relative-frequency"><i class="fa fa-check"></i><b>13.1.1</b> Relative frequency</a></li>
<li class="chapter" data-level="13.1.2" data-path="probability.html"><a href="probability.html#notation"><i class="fa fa-check"></i><b>13.1.2</b> Notation</a></li>
<li class="chapter" data-level="13.1.3" data-path="probability.html"><a href="probability.html#probability-distributions"><i class="fa fa-check"></i><b>13.1.3</b> Probability distributions</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="probability.html"><a href="probability.html#monte-carlo-simulations-for-categorical-data"><i class="fa fa-check"></i><b>13.2</b> Monte Carlo simulations for categorical data</a><ul>
<li class="chapter" data-level="13.2.1" data-path="probability.html"><a href="probability.html#setting-the-random-seed"><i class="fa fa-check"></i><b>13.2.1</b> Setting the random seed</a></li>
<li class="chapter" data-level="13.2.2" data-path="probability.html"><a href="probability.html#with-and-without-replacement"><i class="fa fa-check"></i><b>13.2.2</b> With and without replacement</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="probability.html"><a href="probability.html#independence"><i class="fa fa-check"></i><b>13.3</b> Independence</a></li>
<li class="chapter" data-level="13.4" data-path="probability.html"><a href="probability.html#conditional-probabilities"><i class="fa fa-check"></i><b>13.4</b> Conditional probabilities</a></li>
<li class="chapter" data-level="13.5" data-path="probability.html"><a href="probability.html#addition-and-multiplication-rules"><i class="fa fa-check"></i><b>13.5</b> Addition and multiplication rules</a><ul>
<li class="chapter" data-level="13.5.1" data-path="probability.html"><a href="probability.html#multiplication-rule"><i class="fa fa-check"></i><b>13.5.1</b> Multiplication rule</a></li>
<li class="chapter" data-level="13.5.2" data-path="probability.html"><a href="probability.html#multiplication-rule-under-independence"><i class="fa fa-check"></i><b>13.5.2</b> Multiplication rule under independence</a></li>
<li class="chapter" data-level="13.5.3" data-path="probability.html"><a href="probability.html#addition-rule"><i class="fa fa-check"></i><b>13.5.3</b> Addition rule</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="probability.html"><a href="probability.html#combinations-and-permutations"><i class="fa fa-check"></i><b>13.6</b> Combinations and permutations</a><ul>
<li class="chapter" data-level="13.6.1" data-path="probability.html"><a href="probability.html#monte-carlo-example"><i class="fa fa-check"></i><b>13.6.1</b> Monte Carlo example</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="probability.html"><a href="probability.html#examples"><i class="fa fa-check"></i><b>13.7</b> Examples</a><ul>
<li class="chapter" data-level="13.7.1" data-path="probability.html"><a href="probability.html#monty-hall-problem"><i class="fa fa-check"></i><b>13.7.1</b> Monty Hall problem</a></li>
<li class="chapter" data-level="13.7.2" data-path="probability.html"><a href="probability.html#birthday-problem"><i class="fa fa-check"></i><b>13.7.2</b> Birthday problem</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="probability.html"><a href="probability.html#infinity-in-practice"><i class="fa fa-check"></i><b>13.8</b> Infinity in practice</a></li>
<li class="chapter" data-level="13.9" data-path="probability.html"><a href="probability.html#exercises-22"><i class="fa fa-check"></i><b>13.9</b> Exercises</a></li>
<li class="chapter" data-level="13.10" data-path="probability.html"><a href="probability.html#continuous-probability"><i class="fa fa-check"></i><b>13.10</b> Continuous probability</a></li>
<li class="chapter" data-level="13.11" data-path="probability.html"><a href="probability.html#theoretical-continuous-distributions"><i class="fa fa-check"></i><b>13.11</b> Theoretical continuous distributions</a><ul>
<li class="chapter" data-level="13.11.1" data-path="probability.html"><a href="probability.html#theoretical-distributions-as-approximations"><i class="fa fa-check"></i><b>13.11.1</b> Theoretical distributions as approximations</a></li>
<li class="chapter" data-level="13.11.2" data-path="probability.html"><a href="probability.html#the-probability-density"><i class="fa fa-check"></i><b>13.11.2</b> The probability density</a></li>
</ul></li>
<li class="chapter" data-level="13.12" data-path="probability.html"><a href="probability.html#monte-carlo-simulations-for-continuous-variables"><i class="fa fa-check"></i><b>13.12</b> Monte Carlo simulations for continuous variables</a></li>
<li class="chapter" data-level="13.13" data-path="probability.html"><a href="probability.html#continuous-distributions"><i class="fa fa-check"></i><b>13.13</b> Continuous distributions</a></li>
<li class="chapter" data-level="13.14" data-path="probability.html"><a href="probability.html#exercises-23"><i class="fa fa-check"></i><b>13.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="random-variables.html"><a href="random-variables.html"><i class="fa fa-check"></i><b>14</b> Random variables</a><ul>
<li class="chapter" data-level="14.1" data-path="random-variables.html"><a href="random-variables.html#random-variables-1"><i class="fa fa-check"></i><b>14.1</b> Random variables</a></li>
<li class="chapter" data-level="14.2" data-path="random-variables.html"><a href="random-variables.html#sampling-models"><i class="fa fa-check"></i><b>14.2</b> Sampling models</a></li>
<li class="chapter" data-level="14.3" data-path="random-variables.html"><a href="random-variables.html#the-probability-distribution-of-a-random-variable"><i class="fa fa-check"></i><b>14.3</b> The probability distribution of a random variable</a></li>
<li class="chapter" data-level="14.4" data-path="random-variables.html"><a href="random-variables.html#distributions-versus-probability-distributions"><i class="fa fa-check"></i><b>14.4</b> Distributions versus probability distributions</a></li>
<li class="chapter" data-level="14.5" data-path="random-variables.html"><a href="random-variables.html#notation-for-random-variables"><i class="fa fa-check"></i><b>14.5</b> Notation for random variables</a></li>
<li class="chapter" data-level="14.6" data-path="random-variables.html"><a href="random-variables.html#the-expected-value-and-standard-error"><i class="fa fa-check"></i><b>14.6</b> The expected value and standard error</a><ul>
<li class="chapter" data-level="14.6.1" data-path="random-variables.html"><a href="random-variables.html#population-sd-versus-the-sample-sd"><i class="fa fa-check"></i><b>14.6.1</b> Population SD versus the sample SD</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="random-variables.html"><a href="random-variables.html#central-limit-theorem"><i class="fa fa-check"></i><b>14.7</b> Central Limit Theorem</a><ul>
<li class="chapter" data-level="14.7.1" data-path="random-variables.html"><a href="random-variables.html#how-large-is-large-in-the-central-limit-theorem"><i class="fa fa-check"></i><b>14.7.1</b> How large is large in the Central Limit Theorem?</a></li>
</ul></li>
<li class="chapter" data-level="14.8" data-path="random-variables.html"><a href="random-variables.html#statistical-properties-of-averages"><i class="fa fa-check"></i><b>14.8</b> Statistical properties of averages</a></li>
<li class="chapter" data-level="14.9" data-path="random-variables.html"><a href="random-variables.html#law-of-large-numbers"><i class="fa fa-check"></i><b>14.9</b> Law of large numbers</a><ul>
<li class="chapter" data-level="14.9.1" data-path="random-variables.html"><a href="random-variables.html#misinterpreting-law-of-averages"><i class="fa fa-check"></i><b>14.9.1</b> Misinterpreting law of averages</a></li>
</ul></li>
<li class="chapter" data-level="14.10" data-path="random-variables.html"><a href="random-variables.html#exercises-24"><i class="fa fa-check"></i><b>14.10</b> Exercises</a></li>
<li class="chapter" data-level="14.11" data-path="random-variables.html"><a href="random-variables.html#case-study-the-big-short"><i class="fa fa-check"></i><b>14.11</b> Case study: The Big Short</a><ul>
<li class="chapter" data-level="14.11.1" data-path="random-variables.html"><a href="random-variables.html#interest-rates-explained-with-chance-model"><i class="fa fa-check"></i><b>14.11.1</b> Interest rates explained with chance model</a></li>
<li class="chapter" data-level="14.11.2" data-path="random-variables.html"><a href="random-variables.html#the-big-short"><i class="fa fa-check"></i><b>14.11.2</b> The Big Short</a></li>
</ul></li>
<li class="chapter" data-level="14.12" data-path="random-variables.html"><a href="random-variables.html#exercises-25"><i class="fa fa-check"></i><b>14.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>15</b> Statistical inference</a><ul>
<li class="chapter" data-level="15.1" data-path="inference.html"><a href="inference.html#polls"><i class="fa fa-check"></i><b>15.1</b> Polls</a><ul>
<li class="chapter" data-level="15.1.1" data-path="inference.html"><a href="inference.html#the-sampling-model-for-polls"><i class="fa fa-check"></i><b>15.1.1</b> The sampling model for polls</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="inference.html"><a href="inference.html#populations-samples-parameters-and-estimates"><i class="fa fa-check"></i><b>15.2</b> Populations, samples, parameters, and estimates</a><ul>
<li class="chapter" data-level="15.2.1" data-path="inference.html"><a href="inference.html#the-sample-average"><i class="fa fa-check"></i><b>15.2.1</b> The sample average</a></li>
<li class="chapter" data-level="15.2.2" data-path="inference.html"><a href="inference.html#parameters"><i class="fa fa-check"></i><b>15.2.2</b> Parameters</a></li>
<li class="chapter" data-level="15.2.3" data-path="inference.html"><a href="inference.html#polling-versus-forecasting"><i class="fa fa-check"></i><b>15.2.3</b> Polling versus forecasting</a></li>
<li class="chapter" data-level="15.2.4" data-path="inference.html"><a href="inference.html#properties-of-our-estimate-expected-value-and-standard-error"><i class="fa fa-check"></i><b>15.2.4</b> Properties of our estimate: expected value and standard error</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="inference.html"><a href="inference.html#exercises-26"><i class="fa fa-check"></i><b>15.3</b> Exercises</a></li>
<li class="chapter" data-level="15.4" data-path="inference.html"><a href="inference.html#clt"><i class="fa fa-check"></i><b>15.4</b> Central Limit Theorem in practice</a><ul>
<li class="chapter" data-level="15.4.1" data-path="inference.html"><a href="inference.html#a-monte-carlo-simulation"><i class="fa fa-check"></i><b>15.4.1</b> A Monte Carlo simulation</a></li>
<li class="chapter" data-level="15.4.2" data-path="inference.html"><a href="inference.html#the-spread"><i class="fa fa-check"></i><b>15.4.2</b> The spread</a></li>
<li class="chapter" data-level="15.4.3" data-path="inference.html"><a href="inference.html#bias-why-not-run-a-very-large-poll"><i class="fa fa-check"></i><b>15.4.3</b> Bias: why not run a very large poll?</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="inference.html"><a href="inference.html#exercises-27"><i class="fa fa-check"></i><b>15.5</b> Exercises</a></li>
<li class="chapter" data-level="15.6" data-path="inference.html"><a href="inference.html#confidence-intervals"><i class="fa fa-check"></i><b>15.6</b> Confidence intervals</a><ul>
<li class="chapter" data-level="15.6.1" data-path="inference.html"><a href="inference.html#a-monte-carlo-simulation-1"><i class="fa fa-check"></i><b>15.6.1</b> A Monte Carlo simulation</a></li>
<li class="chapter" data-level="15.6.2" data-path="inference.html"><a href="inference.html#the-correct-language"><i class="fa fa-check"></i><b>15.6.2</b> The correct language</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="inference.html"><a href="inference.html#exercises-28"><i class="fa fa-check"></i><b>15.7</b> Exercises</a></li>
<li class="chapter" data-level="15.8" data-path="inference.html"><a href="inference.html#power"><i class="fa fa-check"></i><b>15.8</b> Power</a></li>
<li class="chapter" data-level="15.9" data-path="inference.html"><a href="inference.html#p-values"><i class="fa fa-check"></i><b>15.9</b> p-values</a></li>
<li class="chapter" data-level="15.10" data-path="inference.html"><a href="inference.html#association-tests"><i class="fa fa-check"></i><b>15.10</b> Association tests</a><ul>
<li class="chapter" data-level="15.10.1" data-path="inference.html"><a href="inference.html#lady-tasting-tea"><i class="fa fa-check"></i><b>15.10.1</b> Lady Tasting Tea</a></li>
<li class="chapter" data-level="15.10.2" data-path="inference.html"><a href="inference.html#two-by-two-tables"><i class="fa fa-check"></i><b>15.10.2</b> Two-by-two tables</a></li>
<li class="chapter" data-level="15.10.3" data-path="inference.html"><a href="inference.html#chi-square-test"><i class="fa fa-check"></i><b>15.10.3</b> Chi-square Test</a></li>
<li class="chapter" data-level="15.10.4" data-path="inference.html"><a href="inference.html#odds-ratio"><i class="fa fa-check"></i><b>15.10.4</b> The odds ratio</a></li>
<li class="chapter" data-level="15.10.5" data-path="inference.html"><a href="inference.html#confidence-intervals-for-the-odds-ratio"><i class="fa fa-check"></i><b>15.10.5</b> Confidence intervals for the odds ratio</a></li>
<li class="chapter" data-level="15.10.6" data-path="inference.html"><a href="inference.html#small-count-correction"><i class="fa fa-check"></i><b>15.10.6</b> Small count correction</a></li>
<li class="chapter" data-level="15.10.7" data-path="inference.html"><a href="inference.html#large-samples-small-p-values"><i class="fa fa-check"></i><b>15.10.7</b> Large samples, small p-values</a></li>
</ul></li>
<li class="chapter" data-level="15.11" data-path="inference.html"><a href="inference.html#exercises-29"><i class="fa fa-check"></i><b>15.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>16</b> Statistical models</a><ul>
<li class="chapter" data-level="16.1" data-path="models.html"><a href="models.html#poll-aggregators"><i class="fa fa-check"></i><b>16.1</b> Poll aggregators</a><ul>
<li class="chapter" data-level="16.1.1" data-path="models.html"><a href="models.html#poll-data"><i class="fa fa-check"></i><b>16.1.1</b> Poll data</a></li>
<li class="chapter" data-level="16.1.2" data-path="models.html"><a href="models.html#pollster-bias"><i class="fa fa-check"></i><b>16.1.2</b> Pollster bias</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="models.html"><a href="models.html#data-driven-model"><i class="fa fa-check"></i><b>16.2</b> Data-driven models</a></li>
<li class="chapter" data-level="16.3" data-path="models.html"><a href="models.html#exercises-30"><i class="fa fa-check"></i><b>16.3</b> Exercises</a></li>
<li class="chapter" data-level="16.4" data-path="models.html"><a href="models.html#bayesian-statistics"><i class="fa fa-check"></i><b>16.4</b> Bayesian statistics</a><ul>
<li class="chapter" data-level="16.4.1" data-path="models.html"><a href="models.html#bayes-theorem"><i class="fa fa-check"></i><b>16.4.1</b> Bayes theorem</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="models.html"><a href="models.html#bayes-theorem-simulation"><i class="fa fa-check"></i><b>16.5</b> Bayes theorem simulation</a><ul>
<li class="chapter" data-level="16.5.1" data-path="models.html"><a href="models.html#bayes-in-practice"><i class="fa fa-check"></i><b>16.5.1</b> Bayes in practice</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="models.html"><a href="models.html#hierarchical-models"><i class="fa fa-check"></i><b>16.6</b> Hierarchical models</a></li>
<li class="chapter" data-level="16.7" data-path="models.html"><a href="models.html#exercises-31"><i class="fa fa-check"></i><b>16.7</b> Exercises</a></li>
<li class="chapter" data-level="16.8" data-path="models.html"><a href="models.html#election-forecasting"><i class="fa fa-check"></i><b>16.8</b> Case study: election forecasting</a><ul>
<li class="chapter" data-level="16.8.1" data-path="models.html"><a href="models.html#bayesian-approach"><i class="fa fa-check"></i><b>16.8.1</b> Bayesian approach</a></li>
<li class="chapter" data-level="16.8.2" data-path="models.html"><a href="models.html#the-general-bias"><i class="fa fa-check"></i><b>16.8.2</b> The general bias</a></li>
<li class="chapter" data-level="16.8.3" data-path="models.html"><a href="models.html#mathematical-representations-of-models"><i class="fa fa-check"></i><b>16.8.3</b> Mathematical representations of models</a></li>
<li class="chapter" data-level="16.8.4" data-path="models.html"><a href="models.html#predicting-the-electoral-college"><i class="fa fa-check"></i><b>16.8.4</b> Predicting the electoral college</a></li>
<li class="chapter" data-level="16.8.5" data-path="models.html"><a href="models.html#forecasting"><i class="fa fa-check"></i><b>16.8.5</b> Forecasting</a></li>
</ul></li>
<li class="chapter" data-level="16.9" data-path="models.html"><a href="models.html#exercises-32"><i class="fa fa-check"></i><b>16.9</b> Exercises</a></li>
<li class="chapter" data-level="16.10" data-path="models.html"><a href="models.html#t-dist"><i class="fa fa-check"></i><b>16.10</b> The t-distribution</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>17</b> Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="regression.html"><a href="regression.html#case-study-is-height-hereditary"><i class="fa fa-check"></i><b>17.1</b> Case study: is height hereditary?</a></li>
<li class="chapter" data-level="17.2" data-path="regression.html"><a href="regression.html#corr-coef"><i class="fa fa-check"></i><b>17.2</b> The correlation coefficient</a><ul>
<li class="chapter" data-level="17.2.1" data-path="regression.html"><a href="regression.html#sample-correlation-is-a-random-variable"><i class="fa fa-check"></i><b>17.2.1</b> Sample correlation is a random variable</a></li>
<li class="chapter" data-level="17.2.2" data-path="regression.html"><a href="regression.html#correlation-is-not-always-a-useful-summary"><i class="fa fa-check"></i><b>17.2.2</b> Correlation is not always a useful summary</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="regression.html"><a href="regression.html#conditional-expectation"><i class="fa fa-check"></i><b>17.3</b> Conditional expectations</a></li>
<li class="chapter" data-level="17.4" data-path="regression.html"><a href="regression.html#the-regression-line"><i class="fa fa-check"></i><b>17.4</b> The regression line</a><ul>
<li class="chapter" data-level="17.4.1" data-path="regression.html"><a href="regression.html#regression-improves-precision"><i class="fa fa-check"></i><b>17.4.1</b> Regression improves precision</a></li>
<li class="chapter" data-level="17.4.2" data-path="regression.html"><a href="regression.html#bivariate-normal-distribution-advanced"><i class="fa fa-check"></i><b>17.4.2</b> Bivariate normal distribution (advanced)</a></li>
<li class="chapter" data-level="17.4.3" data-path="regression.html"><a href="regression.html#variance-explained"><i class="fa fa-check"></i><b>17.4.3</b> Variance explained</a></li>
<li class="chapter" data-level="17.4.4" data-path="regression.html"><a href="regression.html#warning-there-are-two-regression-lines"><i class="fa fa-check"></i><b>17.4.4</b> Warning: there are two regression lines</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="regression.html"><a href="regression.html#exercises-33"><i class="fa fa-check"></i><b>17.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>18</b> Linear models</a><ul>
<li class="chapter" data-level="18.1" data-path="linear-models.html"><a href="linear-models.html#case-study-moneyball"><i class="fa fa-check"></i><b>18.1</b> Case study: Moneyball</a><ul>
<li class="chapter" data-level="18.1.1" data-path="linear-models.html"><a href="linear-models.html#sabermetics"><i class="fa fa-check"></i><b>18.1.1</b> Sabermetics</a></li>
<li class="chapter" data-level="18.1.2" data-path="linear-models.html"><a href="linear-models.html#baseball-basics"><i class="fa fa-check"></i><b>18.1.2</b> Baseball basics</a></li>
<li class="chapter" data-level="18.1.3" data-path="linear-models.html"><a href="linear-models.html#no-awards-for-bb"><i class="fa fa-check"></i><b>18.1.3</b> No awards for BB</a></li>
<li class="chapter" data-level="18.1.4" data-path="linear-models.html"><a href="linear-models.html#base-on-balls-or-stolen-bases"><i class="fa fa-check"></i><b>18.1.4</b> Base on balls or stolen bases?</a></li>
<li class="chapter" data-level="18.1.5" data-path="linear-models.html"><a href="linear-models.html#regression-applied-to-baseball-statistics"><i class="fa fa-check"></i><b>18.1.5</b> Regression applied to baseball statistics</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="linear-models.html"><a href="linear-models.html#confounding"><i class="fa fa-check"></i><b>18.2</b> Confounding</a><ul>
<li class="chapter" data-level="18.2.1" data-path="linear-models.html"><a href="linear-models.html#understanding-confounding-through-stratification"><i class="fa fa-check"></i><b>18.2.1</b> Understanding confounding through stratification</a></li>
<li class="chapter" data-level="18.2.2" data-path="linear-models.html"><a href="linear-models.html#multivariate-regression"><i class="fa fa-check"></i><b>18.2.2</b> Multivariate regression</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="linear-models.html"><a href="linear-models.html#lse"><i class="fa fa-check"></i><b>18.3</b> Least squares estimates</a><ul>
<li class="chapter" data-level="18.3.1" data-path="linear-models.html"><a href="linear-models.html#interpreting-linear-models"><i class="fa fa-check"></i><b>18.3.1</b> Interpreting linear models</a></li>
<li class="chapter" data-level="18.3.2" data-path="linear-models.html"><a href="linear-models.html#least-squares-estimates-lse"><i class="fa fa-check"></i><b>18.3.2</b> Least Squares Estimates (LSE)</a></li>
<li class="chapter" data-level="18.3.3" data-path="linear-models.html"><a href="linear-models.html#the-lm-function"><i class="fa fa-check"></i><b>18.3.3</b> The <code>lm</code> function</a></li>
<li class="chapter" data-level="18.3.4" data-path="linear-models.html"><a href="linear-models.html#lse-are-random-variables"><i class="fa fa-check"></i><b>18.3.4</b> LSE are random variables</a></li>
<li class="chapter" data-level="18.3.5" data-path="linear-models.html"><a href="linear-models.html#predicted-values-are-random-variables"><i class="fa fa-check"></i><b>18.3.5</b> Predicted values are random variables</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="linear-models.html"><a href="linear-models.html#exercises-34"><i class="fa fa-check"></i><b>18.4</b> Exercises</a></li>
<li class="chapter" data-level="18.5" data-path="linear-models.html"><a href="linear-models.html#linear-regression-in-the-tidyverse"><i class="fa fa-check"></i><b>18.5</b> Linear regression in the tidyverse</a><ul>
<li class="chapter" data-level="18.5.1" data-path="linear-models.html"><a href="linear-models.html#the-broom-package"><i class="fa fa-check"></i><b>18.5.1</b> The broom package</a></li>
</ul></li>
<li class="chapter" data-level="18.6" data-path="linear-models.html"><a href="linear-models.html#exercises-35"><i class="fa fa-check"></i><b>18.6</b> Exercises</a></li>
<li class="chapter" data-level="18.7" data-path="linear-models.html"><a href="linear-models.html#case-study-moneyball-continued"><i class="fa fa-check"></i><b>18.7</b> Case study: Moneyball (continued)</a><ul>
<li class="chapter" data-level="18.7.1" data-path="linear-models.html"><a href="linear-models.html#adding-salary-and-position-information"><i class="fa fa-check"></i><b>18.7.1</b> Adding salary and position information</a></li>
<li class="chapter" data-level="18.7.2" data-path="linear-models.html"><a href="linear-models.html#picking-nine-players"><i class="fa fa-check"></i><b>18.7.2</b> Picking nine players</a></li>
</ul></li>
<li class="chapter" data-level="18.8" data-path="linear-models.html"><a href="linear-models.html#the-regression-fallacy"><i class="fa fa-check"></i><b>18.8</b> The regression fallacy</a></li>
<li class="chapter" data-level="18.9" data-path="linear-models.html"><a href="linear-models.html#measurement-error-models"><i class="fa fa-check"></i><b>18.9</b> Measurement error models</a></li>
<li class="chapter" data-level="18.10" data-path="linear-models.html"><a href="linear-models.html#exercises-36"><i class="fa fa-check"></i><b>18.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html"><i class="fa fa-check"></i><b>19</b> Association is not causation</a><ul>
<li class="chapter" data-level="19.1" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#spurious-correlation"><i class="fa fa-check"></i><b>19.1</b> Spurious correlation</a></li>
<li class="chapter" data-level="19.2" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#outliers-1"><i class="fa fa-check"></i><b>19.2</b> Outliers</a></li>
<li class="chapter" data-level="19.3" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#reversing-cause-and-effect"><i class="fa fa-check"></i><b>19.3</b> Reversing cause and effect</a></li>
<li class="chapter" data-level="19.4" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#confounders"><i class="fa fa-check"></i><b>19.4</b> Confounders</a><ul>
<li class="chapter" data-level="19.4.1" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#example-uc-berkeley-admissions"><i class="fa fa-check"></i><b>19.4.1</b> Example: UC Berkeley admissions</a></li>
<li class="chapter" data-level="19.4.2" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#confounding-explained-graphically"><i class="fa fa-check"></i><b>19.4.2</b> Confounding explained graphically</a></li>
<li class="chapter" data-level="19.4.3" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#average-after-stratifying"><i class="fa fa-check"></i><b>19.4.3</b> Average after stratifying</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#simpsons-paradox"><i class="fa fa-check"></i><b>19.5</b> Simpson’s paradox</a></li>
<li class="chapter" data-level="19.6" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#exercises-37"><i class="fa fa-check"></i><b>19.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Data Wrangling</b></span></li>
<li class="chapter" data-level="20" data-path="introduction-to-data-wrangling.html"><a href="introduction-to-data-wrangling.html"><i class="fa fa-check"></i><b>20</b> Introduction to data wrangling</a></li>
<li class="chapter" data-level="21" data-path="reshaping-data.html"><a href="reshaping-data.html"><i class="fa fa-check"></i><b>21</b> Reshaping data</a><ul>
<li class="chapter" data-level="21.1" data-path="reshaping-data.html"><a href="reshaping-data.html#gather"><i class="fa fa-check"></i><b>21.1</b> <code>gather</code></a></li>
<li class="chapter" data-level="21.2" data-path="reshaping-data.html"><a href="reshaping-data.html#spread"><i class="fa fa-check"></i><b>21.2</b> <code>spread</code></a></li>
<li class="chapter" data-level="21.3" data-path="reshaping-data.html"><a href="reshaping-data.html#separate"><i class="fa fa-check"></i><b>21.3</b> <code>separate</code></a></li>
<li class="chapter" data-level="21.4" data-path="reshaping-data.html"><a href="reshaping-data.html#unite"><i class="fa fa-check"></i><b>21.4</b> <code>unite</code></a></li>
<li class="chapter" data-level="21.5" data-path="reshaping-data.html"><a href="reshaping-data.html#exercises-38"><i class="fa fa-check"></i><b>21.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="joining-tables.html"><a href="joining-tables.html"><i class="fa fa-check"></i><b>22</b> Joining tables</a><ul>
<li class="chapter" data-level="22.1" data-path="joining-tables.html"><a href="joining-tables.html#joins"><i class="fa fa-check"></i><b>22.1</b> Joins</a><ul>
<li class="chapter" data-level="22.1.1" data-path="joining-tables.html"><a href="joining-tables.html#left-join"><i class="fa fa-check"></i><b>22.1.1</b> Left join</a></li>
<li class="chapter" data-level="22.1.2" data-path="joining-tables.html"><a href="joining-tables.html#right-join"><i class="fa fa-check"></i><b>22.1.2</b> Right join</a></li>
<li class="chapter" data-level="22.1.3" data-path="joining-tables.html"><a href="joining-tables.html#inner-join"><i class="fa fa-check"></i><b>22.1.3</b> Inner join</a></li>
<li class="chapter" data-level="22.1.4" data-path="joining-tables.html"><a href="joining-tables.html#full-join"><i class="fa fa-check"></i><b>22.1.4</b> Full join</a></li>
<li class="chapter" data-level="22.1.5" data-path="joining-tables.html"><a href="joining-tables.html#semi-join"><i class="fa fa-check"></i><b>22.1.5</b> Semi join</a></li>
<li class="chapter" data-level="22.1.6" data-path="joining-tables.html"><a href="joining-tables.html#anti-join"><i class="fa fa-check"></i><b>22.1.6</b> Anti join</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="joining-tables.html"><a href="joining-tables.html#binding"><i class="fa fa-check"></i><b>22.2</b> Binding</a><ul>
<li class="chapter" data-level="22.2.1" data-path="joining-tables.html"><a href="joining-tables.html#binding-columns"><i class="fa fa-check"></i><b>22.2.1</b> Binding columns</a></li>
<li class="chapter" data-level="22.2.2" data-path="joining-tables.html"><a href="joining-tables.html#binding-by-rows"><i class="fa fa-check"></i><b>22.2.2</b> Binding by rows</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="joining-tables.html"><a href="joining-tables.html#set-operators"><i class="fa fa-check"></i><b>22.3</b> Set operators</a><ul>
<li class="chapter" data-level="22.3.1" data-path="joining-tables.html"><a href="joining-tables.html#intersect"><i class="fa fa-check"></i><b>22.3.1</b> Intersect</a></li>
<li class="chapter" data-level="22.3.2" data-path="joining-tables.html"><a href="joining-tables.html#union"><i class="fa fa-check"></i><b>22.3.2</b> Union</a></li>
<li class="chapter" data-level="22.3.3" data-path="joining-tables.html"><a href="joining-tables.html#setdiff"><i class="fa fa-check"></i><b>22.3.3</b> <code>setdiff</code></a></li>
<li class="chapter" data-level="22.3.4" data-path="joining-tables.html"><a href="joining-tables.html#setequal"><i class="fa fa-check"></i><b>22.3.4</b> <code>setequal</code></a></li>
</ul></li>
<li class="chapter" data-level="22.4" data-path="joining-tables.html"><a href="joining-tables.html#exercises-39"><i class="fa fa-check"></i><b>22.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="web-scraping.html"><a href="web-scraping.html"><i class="fa fa-check"></i><b>23</b> Web scraping</a><ul>
<li class="chapter" data-level="23.1" data-path="web-scraping.html"><a href="web-scraping.html#html"><i class="fa fa-check"></i><b>23.1</b> HTML</a></li>
<li class="chapter" data-level="23.2" data-path="web-scraping.html"><a href="web-scraping.html#the-rvest-package"><i class="fa fa-check"></i><b>23.2</b> The rvest package</a></li>
<li class="chapter" data-level="23.3" data-path="web-scraping.html"><a href="web-scraping.html#css-selectors"><i class="fa fa-check"></i><b>23.3</b> CSS selectors</a></li>
<li class="chapter" data-level="23.4" data-path="web-scraping.html"><a href="web-scraping.html#json"><i class="fa fa-check"></i><b>23.4</b> JSON</a></li>
<li class="chapter" data-level="23.5" data-path="web-scraping.html"><a href="web-scraping.html#exercises-40"><i class="fa fa-check"></i><b>23.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="string-processing.html"><a href="string-processing.html"><i class="fa fa-check"></i><b>24</b> String processing</a><ul>
<li class="chapter" data-level="24.1" data-path="string-processing.html"><a href="string-processing.html#stringr"><i class="fa fa-check"></i><b>24.1</b> The stringr package</a></li>
<li class="chapter" data-level="24.2" data-path="string-processing.html"><a href="string-processing.html#case-study-1-us-murders-data"><i class="fa fa-check"></i><b>24.2</b> Case study 1: US murders data</a></li>
<li class="chapter" data-level="24.3" data-path="string-processing.html"><a href="string-processing.html#case-study-2-self-reported-heights"><i class="fa fa-check"></i><b>24.3</b> Case study 2: self-reported heights</a></li>
<li class="chapter" data-level="24.4" data-path="string-processing.html"><a href="string-processing.html#how-to-escape-when-defining-strings"><i class="fa fa-check"></i><b>24.4</b> How to <em>escape</em> when defining strings</a></li>
<li class="chapter" data-level="24.5" data-path="string-processing.html"><a href="string-processing.html#regular-expressions"><i class="fa fa-check"></i><b>24.5</b> Regular expressions</a><ul>
<li class="chapter" data-level="24.5.1" data-path="string-processing.html"><a href="string-processing.html#strings-are-a-regexp"><i class="fa fa-check"></i><b>24.5.1</b> Strings are a regexp</a></li>
<li class="chapter" data-level="24.5.2" data-path="string-processing.html"><a href="string-processing.html#special-characters"><i class="fa fa-check"></i><b>24.5.2</b> Special characters</a></li>
<li class="chapter" data-level="24.5.3" data-path="string-processing.html"><a href="string-processing.html#character-classes"><i class="fa fa-check"></i><b>24.5.3</b> Character classes</a></li>
<li class="chapter" data-level="24.5.4" data-path="string-processing.html"><a href="string-processing.html#anchors"><i class="fa fa-check"></i><b>24.5.4</b> Anchors</a></li>
<li class="chapter" data-level="24.5.5" data-path="string-processing.html"><a href="string-processing.html#quantifiers"><i class="fa fa-check"></i><b>24.5.5</b> Quantifiers</a></li>
<li class="chapter" data-level="24.5.6" data-path="string-processing.html"><a href="string-processing.html#white-space-s"><i class="fa fa-check"></i><b>24.5.6</b> White space <code>\s</code></a></li>
<li class="chapter" data-level="24.5.7" data-path="string-processing.html"><a href="string-processing.html#quantifiers-1"><i class="fa fa-check"></i><b>24.5.7</b> Quantifiers: <code>*</code>, <code>?</code>, <code>+</code></a></li>
<li class="chapter" data-level="24.5.8" data-path="string-processing.html"><a href="string-processing.html#not"><i class="fa fa-check"></i><b>24.5.8</b> Not</a></li>
<li class="chapter" data-level="24.5.9" data-path="string-processing.html"><a href="string-processing.html#groups"><i class="fa fa-check"></i><b>24.5.9</b> Groups</a></li>
</ul></li>
<li class="chapter" data-level="24.6" data-path="string-processing.html"><a href="string-processing.html#search-and-replace-with-regex"><i class="fa fa-check"></i><b>24.6</b> Search and replace with regex</a><ul>
<li class="chapter" data-level="24.6.1" data-path="string-processing.html"><a href="string-processing.html#search-and-replace-using-groups"><i class="fa fa-check"></i><b>24.6.1</b> Search and replace using groups</a></li>
</ul></li>
<li class="chapter" data-level="24.7" data-path="string-processing.html"><a href="string-processing.html#testing-and-improving"><i class="fa fa-check"></i><b>24.7</b> Testing and improving</a></li>
<li class="chapter" data-level="24.8" data-path="string-processing.html"><a href="string-processing.html#trimming"><i class="fa fa-check"></i><b>24.8</b> Trimming</a></li>
<li class="chapter" data-level="24.9" data-path="string-processing.html"><a href="string-processing.html#changing-lettercase"><i class="fa fa-check"></i><b>24.9</b> Changing lettercase</a></li>
<li class="chapter" data-level="24.10" data-path="string-processing.html"><a href="string-processing.html#case-study-2-self-reported-heights-continued"><i class="fa fa-check"></i><b>24.10</b> Case study 2: self-reported heights (continued)</a><ul>
<li class="chapter" data-level="24.10.1" data-path="string-processing.html"><a href="string-processing.html#the-extract-function"><i class="fa fa-check"></i><b>24.10.1</b> The <code>extract</code> function</a></li>
<li class="chapter" data-level="24.10.2" data-path="string-processing.html"><a href="string-processing.html#putting-it-all-together-1"><i class="fa fa-check"></i><b>24.10.2</b> Putting it all together</a></li>
</ul></li>
<li class="chapter" data-level="24.11" data-path="string-processing.html"><a href="string-processing.html#string-splitting"><i class="fa fa-check"></i><b>24.11</b> String splitting</a></li>
<li class="chapter" data-level="24.12" data-path="string-processing.html"><a href="string-processing.html#case-study-3-extracting-tables-from-a-pdf"><i class="fa fa-check"></i><b>24.12</b> Case study 3: extracting tables from a PDF</a></li>
<li class="chapter" data-level="24.13" data-path="string-processing.html"><a href="string-processing.html#recode"><i class="fa fa-check"></i><b>24.13</b> Recoding</a></li>
<li class="chapter" data-level="24.14" data-path="string-processing.html"><a href="string-processing.html#exercises-41"><i class="fa fa-check"></i><b>24.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html"><i class="fa fa-check"></i><b>25</b> Parsing dates and times</a><ul>
<li class="chapter" data-level="25.1" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html#the-date-data-type"><i class="fa fa-check"></i><b>25.1</b> The date data type</a></li>
<li class="chapter" data-level="25.2" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html#lubridate"><i class="fa fa-check"></i><b>25.2</b> The lubridate package</a></li>
<li class="chapter" data-level="25.3" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html#exercises-42"><i class="fa fa-check"></i><b>25.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="text-mining.html"><a href="text-mining.html"><i class="fa fa-check"></i><b>26</b> Text mining</a><ul>
<li class="chapter" data-level="26.1" data-path="text-mining.html"><a href="text-mining.html#case-study-trump-tweets"><i class="fa fa-check"></i><b>26.1</b> Case study: Trump tweets</a></li>
<li class="chapter" data-level="26.2" data-path="text-mining.html"><a href="text-mining.html#text-as-data"><i class="fa fa-check"></i><b>26.2</b> Text as data</a></li>
<li class="chapter" data-level="26.3" data-path="text-mining.html"><a href="text-mining.html#sentiment-analysis"><i class="fa fa-check"></i><b>26.3</b> Sentiment analysis</a></li>
<li class="chapter" data-level="26.4" data-path="text-mining.html"><a href="text-mining.html#exercises-43"><i class="fa fa-check"></i><b>26.4</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Machine Learning</b></span></li>
<li class="chapter" data-level="27" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html"><i class="fa fa-check"></i><b>27</b> Introduction to machine learning</a><ul>
<li class="chapter" data-level="27.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#notation-1"><i class="fa fa-check"></i><b>27.1</b> Notation</a></li>
<li class="chapter" data-level="27.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#an-example"><i class="fa fa-check"></i><b>27.2</b> An example</a></li>
<li class="chapter" data-level="27.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#exercises-44"><i class="fa fa-check"></i><b>27.3</b> Exercises</a></li>
<li class="chapter" data-level="27.4" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#evaluation-metrics"><i class="fa fa-check"></i><b>27.4</b> Evaluation metrics</a><ul>
<li class="chapter" data-level="27.4.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#training-and-test-sets"><i class="fa fa-check"></i><b>27.4.1</b> Training and test sets</a></li>
<li class="chapter" data-level="27.4.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#overall-accuracy"><i class="fa fa-check"></i><b>27.4.2</b> Overall accuracy</a></li>
<li class="chapter" data-level="27.4.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#the-confusion-matrix"><i class="fa fa-check"></i><b>27.4.3</b> The confusion matrix</a></li>
<li class="chapter" data-level="27.4.4" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#sensitivity-and-specificity"><i class="fa fa-check"></i><b>27.4.4</b> Sensitivity and specificity</a></li>
<li class="chapter" data-level="27.4.5" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#balanced-accuracy-and-f_1-score"><i class="fa fa-check"></i><b>27.4.5</b> Balanced accuracy and <span class="math inline">\(F_1\)</span> score</a></li>
<li class="chapter" data-level="27.4.6" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#prevalence-matters-in-practice"><i class="fa fa-check"></i><b>27.4.6</b> Prevalence matters in practice</a></li>
<li class="chapter" data-level="27.4.7" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#roc-and-precision-recall-curves"><i class="fa fa-check"></i><b>27.4.7</b> ROC and precision-recall curves</a></li>
<li class="chapter" data-level="27.4.8" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#loss-function"><i class="fa fa-check"></i><b>27.4.8</b> The loss function</a></li>
</ul></li>
<li class="chapter" data-level="27.5" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#exercises-45"><i class="fa fa-check"></i><b>27.5</b> Exercises</a></li>
<li class="chapter" data-level="27.6" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-probabilities-and-expectations"><i class="fa fa-check"></i><b>27.6</b> Conditional probabilities and expectations</a><ul>
<li class="chapter" data-level="27.6.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-probabilities-1"><i class="fa fa-check"></i><b>27.6.1</b> Conditional probabilities</a></li>
<li class="chapter" data-level="27.6.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-expectations"><i class="fa fa-check"></i><b>27.6.2</b> Conditional expectations</a></li>
<li class="chapter" data-level="27.6.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-expectation-minimizes-squared-loss-function"><i class="fa fa-check"></i><b>27.6.3</b> Conditional expectation minimizes squared loss function</a></li>
</ul></li>
<li class="chapter" data-level="27.7" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#exercises-46"><i class="fa fa-check"></i><b>27.7</b> Exercises</a></li>
<li class="chapter" data-level="27.8" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#two-or-seven"><i class="fa fa-check"></i><b>27.8</b> Case study: is it a 2 or a 7?</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>28</b> Smoothing</a><ul>
<li class="chapter" data-level="28.1" data-path="smoothing.html"><a href="smoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>28.1</b> Bin smoothing</a></li>
<li class="chapter" data-level="28.2" data-path="smoothing.html"><a href="smoothing.html#kernels"><i class="fa fa-check"></i><b>28.2</b> Kernels</a></li>
<li class="chapter" data-level="28.3" data-path="smoothing.html"><a href="smoothing.html#local-weighted-regression-loess"><i class="fa fa-check"></i><b>28.3</b> Local weighted regression (loess)</a><ul>
<li class="chapter" data-level="28.3.1" data-path="smoothing.html"><a href="smoothing.html#fitting-parabolas"><i class="fa fa-check"></i><b>28.3.1</b> Fitting parabolas</a></li>
<li class="chapter" data-level="28.3.2" data-path="smoothing.html"><a href="smoothing.html#beware-of-default-smoothing-parameters"><i class="fa fa-check"></i><b>28.3.2</b> Beware of default smoothing parameters</a></li>
</ul></li>
<li class="chapter" data-level="28.4" data-path="smoothing.html"><a href="smoothing.html#smoothing-ml-connection"><i class="fa fa-check"></i><b>28.4</b> Connecting smoothing to machine learning</a></li>
<li class="chapter" data-level="28.5" data-path="smoothing.html"><a href="smoothing.html#exercises-47"><i class="fa fa-check"></i><b>28.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>29</b> Cross validation</a><ul>
<li class="chapter" data-level="29.1" data-path="cross-validation.html"><a href="cross-validation.html#knn-cv-intro"><i class="fa fa-check"></i><b>29.1</b> Motivation with k-nearest neighbors</a><ul>
<li class="chapter" data-level="29.1.1" data-path="cross-validation.html"><a href="cross-validation.html#over-training"><i class="fa fa-check"></i><b>29.1.1</b> Over-training</a></li>
<li class="chapter" data-level="29.1.2" data-path="cross-validation.html"><a href="cross-validation.html#over-smoothing"><i class="fa fa-check"></i><b>29.1.2</b> Over-smoothing</a></li>
<li class="chapter" data-level="29.1.3" data-path="cross-validation.html"><a href="cross-validation.html#picking-the-k-in-knn"><i class="fa fa-check"></i><b>29.1.3</b> Picking the <span class="math inline">\(k\)</span> in kNN</a></li>
</ul></li>
<li class="chapter" data-level="29.2" data-path="cross-validation.html"><a href="cross-validation.html#mathematical-description-of-cross-validation"><i class="fa fa-check"></i><b>29.2</b> Mathematical description of cross validation</a></li>
<li class="chapter" data-level="29.3" data-path="cross-validation.html"><a href="cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>29.3</b> K-fold cross validation</a></li>
<li class="chapter" data-level="29.4" data-path="cross-validation.html"><a href="cross-validation.html#exercises-48"><i class="fa fa-check"></i><b>29.4</b> Exercises</a></li>
<li class="chapter" data-level="29.5" data-path="cross-validation.html"><a href="cross-validation.html#bootstrap"><i class="fa fa-check"></i><b>29.5</b> Bootstrap</a></li>
<li class="chapter" data-level="29.6" data-path="cross-validation.html"><a href="cross-validation.html#exercises-49"><i class="fa fa-check"></i><b>29.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>30</b> The caret package</a><ul>
<li class="chapter" data-level="30.1" data-path="caret.html"><a href="caret.html#the-caret-train-functon"><i class="fa fa-check"></i><b>30.1</b> The caret <code>train</code> functon</a></li>
<li class="chapter" data-level="30.2" data-path="caret.html"><a href="caret.html#caret-cv"><i class="fa fa-check"></i><b>30.2</b> Cross validation</a></li>
<li class="chapter" data-level="30.3" data-path="caret.html"><a href="caret.html#example-fitting-with-loess"><i class="fa fa-check"></i><b>30.3</b> Example: fitting with loess</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html"><i class="fa fa-check"></i><b>31</b> Examples of algorithms</a><ul>
<li class="chapter" data-level="31.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#linear-regression"><i class="fa fa-check"></i><b>31.1</b> Linear regression</a><ul>
<li class="chapter" data-level="31.1.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#the-predict-function"><i class="fa fa-check"></i><b>31.1.1</b> The <code>predict</code> function</a></li>
</ul></li>
<li class="chapter" data-level="31.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-50"><i class="fa fa-check"></i><b>31.2</b> Exercises</a></li>
<li class="chapter" data-level="31.3" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#logistic-regression"><i class="fa fa-check"></i><b>31.3</b> Logistic regression</a><ul>
<li class="chapter" data-level="31.3.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#generalized-linear-models"><i class="fa fa-check"></i><b>31.3.1</b> Generalized linear models</a></li>
<li class="chapter" data-level="31.3.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#logistic-regression-with-more-than-one-predictor"><i class="fa fa-check"></i><b>31.3.2</b> Logistic regression with more than one predictor</a></li>
</ul></li>
<li class="chapter" data-level="31.4" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-51"><i class="fa fa-check"></i><b>31.4</b> Exercises</a></li>
<li class="chapter" data-level="31.5" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>31.5</b> k-nearest neighbors</a></li>
<li class="chapter" data-level="31.6" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-52"><i class="fa fa-check"></i><b>31.6</b> Exercises</a></li>
<li class="chapter" data-level="31.7" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#generative-models"><i class="fa fa-check"></i><b>31.7</b> Generative models</a><ul>
<li class="chapter" data-level="31.7.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#naive-bayes"><i class="fa fa-check"></i><b>31.7.1</b> Naive Bayes</a></li>
<li class="chapter" data-level="31.7.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#controlling-prevalence"><i class="fa fa-check"></i><b>31.7.2</b> Controlling prevalence</a></li>
<li class="chapter" data-level="31.7.3" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>31.7.3</b> Quadratic discriminant analysis</a></li>
<li class="chapter" data-level="31.7.4" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>31.7.4</b> Linear discriminant analysis</a></li>
<li class="chapter" data-level="31.7.5" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#connection-to-distance"><i class="fa fa-check"></i><b>31.7.5</b> Connection to distance</a></li>
</ul></li>
<li class="chapter" data-level="31.8" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#case-study-more-than-three-classes"><i class="fa fa-check"></i><b>31.8</b> Case study: more than three classes</a></li>
<li class="chapter" data-level="31.9" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-53"><i class="fa fa-check"></i><b>31.9</b> Exercises</a></li>
<li class="chapter" data-level="31.10" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#classification-and-regression-trees-cart"><i class="fa fa-check"></i><b>31.10</b> Classification and regression trees (CART)</a><ul>
<li class="chapter" data-level="31.10.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>31.10.1</b> The curse of dimensionality</a></li>
<li class="chapter" data-level="31.10.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#cart-motivation"><i class="fa fa-check"></i><b>31.10.2</b> CART motivation</a></li>
<li class="chapter" data-level="31.10.3" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#regression-trees"><i class="fa fa-check"></i><b>31.10.3</b> Regression trees</a></li>
<li class="chapter" data-level="31.10.4" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#classification-decision-trees"><i class="fa fa-check"></i><b>31.10.4</b> Classification (decision) trees</a></li>
</ul></li>
<li class="chapter" data-level="31.11" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#random-forests"><i class="fa fa-check"></i><b>31.11</b> Random forests</a></li>
<li class="chapter" data-level="31.12" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-54"><i class="fa fa-check"></i><b>31.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html"><i class="fa fa-check"></i><b>32</b> Machine learning in practice</a><ul>
<li class="chapter" data-level="32.1" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#preprocessing"><i class="fa fa-check"></i><b>32.1</b> Preprocessing</a></li>
<li class="chapter" data-level="32.2" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#k-nearest-neighbor-and-random-forest"><i class="fa fa-check"></i><b>32.2</b> k-nearest neighbor and random forest</a></li>
<li class="chapter" data-level="32.3" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#variable-importance"><i class="fa fa-check"></i><b>32.3</b> Variable importance</a></li>
<li class="chapter" data-level="32.4" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#visual-assessments"><i class="fa fa-check"></i><b>32.4</b> Visual assessments</a></li>
<li class="chapter" data-level="32.5" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#ensembles"><i class="fa fa-check"></i><b>32.5</b> Ensembles</a></li>
<li class="chapter" data-level="32.6" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#exercises-55"><i class="fa fa-check"></i><b>32.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="33" data-path="large-datasets.html"><a href="large-datasets.html"><i class="fa fa-check"></i><b>33</b> Large datasets</a><ul>
<li class="chapter" data-level="33.1" data-path="large-datasets.html"><a href="large-datasets.html#matrix-algebra"><i class="fa fa-check"></i><b>33.1</b> Matrix algebra</a><ul>
<li class="chapter" data-level="33.1.1" data-path="large-datasets.html"><a href="large-datasets.html#notation-2"><i class="fa fa-check"></i><b>33.1.1</b> Notation</a></li>
<li class="chapter" data-level="33.1.2" data-path="large-datasets.html"><a href="large-datasets.html#converting-a-vector-to-a-matrix"><i class="fa fa-check"></i><b>33.1.2</b> Converting a vector to a matrix</a></li>
<li class="chapter" data-level="33.1.3" data-path="large-datasets.html"><a href="large-datasets.html#row-and-column-summaries"><i class="fa fa-check"></i><b>33.1.3</b> Row and column summaries</a></li>
<li class="chapter" data-level="33.1.4" data-path="large-datasets.html"><a href="large-datasets.html#apply"><i class="fa fa-check"></i><b>33.1.4</b> <code>apply</code></a></li>
<li class="chapter" data-level="33.1.5" data-path="large-datasets.html"><a href="large-datasets.html#filtering-columns-based-on-summaries"><i class="fa fa-check"></i><b>33.1.5</b> Filtering columns based on summaries</a></li>
<li class="chapter" data-level="33.1.6" data-path="large-datasets.html"><a href="large-datasets.html#indexing-with-matrices"><i class="fa fa-check"></i><b>33.1.6</b> Indexing with matrices</a></li>
<li class="chapter" data-level="33.1.7" data-path="large-datasets.html"><a href="large-datasets.html#binarizing-the-data"><i class="fa fa-check"></i><b>33.1.7</b> Binarizing the data</a></li>
<li class="chapter" data-level="33.1.8" data-path="large-datasets.html"><a href="large-datasets.html#vectorization-for-matrices"><i class="fa fa-check"></i><b>33.1.8</b> Vectorization for matrices</a></li>
<li class="chapter" data-level="33.1.9" data-path="large-datasets.html"><a href="large-datasets.html#matrix-algebra-operations"><i class="fa fa-check"></i><b>33.1.9</b> Matrix algebra operations</a></li>
</ul></li>
<li class="chapter" data-level="33.2" data-path="large-datasets.html"><a href="large-datasets.html#exercises-56"><i class="fa fa-check"></i><b>33.2</b> Exercises</a></li>
<li class="chapter" data-level="33.3" data-path="large-datasets.html"><a href="large-datasets.html#distance"><i class="fa fa-check"></i><b>33.3</b> Distance</a><ul>
<li class="chapter" data-level="33.3.1" data-path="large-datasets.html"><a href="large-datasets.html#euclidean-distance"><i class="fa fa-check"></i><b>33.3.1</b> Euclidean distance</a></li>
<li class="chapter" data-level="33.3.2" data-path="large-datasets.html"><a href="large-datasets.html#distance-in-higher-dimensions"><i class="fa fa-check"></i><b>33.3.2</b> Distance in higher dimensions</a></li>
<li class="chapter" data-level="33.3.3" data-path="large-datasets.html"><a href="large-datasets.html#euclidean-distance-example"><i class="fa fa-check"></i><b>33.3.3</b> Euclidean distance example</a></li>
<li class="chapter" data-level="33.3.4" data-path="large-datasets.html"><a href="large-datasets.html#predictor-space"><i class="fa fa-check"></i><b>33.3.4</b> Predictor space</a></li>
<li class="chapter" data-level="33.3.5" data-path="large-datasets.html"><a href="large-datasets.html#distance-between-predictors"><i class="fa fa-check"></i><b>33.3.5</b> Distance between predictors</a></li>
</ul></li>
<li class="chapter" data-level="33.4" data-path="large-datasets.html"><a href="large-datasets.html#exercises-57"><i class="fa fa-check"></i><b>33.4</b> Exercises</a></li>
<li class="chapter" data-level="33.5" data-path="large-datasets.html"><a href="large-datasets.html#dimension-reduction"><i class="fa fa-check"></i><b>33.5</b> Dimension reduction</a><ul>
<li class="chapter" data-level="33.5.1" data-path="large-datasets.html"><a href="large-datasets.html#preserving-distance"><i class="fa fa-check"></i><b>33.5.1</b> Preserving distance</a></li>
<li class="chapter" data-level="33.5.2" data-path="large-datasets.html"><a href="large-datasets.html#linear-transformations-advanced"><i class="fa fa-check"></i><b>33.5.2</b> Linear transformations (advanced)</a></li>
<li class="chapter" data-level="33.5.3" data-path="large-datasets.html"><a href="large-datasets.html#orthogonal-transformations-advanced"><i class="fa fa-check"></i><b>33.5.3</b> Orthogonal transformations (advanced)</a></li>
<li class="chapter" data-level="33.5.4" data-path="large-datasets.html"><a href="large-datasets.html#principal-component-analysis"><i class="fa fa-check"></i><b>33.5.4</b> Principal component analysis</a></li>
<li class="chapter" data-level="33.5.5" data-path="large-datasets.html"><a href="large-datasets.html#iris-example"><i class="fa fa-check"></i><b>33.5.5</b> Iris example</a></li>
<li class="chapter" data-level="33.5.6" data-path="large-datasets.html"><a href="large-datasets.html#mnist-example"><i class="fa fa-check"></i><b>33.5.6</b> MNIST example</a></li>
</ul></li>
<li class="chapter" data-level="33.6" data-path="large-datasets.html"><a href="large-datasets.html#exercises-58"><i class="fa fa-check"></i><b>33.6</b> Exercises</a></li>
<li class="chapter" data-level="33.7" data-path="large-datasets.html"><a href="large-datasets.html#recommendation-systems"><i class="fa fa-check"></i><b>33.7</b> Recommendation systems</a><ul>
<li class="chapter" data-level="33.7.1" data-path="large-datasets.html"><a href="large-datasets.html#movielens-data"><i class="fa fa-check"></i><b>33.7.1</b> Movielens data</a></li>
<li class="chapter" data-level="33.7.2" data-path="large-datasets.html"><a href="large-datasets.html#recommendation-systems-as-a-machine-learning-challenge"><i class="fa fa-check"></i><b>33.7.2</b> Recommendation systems as a machine learning challenge</a></li>
<li class="chapter" data-level="33.7.3" data-path="large-datasets.html"><a href="large-datasets.html#netflix-loss-function"><i class="fa fa-check"></i><b>33.7.3</b> Loss function</a></li>
<li class="chapter" data-level="33.7.4" data-path="large-datasets.html"><a href="large-datasets.html#a-first-model"><i class="fa fa-check"></i><b>33.7.4</b> A first model</a></li>
<li class="chapter" data-level="33.7.5" data-path="large-datasets.html"><a href="large-datasets.html#modeling-movie-effects"><i class="fa fa-check"></i><b>33.7.5</b> Modeling movie effects</a></li>
<li class="chapter" data-level="33.7.6" data-path="large-datasets.html"><a href="large-datasets.html#user-effects"><i class="fa fa-check"></i><b>33.7.6</b> User effects</a></li>
</ul></li>
<li class="chapter" data-level="33.8" data-path="large-datasets.html"><a href="large-datasets.html#exercises-59"><i class="fa fa-check"></i><b>33.8</b> Exercises</a></li>
<li class="chapter" data-level="33.9" data-path="large-datasets.html"><a href="large-datasets.html#regularization"><i class="fa fa-check"></i><b>33.9</b> Regularization</a><ul>
<li class="chapter" data-level="33.9.1" data-path="large-datasets.html"><a href="large-datasets.html#motivation"><i class="fa fa-check"></i><b>33.9.1</b> Motivation</a></li>
<li class="chapter" data-level="33.9.2" data-path="large-datasets.html"><a href="large-datasets.html#penalized-least-squares"><i class="fa fa-check"></i><b>33.9.2</b> Penalized least squares</a></li>
<li class="chapter" data-level="33.9.3" data-path="large-datasets.html"><a href="large-datasets.html#choosing-the-penalty-terms"><i class="fa fa-check"></i><b>33.9.3</b> Choosing the penalty terms</a></li>
</ul></li>
<li class="chapter" data-level="33.10" data-path="large-datasets.html"><a href="large-datasets.html#exercises-60"><i class="fa fa-check"></i><b>33.10</b> Exercises</a></li>
<li class="chapter" data-level="33.11" data-path="large-datasets.html"><a href="large-datasets.html#matrix-factorization"><i class="fa fa-check"></i><b>33.11</b> Matrix factorization</a><ul>
<li class="chapter" data-level="33.11.1" data-path="large-datasets.html"><a href="large-datasets.html#factors-analysis"><i class="fa fa-check"></i><b>33.11.1</b> Factors analysis</a></li>
<li class="chapter" data-level="33.11.2" data-path="large-datasets.html"><a href="large-datasets.html#connection-to-svd-and-pca"><i class="fa fa-check"></i><b>33.11.2</b> Connection to SVD and PCA</a></li>
</ul></li>
<li class="chapter" data-level="33.12" data-path="large-datasets.html"><a href="large-datasets.html#exercises-61"><i class="fa fa-check"></i><b>33.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="34" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>34</b> Clustering</a><ul>
<li class="chapter" data-level="34.1" data-path="clustering.html"><a href="clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>34.1</b> Hierarchical clustering</a></li>
<li class="chapter" data-level="34.2" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>34.2</b> k-means</a></li>
<li class="chapter" data-level="34.3" data-path="clustering.html"><a href="clustering.html#heatmaps"><i class="fa fa-check"></i><b>34.3</b> Heatmaps</a></li>
<li class="chapter" data-level="34.4" data-path="clustering.html"><a href="clustering.html#filtering-features"><i class="fa fa-check"></i><b>34.4</b> Filtering features</a></li>
<li class="chapter" data-level="34.5" data-path="clustering.html"><a href="clustering.html#exercises-62"><i class="fa fa-check"></i><b>34.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Productivity Tools</b></span></li>
<li class="chapter" data-level="35" data-path="introduction-to-productivity-tools.html"><a href="introduction-to-productivity-tools.html"><i class="fa fa-check"></i><b>35</b> Introduction to productivity tools</a></li>
<li class="chapter" data-level="36" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html"><i class="fa fa-check"></i><b>36</b> Installing R and RStudio</a><ul>
<li class="chapter" data-level="36.1" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html#installing-r"><i class="fa fa-check"></i><b>36.1</b> Installing R</a></li>
<li class="chapter" data-level="36.2" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html#installing-rstudio"><i class="fa fa-check"></i><b>36.2</b> Installing RStudio</a></li>
</ul></li>
<li class="chapter" data-level="37" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html"><i class="fa fa-check"></i><b>37</b> Accessing the terminal and installing Git</a><ul>
<li class="chapter" data-level="37.1" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html#terminal-on-mac"><i class="fa fa-check"></i><b>37.1</b> Accessing the terminal on a Mac</a></li>
<li class="chapter" data-level="37.2" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html#installing-git-on-the-mac"><i class="fa fa-check"></i><b>37.2</b> Installing Git on the Mac</a></li>
<li class="chapter" data-level="37.3" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html#installing-git-and-git-bash-on-windows"><i class="fa fa-check"></i><b>37.3</b> Installing Git and Git Bash on Windows</a></li>
<li class="chapter" data-level="37.4" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html#terminal-on-windows"><i class="fa fa-check"></i><b>37.4</b> Accessing the terminal on Windows</a></li>
</ul></li>
<li class="chapter" data-level="38" data-path="unix.html"><a href="unix.html"><i class="fa fa-check"></i><b>38</b> Organizing with Unix</a><ul>
<li class="chapter" data-level="38.1" data-path="unix.html"><a href="unix.html#naming-convention"><i class="fa fa-check"></i><b>38.1</b> Naming convention</a></li>
<li class="chapter" data-level="38.2" data-path="unix.html"><a href="unix.html#the-terminal"><i class="fa fa-check"></i><b>38.2</b> The terminal</a></li>
<li class="chapter" data-level="38.3" data-path="unix.html"><a href="unix.html#filesystem"><i class="fa fa-check"></i><b>38.3</b> The filesystem</a><ul>
<li class="chapter" data-level="38.3.1" data-path="unix.html"><a href="unix.html#directories-and-subdirectories"><i class="fa fa-check"></i><b>38.3.1</b> Directories and subdirectories</a></li>
<li class="chapter" data-level="38.3.2" data-path="unix.html"><a href="unix.html#the-home-directory"><i class="fa fa-check"></i><b>38.3.2</b> The home directory</a></li>
<li class="chapter" data-level="38.3.3" data-path="unix.html"><a href="unix.html#working-directory"><i class="fa fa-check"></i><b>38.3.3</b> Working directory</a></li>
<li class="chapter" data-level="38.3.4" data-path="unix.html"><a href="unix.html#paths"><i class="fa fa-check"></i><b>38.3.4</b> Paths</a></li>
</ul></li>
<li class="chapter" data-level="38.4" data-path="unix.html"><a href="unix.html#unix-commands"><i class="fa fa-check"></i><b>38.4</b> Unix commands</a><ul>
<li class="chapter" data-level="38.4.1" data-path="unix.html"><a href="unix.html#ls-listing-directory-content"><i class="fa fa-check"></i><b>38.4.1</b> <code>ls</code>: Listing directory content</a></li>
<li class="chapter" data-level="38.4.2" data-path="unix.html"><a href="unix.html#mkdir-and-rmdir-make-and-remove-a-directory"><i class="fa fa-check"></i><b>38.4.2</b> <code>mkdir</code> and <code>rmdir</code>: make and remove a directory</a></li>
<li class="chapter" data-level="38.4.3" data-path="unix.html"><a href="unix.html#cd-navigating-the-filesystem-by-changing-directories"><i class="fa fa-check"></i><b>38.4.3</b> <code>cd</code>: navigating the filesystem by changing directories</a></li>
</ul></li>
<li class="chapter" data-level="38.5" data-path="unix.html"><a href="unix.html#some-examples"><i class="fa fa-check"></i><b>38.5</b> Some examples</a></li>
<li class="chapter" data-level="38.6" data-path="unix.html"><a href="unix.html#more-unix-commands"><i class="fa fa-check"></i><b>38.6</b> More Unix commands</a><ul>
<li class="chapter" data-level="38.6.1" data-path="unix.html"><a href="unix.html#mv-moving-files"><i class="fa fa-check"></i><b>38.6.1</b> <code>mv</code>: moving files</a></li>
<li class="chapter" data-level="38.6.2" data-path="unix.html"><a href="unix.html#cp-copying-files"><i class="fa fa-check"></i><b>38.6.2</b> <code>cp</code>: copying files</a></li>
<li class="chapter" data-level="38.6.3" data-path="unix.html"><a href="unix.html#rm-removing-files"><i class="fa fa-check"></i><b>38.6.3</b> <code>rm</code>: removing files</a></li>
<li class="chapter" data-level="38.6.4" data-path="unix.html"><a href="unix.html#less-looking-at-a-file"><i class="fa fa-check"></i><b>38.6.4</b> <code>less</code>: looking at a file</a></li>
</ul></li>
<li class="chapter" data-level="38.7" data-path="unix.html"><a href="unix.html#prep-project"><i class="fa fa-check"></i><b>38.7</b> Preparing for a data science project</a></li>
<li class="chapter" data-level="38.8" data-path="unix.html"><a href="unix.html#advanced-unix"><i class="fa fa-check"></i><b>38.8</b> Advanced Unix</a><ul>
<li class="chapter" data-level="38.8.1" data-path="unix.html"><a href="unix.html#arguments"><i class="fa fa-check"></i><b>38.8.1</b> Arguments</a></li>
<li class="chapter" data-level="38.8.2" data-path="unix.html"><a href="unix.html#getting-help"><i class="fa fa-check"></i><b>38.8.2</b> Getting help</a></li>
<li class="chapter" data-level="38.8.3" data-path="unix.html"><a href="unix.html#pipes"><i class="fa fa-check"></i><b>38.8.3</b> Pipes</a></li>
<li class="chapter" data-level="38.8.4" data-path="unix.html"><a href="unix.html#wild-cards"><i class="fa fa-check"></i><b>38.8.4</b> Wild cards</a></li>
<li class="chapter" data-level="38.8.5" data-path="unix.html"><a href="unix.html#environment-variables"><i class="fa fa-check"></i><b>38.8.5</b> Environment variables</a></li>
<li class="chapter" data-level="38.8.6" data-path="unix.html"><a href="unix.html#shells"><i class="fa fa-check"></i><b>38.8.6</b> Shells</a></li>
<li class="chapter" data-level="38.8.7" data-path="unix.html"><a href="unix.html#executables"><i class="fa fa-check"></i><b>38.8.7</b> Executables</a></li>
<li class="chapter" data-level="38.8.8" data-path="unix.html"><a href="unix.html#permissions-and-file-types"><i class="fa fa-check"></i><b>38.8.8</b> Permissions and file types</a></li>
<li class="chapter" data-level="38.8.9" data-path="unix.html"><a href="unix.html#commands-you-should-learn"><i class="fa fa-check"></i><b>38.8.9</b> Commands you should learn</a></li>
<li class="chapter" data-level="38.8.10" data-path="unix.html"><a href="unix.html#file-manipulation-in-r"><i class="fa fa-check"></i><b>38.8.10</b> File manipulation in R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="39" data-path="git.html"><a href="git.html"><i class="fa fa-check"></i><b>39</b> Git and GitHub</a><ul>
<li class="chapter" data-level="39.1" data-path="git.html"><a href="git.html#why-use-git-and-github"><i class="fa fa-check"></i><b>39.1</b> Why use Git and GitHub?</a></li>
<li class="chapter" data-level="39.2" data-path="git.html"><a href="git.html#github-accounts"><i class="fa fa-check"></i><b>39.2</b> GitHub accounts</a></li>
<li class="chapter" data-level="39.3" data-path="git.html"><a href="git.html#github-repos"><i class="fa fa-check"></i><b>39.3</b> GitHub repositories</a></li>
<li class="chapter" data-level="39.4" data-path="git.html"><a href="git.html#git-overview"><i class="fa fa-check"></i><b>39.4</b> Overview of Git</a><ul>
<li class="chapter" data-level="39.4.1" data-path="git.html"><a href="git.html#clone"><i class="fa fa-check"></i><b>39.4.1</b> Clone</a></li>
</ul></li>
<li class="chapter" data-level="39.5" data-path="git.html"><a href="git.html#init"><i class="fa fa-check"></i><b>39.5</b> Initializing a Git directory</a></li>
<li class="chapter" data-level="39.6" data-path="git.html"><a href="git.html#rstudio-git"><i class="fa fa-check"></i><b>39.6</b> Using Git and GitHub in RStudio</a></li>
</ul></li>
<li class="chapter" data-level="40" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html"><i class="fa fa-check"></i><b>40</b> Reproducible projects with RStudio and R markdown</a><ul>
<li class="chapter" data-level="40.1" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#rstudio-projects"><i class="fa fa-check"></i><b>40.1</b> RStudio projects</a></li>
<li class="chapter" data-level="40.2" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#r-markdown"><i class="fa fa-check"></i><b>40.2</b> R markdown</a><ul>
<li class="chapter" data-level="40.2.1" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#the-header"><i class="fa fa-check"></i><b>40.2.1</b> The header</a></li>
<li class="chapter" data-level="40.2.2" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#r-code-chunks"><i class="fa fa-check"></i><b>40.2.2</b> R code chunks</a></li>
<li class="chapter" data-level="40.2.3" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#global-options"><i class="fa fa-check"></i><b>40.2.3</b> Global options</a></li>
<li class="chapter" data-level="40.2.4" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#knitr"><i class="fa fa-check"></i><b>40.2.4</b> knitR</a></li>
<li class="chapter" data-level="40.2.5" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#more-on-r-markdown"><i class="fa fa-check"></i><b>40.2.5</b> More on R markdown</a></li>
</ul></li>
<li class="chapter" data-level="40.3" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#organizing"><i class="fa fa-check"></i><b>40.3</b> Organizing a data science project</a><ul>
<li class="chapter" data-level="40.3.1" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#create-directories-in-unix"><i class="fa fa-check"></i><b>40.3.1</b> Create directories in Unix</a></li>
<li class="chapter" data-level="40.3.2" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#create-an-rstudio-project"><i class="fa fa-check"></i><b>40.3.2</b> Create an RStudio project</a></li>
<li class="chapter" data-level="40.3.3" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#edit-some-r-scripts"><i class="fa fa-check"></i><b>40.3.3</b> Edit some R scripts</a></li>
<li class="chapter" data-level="40.3.4" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#create-some-more-directories-using-unix"><i class="fa fa-check"></i><b>40.3.4</b> Create some more directories using Unix</a></li>
<li class="chapter" data-level="40.3.5" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#add-a-readme-file"><i class="fa fa-check"></i><b>40.3.5</b> Add a README file</a></li>
<li class="chapter" data-level="40.3.6" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#initializing-a-git-directory"><i class="fa fa-check"></i><b>40.3.6</b> Initializing a Git directory</a></li>
<li class="chapter" data-level="40.3.7" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#add-commit-and-push-files-using-rstudio"><i class="fa fa-check"></i><b>40.3.7</b> Add, commit, and push files using RStudio</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inference" class="section level1">
<h1><span class="header-section-number">Chapter 15</span> Statistical inference</h1>
<p>In Chapter <a href="models.html#models">16</a> we will describe, in some detail, how poll aggregators such as FiveThirtyEight use data to predict election outcomes. To understand how they do this, we first need to learn the basics of <em>Statistical Inference</em>, the part of statistics that helps distinguish patterns arising from signal from those arising from chance. Statistical inference is a broad topic and here we go over the very basics using polls as a motivating example. To describe the concepts, we complement the mathematical formulas with Monte Carlo simulations and R code.</p>
<div id="polls" class="section level2">
<h2><span class="header-section-number">15.1</span> Polls</h2>
<p>Opinion polling has been conducted since the 19th century. The general goal is to describe the opinions held by a specific population on a given set of topics. In recent times, these polls have been pervasive during presidential elections. Polls are useful when interviewing every member of a particular population is logistically impossible. The general strategy is to interview a smaller group, chosen at random, and then infer the opinions of the entire population from the opinions of the smaller group. Statistical theory is used to justify the process. This theory is referred to as <em>inference</em> and it is the main topic of this chapter.</p>
<p>Perhaps the best known opinion polls are those conducted to determine which candidate is preferred by voters in a given election. Political strategists make extensive use of polls to decide, among other things, how to invest resources. For example, they may want to know in which geographical locations to focus their “get out the vote” efforts.</p>
<p>Elections are a particularly interesting case of opinion polls because the actual opinion of the entire population is revealed on election day. Of course, it costs millions of dollars to run an actual election which makes polling a cost effective strategy for those that want to forecast the results.</p>
<p>Although typically the results of these polls are kept private, similar polls are conducted by news organizations because results tend to be of interest to the general public and made public. We will eventually be looking at such data.</p>
<p>Real Clear Politics<a href="#fn50" class="footnoteRef" id="fnref50"><sup>50</sup></a> is an example of a news aggregator that organizes and publishes poll results. For example, they present the following poll results reporting estimates of the popular vote for the 2016 presidential election<a href="#fn51" class="footnoteRef" id="fnref51"><sup>51</sup></a>:</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Poll
</th>
<th style="text-align:left;">
Date
</th>
<th style="text-align:left;">
Sample
</th>
<th style="text-align:left;">
MoE
</th>
<th style="text-align:right;">
Clinton
</th>
<th style="text-align:right;">
Trump
</th>
<th style="text-align:left;">
Spread
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Final Results
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
48.2
</td>
<td style="text-align:right;">
46.1
</td>
<td style="text-align:left;">
Clinton +2.1
</td>
</tr>
<tr>
<td style="text-align:left;">
RCP Average
</td>
<td style="text-align:left;">
11/1 - 11/7
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
46.8
</td>
<td style="text-align:right;">
43.6
</td>
<td style="text-align:left;">
Clinton +3.2
</td>
</tr>
<tr>
<td style="text-align:left;">
Bloomberg
</td>
<td style="text-align:left;">
11/4 - 11/6
</td>
<td style="text-align:left;">
799 LV
</td>
<td style="text-align:left;">
3.5
</td>
<td style="text-align:right;">
46.0
</td>
<td style="text-align:right;">
43.0
</td>
<td style="text-align:left;">
Clinton +3
</td>
</tr>
<tr>
<td style="text-align:left;">
IBD
</td>
<td style="text-align:left;">
11/4 - 11/7
</td>
<td style="text-align:left;">
1107 LV
</td>
<td style="text-align:left;">
3.1
</td>
<td style="text-align:right;">
43.0
</td>
<td style="text-align:right;">
42.0
</td>
<td style="text-align:left;">
Clinton +1
</td>
</tr>
<tr>
<td style="text-align:left;">
Economist
</td>
<td style="text-align:left;">
11/4 - 11/7
</td>
<td style="text-align:left;">
3669 LV
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
49.0
</td>
<td style="text-align:right;">
45.0
</td>
<td style="text-align:left;">
Clinton +4
</td>
</tr>
<tr>
<td style="text-align:left;">
LA Times
</td>
<td style="text-align:left;">
11/1 - 11/7
</td>
<td style="text-align:left;">
2935 LV
</td>
<td style="text-align:left;">
4.5
</td>
<td style="text-align:right;">
44.0
</td>
<td style="text-align:right;">
47.0
</td>
<td style="text-align:left;">
Trump +3
</td>
</tr>
<tr>
<td style="text-align:left;">
ABC
</td>
<td style="text-align:left;">
11/3 - 11/6
</td>
<td style="text-align:left;">
2220 LV
</td>
<td style="text-align:left;">
2.5
</td>
<td style="text-align:right;">
49.0
</td>
<td style="text-align:right;">
46.0
</td>
<td style="text-align:left;">
Clinton +3
</td>
</tr>
<tr>
<td style="text-align:left;">
FOX News
</td>
<td style="text-align:left;">
11/3 - 11/6
</td>
<td style="text-align:left;">
1295 LV
</td>
<td style="text-align:left;">
2.5
</td>
<td style="text-align:right;">
48.0
</td>
<td style="text-align:right;">
44.0
</td>
<td style="text-align:left;">
Clinton +4
</td>
</tr>
<tr>
<td style="text-align:left;">
Monmouth
</td>
<td style="text-align:left;">
11/3 - 11/6
</td>
<td style="text-align:left;">
748 LV
</td>
<td style="text-align:left;">
3.6
</td>
<td style="text-align:right;">
50.0
</td>
<td style="text-align:right;">
44.0
</td>
<td style="text-align:left;">
Clinton +6
</td>
</tr>
<tr>
<td style="text-align:left;">
NBC News
</td>
<td style="text-align:left;">
11/3 - 11/5
</td>
<td style="text-align:left;">
1282 LV
</td>
<td style="text-align:left;">
2.7
</td>
<td style="text-align:right;">
48.0
</td>
<td style="text-align:right;">
43.0
</td>
<td style="text-align:left;">
Clinton +5
</td>
</tr>
<tr>
<td style="text-align:left;">
CBS News
</td>
<td style="text-align:left;">
11/2 - 11/6
</td>
<td style="text-align:left;">
1426 LV
</td>
<td style="text-align:left;">
3.0
</td>
<td style="text-align:right;">
47.0
</td>
<td style="text-align:right;">
43.0
</td>
<td style="text-align:left;">
Clinton +4
</td>
</tr>
<tr>
<td style="text-align:left;">
Reuters
</td>
<td style="text-align:left;">
11/2 - 11/6
</td>
<td style="text-align:left;">
2196 LV
</td>
<td style="text-align:left;">
2.3
</td>
<td style="text-align:right;">
44.0
</td>
<td style="text-align:right;">
39.0
</td>
<td style="text-align:left;">
Clinton +5
</td>
</tr>
</tbody>
</table>
<!-- (Source: [Real Clear Politics](https://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html)) -->
<p>Although in the United States the popular vote does not determine the result of the presidential election, we will use it as an illustrative and simple example of how well polls work. Forecasting the election is a more complex process since it involves combining results from 50 states and DC and we describe it in Section <a href="models.html#election-forecasting">16.8</a>.</p>
<p>Let’s make some observations about the table above. First, note that different polls, all taken days before the election, report a different <em>spread</em>: the estimated difference between support for the two candidates. Notice also that the reported spreads hover around what ended up being the actual result: Clinton won the popular vote by 2.1%. We also see a column titled <strong>MoE</strong> which stands for <em>margin of error</em>.</p>
<p>In this section, we will show how the probability concepts we learned in the previous chapter can be applied to develop the statistical approaches that make polls an effective tool. We will learn the statistical concepts necessary to define <em>estimates</em> and <em>margins of errors</em>, and show how we can use these to forecast final results relatively well and also provide an estimate of the precision of our forecast. Once we learn this, we will be able to understand two concepts that are ubiquitous in data science: <em>confidence intervals</em> and <em>p-values</em>. Finally, to understand probabilistic statements about the probability of a candidate winning, we will have to learn about Bayesian modeling. In the final sections, we put it all together to recreate the simplified version of the FiveThirtyEight model and apply it to the 2016 election.</p>
<p>We start by connecting probability theory to the task of using polls to learn about a population.</p>
<div id="the-sampling-model-for-polls" class="section level3">
<h3><span class="header-section-number">15.1.1</span> The sampling model for polls</h3>
<p>To help us understand the connection between polls and what we have learned, let’s construct a similar situation to the one pollsters face. To mimic the challenge real pollsters face in terms of competing with other pollsters for media attention, we will use an urn full of beads to represent voters and pretend we are competing for a $25 dollar prize. The challenge is to guess the spread between the proportion of blue and red beads in this urn (in this case, a pickle jar):</p>
<p><img src="inference/img/urn.jpg" width="1.5in" style="display: block; margin: auto;" /></p>
<p>Before making a prediction, you can take a sample (with replacement) from the urn. To mimic the fact that running polls is expensive, it costs you $0.10 per each bead you sample. Therefore, if your sample size is 250, and you win, you will break even since you will pay $25 to collect your $25 prize. Your entry into the competition can be an interval. If the interval you submit contains the true proportion, you get half what you paid and pass to the second phase of the competition. In the second phase, the entry with the smallest interval is selected as the winner.</p>
<p>The <strong>dslabs</strong> package includes a function that shows a random draw from this urn:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(dslabs)
<span class="kw">take_poll</span>(<span class="dv">25</span>)</code></pre></div>
<p><img src="book_files/figure-html/first-simulated-poll-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Think about how you would construct your interval based on the data shown above.</p>
<p>We have just described a simple sampling model for opinion polls. The beads inside the urn represent the individuals that will vote on election day. Those that will vote for the Republican candidate are represented with red beads and the Democrats with the blue beads. For simplicity, assume there are no other colors. That is, that there are just two parties: Republican and Democratic.</p>
</div>
</div>
<div id="populations-samples-parameters-and-estimates" class="section level2">
<h2><span class="header-section-number">15.2</span> Populations, samples, parameters, and estimates</h2>
<p>We want to predict the proportion of blue beads in the urn. Let’s call this quantity <span class="math inline">\(p\)</span>, which then tells us the proportion of red beads <span class="math inline">\(1-p\)</span>, and the spread <span class="math inline">\(p - (1-p)\)</span>, which simplifies to <span class="math inline">\(2p - 1\)</span>.</p>
<p>In statistical textbooks, the beads in the urn are called the <em>population</em>. The proportion of blue beads in the population <span class="math inline">\(p\)</span> is called a <em>parameter</em>. The 25 beads we see in the previous plot are called a <em>sample</em>. The task of statistical inference is to predict the parameter <span class="math inline">\(p\)</span> using the observed data in the sample.</p>
<p>Can we do this with the 25 observations above? It is certainly informative. For example, given that we see 13 red and 12 blue beads, it is unlikely that <span class="math inline">\(p\)</span> &gt; .9 or <span class="math inline">\(p\)</span> &lt; .1. But are we ready to predict with certainty that there are more red beads than blue in the jar?</p>
<p>We want to construct an estimate of <span class="math inline">\(p\)</span> using only the information we observe. An estimate should be thought of as a summary of the observed data that we think is informative about the parameter of interest. It seems intuitive to think that the proportion of blue beads in the sample <span class="math inline">\(0.48\)</span> must be at least related to the actual proportion <span class="math inline">\(p\)</span>. But do we simply predict <span class="math inline">\(p\)</span> to be 0.48? First, remember that the sample proportion is a random variable. If we run the command <code>take_poll(25)</code> four times, we get a different answer each time, since the sample proportion is a random variable.</p>
<p><img src="book_files/figure-html/four-simulated-polls-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Note that in the four random samples shown above, the sample proportions range from 0.44 to 0.60. By describing the distribution of this random variable, we will be able to gain insights into how good this estimate is and how we can make it better.</p>
<div id="the-sample-average" class="section level3">
<h3><span class="header-section-number">15.2.1</span> The sample average</h3>
<p>Conducting an opinion poll is being modeled as taking a random sample from an urn. We are proposing the use of the proportion of blue beads in our sample as an <em>estimate</em> of the parameter <span class="math inline">\(p\)</span>. Once we have this estimate, we can easily report an estimate for the spread <span class="math inline">\(2p-1\)</span>, but for simplicity we will illustrate the concepts for estimating <span class="math inline">\(p\)</span>. We will use our knowledge of probability to defend our use of the sample proportion and quantify how close we think it is from the population proportion <span class="math inline">\(p\)</span>.</p>
<p>We start by defining the random variable <span class="math inline">\(X\)</span> as: <span class="math inline">\(X=1\)</span> if we pick a blue bead at random and <span class="math inline">\(X=0\)</span> if it is red. This implies that the population is a list of 0s and 1s. If we sample <span class="math inline">\(N\)</span> beads, then the average of the draws <span class="math inline">\(X_1, \dots, X_N\)</span> is equivalent to the proportion of blue beads in our sample. This is because adding the <span class="math inline">\(X\)</span>s is equivalent to counting the blue beads and dividing this count by the total <span class="math inline">\(N\)</span> is equivalent to computing a proportion. We use the symbol <span class="math inline">\(\bar{X}\)</span> to represent this average. In general, in statistics textbooks a bar on top of a symbol means the average. The theory we just learned about the sum of draws becomes useful because the average is a sum of draws multiplied by the constant <span class="math inline">\(1/N\)</span>:</p>
<p><span class="math display">\[\bar{X} = 1/N \times \sum_{i=1}^N X_i\]</span></p>
<p>For simplicity, let’s assume that the draws are independent: after we see each sampled bead, we return it to the urn. In this case, what do we know about the distribution of the sum of draws? First, we know that the expected value of the sum of draws is <span class="math inline">\(N\)</span> times the average of the values in the urn. We know that the average of the 0s and 1s in the urn must be <span class="math inline">\(p\)</span>, the proportion of blue beads.</p>
<p>Here we encounter an important difference with what we did in the Probability chapter: we don’t know what is in the urn. We know there are blue and red beads, but we don’t know how many of each. This is what we want to find out: we are trying to <strong>estimate</strong> <span class="math inline">\(p\)</span>.</p>
</div>
<div id="parameters" class="section level3">
<h3><span class="header-section-number">15.2.2</span> Parameters</h3>
<p>Just like we use variables to define unknowns in systems of equations, in statistical inference we define <em>parameters</em> to define unknown parts of our models. In the urn model which we are using to mimic an opinion poll, we do not know the proportion of blue beads in the urn. We define the parameters <span class="math inline">\(p\)</span> to represent this quantity. <span class="math inline">\(p\)</span> is the average of the urn because if we take the average of the 1s (blue) and 0s (red), we get the proportion of blue beads. Since our main goal is figuring out what is <span class="math inline">\(p\)</span>, we are going to <em>estimate this parameter</em>.</p>
<p>The ideas presented here on how we estimate parameters, and provide insights into how good these estimates are, extrapolate to many data science tasks. For example, we may want to determine the difference in health improvement between patients receiving treatment and a control group. We may ask, what are the health effects of smoking on a population? What are the differences in racial groups of fatal shootings by police? What is the rate of change in life expectancy in the US during the last 10 years? All these questions can be framed as a task of estimating a parameter from a sample.</p>
</div>
<div id="polling-versus-forecasting" class="section level3">
<h3><span class="header-section-number">15.2.3</span> Polling versus forecasting</h3>
<p>Before we continue, let’s make an important clarification related to the practical problem of forecasting the election. If a poll is conducted four months before the election, it is estimating the <span class="math inline">\(p\)</span> for that moment and not for election day. The <span class="math inline">\(p\)</span> for election night might be different since people’s opinions fluctuate through time. The polls provided the night before the election tend to be the most accurate since opinions don’t change that much in a day. However, forecasters try to build tools that model how opinions vary across time and try to predict the election night results taking into consideration the fact that opinions fluctuate. We will describe some approaches for doing this in a later section.</p>
</div>
<div id="properties-of-our-estimate-expected-value-and-standard-error" class="section level3">
<h3><span class="header-section-number">15.2.4</span> Properties of our estimate: expected value and standard error</h3>
<p>To understand how good our estimate is, we will describe the statistical properties of the random variable defined above: the sample proportion <span class="math inline">\(\bar{X}\)</span>. Remember that <span class="math inline">\(\bar{X}\)</span> is the sum of independent draws so the rules we covered in the probability chapter apply.</p>
<p>Using what we have learned, the expected value of the sum <span class="math inline">\(N\bar{X}\)</span> is <span class="math inline">\(N \times\)</span> the average of the urn, <span class="math inline">\(p\)</span>. So dividing by the non-random constant <span class="math inline">\(N\)</span> gives us that the expected value of the average <span class="math inline">\(\bar{X}\)</span> is <span class="math inline">\(p\)</span>. We can write it using our mathematical notation:</p>
<p><span class="math display">\[
\mbox{E}(\bar{X}) = p
\]</span></p>
<p>We can also use what we learned to figure out the standard error: the standard error of the sum is <span class="math inline">\(\sqrt{N} \times\)</span> the standard deviation of the urn. Can we compute the standard error of the urn? We learned a formula that tells us that it is <span class="math inline">\((1-0) \sqrt{p (1-p)}\)</span> = <span class="math inline">\(\sqrt{p (1-p)}\)</span>. Because we are dividing the sum by <span class="math inline">\(N\)</span>, we arrive at the following formula for the standard error of the average:</p>
<p><span class="math display">\[
\mbox{SE}(\bar{X}) = \sqrt{p(1-p)/N}
\]</span></p>
<p>This result reveals the power of polls. The expected value of the sample proportion <span class="math inline">\(\bar{X}\)</span> is the parameter of interest <span class="math inline">\(p\)</span> and we can make the standard error as small as we want by increasing <span class="math inline">\(N\)</span>. The law of large numbers tells us that with a large enough poll, our estimate converges to <span class="math inline">\(p\)</span>.</p>
<p>If we take a large enough poll to make our standard error about 1%, we will be quite certain about who will win. But how large does the poll have to be for the standard error to be this small?</p>
<p>One problem is that we do not know <span class="math inline">\(p\)</span>, so we can’t compute the standard error. However, for illustrative purposes, let’s assume that <span class="math inline">\(p=0.51\)</span> and make a plot of the standard error versus the sample size <span class="math inline">\(N\)</span>:</p>
<p><img src="book_files/figure-html/standard-error-versus-sample-size-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>From the plot we see that we would need a poll of over 10,000 people to get the standard error that low. We rarely see polls of this size due in part to costs. From the Real Clear Politics table, we learn that the sample sizes in opinion polls range from 500-3,500 people. For a sample size of 1,000 and <span class="math inline">\(p=0.51\)</span>, the standard error is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(p<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>p))<span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">1000</span>)
<span class="co">#&gt; [1] 0.0158</span></code></pre></div>
<p>or 1.5 percentage points. So even with large polls, for close elections, <span class="math inline">\(\bar{X}\)</span> can lead us astray if we don’t realize it is a random variable. Nonetheless, we can actually say more about how close we get the <span class="math inline">\(p\)</span> and we do that in Section <a href="inference.html#clt">15.4</a>.</p>
</div>
</div>
<div id="exercises-26" class="section level2">
<h2><span class="header-section-number">15.3</span> Exercises</h2>
<p>1. Suppose you poll a population in which a proportion <span class="math inline">\(p\)</span> of voters are Democrats and <span class="math inline">\(1-p\)</span> are Republicans. Your sample size is <span class="math inline">\(N=25\)</span>. Consider the random variable <span class="math inline">\(S\)</span> which is the <strong>total</strong> number of Democrats in your sample. What is the expected value of this random variable? Hint: it’s a function of <span class="math inline">\(p\)</span>.</p>
<p>2. What is the standard error of <span class="math inline">\(S\)</span> ? Hint: it’s a function of <span class="math inline">\(p\)</span>.</p>
<p>3. Consider the random variable <span class="math inline">\(S/N\)</span>. This is equivalent to the sample average, which we have been denoting as <span class="math inline">\(\bar{X}\)</span>. What is the expected value of the <span class="math inline">\(\bar{X}\)</span>? Hint: it’s a function of <span class="math inline">\(p\)</span>.</p>
<p>4. What is the standard error of <span class="math inline">\(\bar{X}\)</span>? Hint: it’s a function of <span class="math inline">\(p\)</span>.</p>
<p>5. Write a line of code that gives you the standard error <code>se</code> for the problem above for several values of <span class="math inline">\(p\)</span>, specifically for <code>p &lt;- seq(0, 1, length = 100)</code>. Make a plot of <code>se</code> versus <code>p</code>.</p>
<p>6. Copy the code above and put it inside a for-loop to make the plot for <span class="math inline">\(N=25\)</span>, <span class="math inline">\(N=100\)</span>, and <span class="math inline">\(N=1000\)</span>.</p>
<p>7. If we are interested in the difference in proportions, <span class="math inline">\(p - (1-p)\)</span>, our estimate is <span class="math inline">\(d = \bar{X} - (1-\bar{X})\)</span>. Use the rules we learned about sums of random variables and scaled random variables to derive the expected value of <span class="math inline">\(d\)</span>.</p>
<p>8. What is the standard error of <span class="math inline">\(d\)</span>?</p>
<p>9. If the actual <span class="math inline">\(p=.45\)</span>, it means the Republicans are winning by a relatively large margin since <span class="math inline">\(d= -.1\)</span>, which is a 10% margin of victory. In this case, what is the standard error of <span class="math inline">\(2\hat{X}-1\)</span> if we take a sample of <span class="math inline">\(N=25\)</span>?</p>
<p>10. Given the answer to 9, which of the following best describes your strategy of using a sample size of <span class="math inline">\(N=25\)</span>?</p>
<ol style="list-style-type: lower-alpha">
<li>The expected value of our estimate <span class="math inline">\(2\bar{X}-1\)</span> is <span class="math inline">\(d\)</span>, so our prediction will be right on.</li>
<li>Our standard error is larger than the difference, so the chances of <span class="math inline">\(2\bar{X}-1\)</span> being positive and throwing us off were not that small. We should pick a larger sample size.</li>
<li>The difference is 10% and the standard error is about 0.2, therefore much smaller than the difference.</li>
<li>Because we don’t know <span class="math inline">\(p\)</span>, we have no way of knowing that making <span class="math inline">\(N\)</span> larger would actually improve our standard error.</li>
</ol>

</div>
<div id="clt" class="section level2">
<h2><span class="header-section-number">15.4</span> Central Limit Theorem in practice</h2>
<p>The CLT tells us that the distribution function for a sum of draws is approximately normal. We also learned that dividing a normally distributed random variable by a constant is also a normally distributed variable. This implies that the distribution of <span class="math inline">\(\bar{X}\)</span> is approximately normal.</p>
<p>In summary, we have that <span class="math inline">\(\bar{X}\)</span> has an approximately normal distribution with expected value <span class="math inline">\(p\)</span> and standard error <span class="math inline">\(\sqrt{p(1-p)/N}\)</span>.</p>
<p>Now how does this help us? Suppose we want to know what is the probability that we are within 1% from <span class="math inline">\(p\)</span>. We are basically asking what is</p>
<p><span class="math display">\[
\mbox{Pr}(| \bar{X} - p| \leq .01)
\]</span> which is the same as:</p>
<p><span class="math display">\[
\mbox{Pr}(\bar{X}\leq p + .01) - \mbox{Pr}(\bar{X} \leq p - .01)
\]</span></p>
<p>Can we answer this question? We can use the mathematical trick we learned in the previous chapter. Subtract the expected value and divide by the standard error to get a standard normal random variable, call it <span class="math inline">\(Z\)</span>, on the left. Since <span class="math inline">\(p\)</span> is the expected value and <span class="math inline">\(\mbox{SE}(\bar{X}) = \sqrt{p(1-p)/N}\)</span> is the standard error we get:</p>
<p><span class="math display">\[
\mbox{Pr}\left(Z \leq \frac{ \,.01} {\mbox{SE}(\bar{X})} \right) -
\mbox{Pr}\left(Z \leq - \frac{ \,.01} {\mbox{SE}(\bar{X})} \right) 
\]</span></p>
<p>One problem we have is that since we don’t know <span class="math inline">\(p\)</span>, we don’t know <span class="math inline">\(\mbox{SE}(\bar{X})\)</span>. But it turns out that the CLT still works if we estimate the standard error by using <span class="math inline">\(\bar{X}\)</span> in place of <span class="math inline">\(p\)</span>. We say that we <em>plug-in</em> the estimate. Our estimate of the standard error is therefore:</p>
<p><span class="math display">\[
\hat{\mbox{SE}}(\bar{X})=\sqrt{\bar{X}(1-\bar{X})/N}
\]</span> In statistics textbooks, we use a little hat to denote estimates. The estimate can be constructed using the observed data and <span class="math inline">\(N\)</span>.</p>
<p>Now we continue with our calculation, but dividing by <span class="math inline">\(\hat{\mbox{SE}}(\bar{X})=\sqrt{\bar{X}(1-\bar{X})/N})\)</span> instead. In our first sample we had 12 blue and 13 red so <span class="math inline">\(\bar{X} = 0.48\)</span> and our estimate of standard error is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x_hat &lt;-<span class="st"> </span><span class="fl">0.48</span>
se &lt;-<span class="st"> </span><span class="kw">sqrt</span>(x_hat<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>x_hat)<span class="op">/</span><span class="dv">25</span>)
se
<span class="co">#&gt; [1] 0.0999</span></code></pre></div>
<p>And now we can answer the question of the probability of being close to <span class="math inline">\(p\)</span>. The answer is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pnorm</span>(<span class="fl">0.01</span><span class="op">/</span>se) <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="op">-</span><span class="fl">0.01</span><span class="op">/</span>se)
<span class="co">#&gt; [1] 0.0797</span></code></pre></div>
<p>Therefore, there is a small chance that we will be close. A poll of only <span class="math inline">\(N=25\)</span> people is not really very useful, at least not for a close election.</p>
<p>Earlier we mentioned the <em>margin of error</em>. Now we can define it because it is simply two times the standard error, which we can now estimate. In our case it is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="fl">1.96</span><span class="op">*</span>se
<span class="co">#&gt; [1] 0.196</span></code></pre></div>
<p>Why do we multiply by 1.96? Because if you ask what is the probability that we are within 1.96 standard errors from <span class="math inline">\(p\)</span>, we get:</p>
<p><span class="math display">\[
\mbox{Pr}\left(Z \leq \, 1.96\,\mbox{SE}(\bar{X})  / \mbox{SE}(\bar{X}) \right) -
\mbox{Pr}\left(Z \leq - 1.96\, \mbox{SE}(\bar{X}) / \mbox{SE}(\bar{X}) \right) 
\]</span> which is:</p>
<p><span class="math display">\[
\mbox{Pr}\left(Z \leq 1.96 \right) -
\mbox{Pr}\left(Z \leq - 1.96\right) 
\]</span></p>
<p>which we know is about 95%:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pnorm</span>(<span class="fl">1.96</span>)<span class="op">-</span><span class="kw">pnorm</span>(<span class="op">-</span><span class="fl">1.96</span>)
<span class="co">#&gt; [1] 0.95</span></code></pre></div>
<p>Hence, there is a 95% probability that <span class="math inline">\(\bar{X}\)</span> will be within <span class="math inline">\(1.96\times \hat{SE}(\bar{X})\)</span>, in our case within about 0.2, of <span class="math inline">\(p\)</span>. Note that 95% is somewhat of an arbitrary choice and sometimes other percentages are used, but it is the most commonly used value to define margin of error. We often round 1.96 up to 2 for simplicity of presentation.</p>
<p>In summary, the CLT tells us that our poll based on a sample size of <span class="math inline">\(25\)</span> is not very useful. We don’t really learn much when the margin of error is this large. All we can really say is that the popular vote will not be won by a large margin. This is why pollsters tend to use larger sample sizes.</p>
<p>From the table above, we see that typical sample sizes range from 700 to 3500. To see how this gives us a much more practical result, notice that if we had obtained a <span class="math inline">\(\bar{X}\)</span>=0.48 with a sample size of 2,000, our standard error <span class="math inline">\(\hat{\mbox{SE}}(\bar{X})\)</span> would have been 0.011. So our result is an estimate of <code>48</code>% with a margin of error of 2%. In this case, the result is much more informative and would make us think that there are more red balls than blue. Keep in mind, however, that this is hypothetical. We did not take a poll of 2,000 since we don’t want to ruin the competition.</p>
<div id="a-monte-carlo-simulation" class="section level3">
<h3><span class="header-section-number">15.4.1</span> A Monte Carlo simulation</h3>
<p>Suppose we want to use a Monte Carlo simulation to corroborate the tools we have built using probability theory. To create the simulation, we would write code like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">B &lt;-<span class="st"> </span><span class="dv">10000</span>
N &lt;-<span class="st"> </span><span class="dv">1000</span>
x_hat &lt;-<span class="st"> </span><span class="kw">replicate</span>(B, {
  x &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">size =</span> N, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">-</span>p, p))
  <span class="kw">mean</span>(x)
})</code></pre></div>
<p>The problem is, of course, we don’t know <code>p</code>. We could construct an urn like the one pictured above and run an analog (without a computer) simulation. It would take a long time, but you could take 10,000 samples, count the beads and keep track of the proportions of blue. We can use the function <code>take_poll(n=1000)</code> instead of drawing from an actual urn, but it would still take time to count the beads and enter the results.</p>
<p>One thing we therefore do to corroborate theoretical results is to pick one or several values of <code>p</code> and run the simulations. Let’s set <code>p=0.45</code>. We can then simulate a poll:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p &lt;-<span class="st"> </span><span class="fl">0.45</span>
N &lt;-<span class="st"> </span><span class="dv">1000</span>

x &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">size =</span> N, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">-</span>p, p))
x_hat &lt;-<span class="st"> </span><span class="kw">mean</span>(x)</code></pre></div>
<p>In this particular sample, our estimate is <code>x_hat</code>. We can use that code to do a Monte Carlo simulation:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">B &lt;-<span class="st"> </span><span class="dv">10000</span>
x_hat &lt;-<span class="st"> </span><span class="kw">replicate</span>(B, {
  x &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">size =</span> N, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">-</span>p, p))
  <span class="kw">mean</span>(x)
})</code></pre></div>
<p>To review, the theory tells us that <span class="math inline">\(\bar{X}\)</span> is approximately normally distributed, has expected value <span class="math inline">\(p=\)</span> 0.45 and standard error <span class="math inline">\(\sqrt{p(1-p)/N}\)</span> = 0.016. The simulation confirms this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(x_hat)
<span class="co">#&gt; [1] 0.45</span>
<span class="kw">sd</span>(x_hat)
<span class="co">#&gt; [1] 0.0157</span></code></pre></div>
<p>A histogram and qq-plot confirm that the normal approximation is accurate as well:</p>
<p><img src="book_files/figure-html/normal-approximation-for-polls-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Of course, in real life we would never be able to run such an experiment because we don’t know <span class="math inline">\(p\)</span>. But we could run it for various values of <span class="math inline">\(p\)</span> and <span class="math inline">\(N\)</span> and see that the theory does indeed work well for most values. You can easily do this by re-running the code above after changing <code>p</code> and <code>N</code>.</p>
</div>
<div id="the-spread" class="section level3">
<h3><span class="header-section-number">15.4.2</span> The spread</h3>
<p>The competition is to predict the spread, not the proportion <span class="math inline">\(p\)</span>. However, because we are assuming there are only two parties, we know that the spread is <span class="math inline">\(p - (1-p) = 2p - 1\)</span>. As a result, everything we have done can easily be adapted to an estimate of <span class="math inline">\(2p - 1\)</span>. Once we have our estimate <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(\hat{\mbox{SE}}(\bar{X})\)</span>, we estimate the spread with <span class="math inline">\(2\bar{X} - 1\)</span> and, since we are multiplying by 2, the standard error is <span class="math inline">\(2\hat{\mbox{SE}}(\bar{X})\)</span>. Note that subtracting 1 does not add any variability so it does not affect the standard error.</p>
<p>For our 25 item sample above, our estimate <span class="math inline">\(p\)</span> is <code>.48</code> with margin of error <code>.20</code> and our estimate of the spread is <code>0.04</code> with margin of error <code>.40</code>. Again, not a very useful sample size. However, the point is that once we have an estimate and standard error for <span class="math inline">\(p\)</span>, we have it for the spread <span class="math inline">\(2p-1\)</span>.</p>
</div>
<div id="bias-why-not-run-a-very-large-poll" class="section level3">
<h3><span class="header-section-number">15.4.3</span> Bias: why not run a very large poll?</h3>
<p>For realistic values of <span class="math inline">\(p\)</span>, say from 0.35 to 0.65, if we run a very large poll with 100,000 people, theory tells us that we would predict the election perfectly since the largest possible margin of error is around 0.3%:</p>
<p><img src="book_files/figure-html/standard-error-versus-p-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>One reason is that running such a poll is very expensive. Another possibly more important reason is that theory has its limitations. Polling is much more complicated than picking beads from an urn. Some people might lie to pollsters and others might not have phones. But perhaps the most important way an actual poll differs from an urn model is that we actually don’t know for sure who is in our population and who is not. How do we know who is going to vote? Are we reaching all possible voters? Hence, even if our margin of error is very small, it might not be exactly right that our expected value is <span class="math inline">\(p\)</span>. We call this bias. Historically, we observe that polls are indeed biased, although not by that much. The typical bias appears to be about 1-2%. This makes election forecasting a bit more interesting and we will talk about how to model this in a later chapter.</p>
</div>
</div>
<div id="exercises-27" class="section level2">
<h2><span class="header-section-number">15.5</span> Exercises</h2>
<p>1. Write an <em>urn model</em> function that takes the proportion of Democrats <span class="math inline">\(p\)</span> and the sample size <span class="math inline">\(N\)</span> as arguments and returns the sample average if Democrats are 1s and Republicans are 0s. Call the function <code>take_sample</code>.</p>
<p>2. Now assume <code>p &lt;- 0.45</code> and that your sample size is <span class="math inline">\(N=100\)</span>. Take a sample 10,000 times and save the vector of <code>mean(X) - p</code> into an object called <code>errors</code>. Hint: use the function you wrote for exercise 1 to write this in one line of code.</p>
<p>3. The vector <code>errors</code> contains, for each simulated sample, the difference between the actual <span class="math inline">\(p\)</span> and our estimate <span class="math inline">\(\bar{X}\)</span>. We refer to this difference as the <em>error</em>. Compute the average and make a histogram of the errors generated in the Monte Carlo simulation and select which of the following best describes their distributions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(errors)
<span class="kw">hist</span>(errors)</code></pre></div>
<ol style="list-style-type: lower-alpha">
<li>The errors are all about 0.05.</li>
<li>The errors are all about -0.05.</li>
<li>The errors are symmetrically distributed around 0.</li>
<li>The errors range from -1 to 1.</li>
</ol>
<p>4. The error <span class="math inline">\(\bar{X}-p\)</span> is a random variable. In practice, the error is not observed because we do not know <span class="math inline">\(p\)</span>. Here we observe it because we constructed the simulation. What is the average size of the error if we define the size by taking the absolute value <span class="math inline">\(\mid \bar{X} - p \mid\)</span> ?</p>
<p>5. The standard error is related to the typical <strong>size</strong> of the error we make when predicting. We say <strong>size</strong> because we just saw that the errors are centered around 0, so thus the average error value is 0. For mathematical reasons related to the Central Limit Theorem, we actually use the standard deviation of <code>errors</code> rather than the average of the absolute values to quantify the typical size. What is this standard deviation of the errors?</p>
<p>6. The theory we just learned tells us what this standard deviation is going to be because it is the standard error of <span class="math inline">\(\bar{X}\)</span>. What does theory tell us is the standard error of <span class="math inline">\(\bar{X}\)</span> for a sample size of 100?</p>
<p>7. In practice, we don’t know <span class="math inline">\(p\)</span>, so we construct an estimate of the theoretical prediction based by plugging in <span class="math inline">\(\bar{X}\)</span> for <span class="math inline">\(p\)</span>. Compute this estimate. Set the seed at 1 with <code>set.seed(1)</code>.</p>
<p>8. Note how close the standard error estimates obtained from the Monte Carlo simulation (exercise 5), the theoretical prediction (exercise 6), and the estimate of the theoretical prediction (exercise 7) are. The theory is working and it gives us a practical approach to knowing the typical error we will make if we predict <span class="math inline">\(p\)</span> with <span class="math inline">\(\bar{X}\)</span>. Another advantage that the theoretical result provides is that it gives an idea of how large a sample size is required to obtain the precision we need. Earlier we learned that the largest standard errors occur for <span class="math inline">\(p=0.5\)</span>. Create a plot of the largest standard error for <span class="math inline">\(N\)</span> ranging from 100 to 5,000. Based on this plot, how large does the sample size have to be to have a standard error of about 1%?</p>
<ol style="list-style-type: lower-alpha">
<li>100</li>
<li>500</li>
<li>2,500</li>
<li>4,000</li>
</ol>
<p>9. For sample size <span class="math inline">\(N=100\)</span>, the central limit theorem tells us that the distribution of <span class="math inline">\(\bar{X}\)</span> is:</p>
<ol style="list-style-type: lower-alpha">
<li>practically equal to <span class="math inline">\(p\)</span>.</li>
<li>approximately normal with expected value <span class="math inline">\(p\)</span> and standard error <span class="math inline">\(\sqrt{p(1-p)/N}\)</span>.</li>
<li>approximately normal with expected value <span class="math inline">\(\bar{X}\)</span> and standard error <span class="math inline">\(\sqrt{\bar{X}(1-\bar{X})/N}\)</span>.</li>
<li>not a random variable.</li>
</ol>
<p>10. Based on the answer from exercise 8, the error <span class="math inline">\(\bar{X} - p\)</span> is:</p>
<ol style="list-style-type: lower-alpha">
<li>practically equal to 0.</li>
<li>approximately normal with expected value <span class="math inline">\(0\)</span> and standard error <span class="math inline">\(\sqrt{p(1-p)/N}\)</span>.</li>
<li>approximately normal with expected value <span class="math inline">\(p\)</span> and standard error <span class="math inline">\(\sqrt{p(1-p)/N}\)</span>.</li>
<li>not a random variable.</li>
</ol>
<p>11. To corroborate your answer to exercise 9, make a qq-plot of the <code>errors</code> you generated in exercise 2 to see if they follow a normal distribution.</p>
<p>12. If <span class="math inline">\(p=0.45\)</span> and <span class="math inline">\(N=100\)</span> as in exercise 2, use the CLT to estimate the probability that <span class="math inline">\(\bar{X}&gt;0.5\)</span>. You can assume you know <span class="math inline">\(p=0.45\)</span> for this calculation.</p>
<p>13. Assume you are in a practical situation and you don’t know <span class="math inline">\(p\)</span>. Take a sample of size <span class="math inline">\(N=100\)</span> and obtain a sample average of <span class="math inline">\(\bar{X} = 0.51\)</span>. What is the CLT approximation for the probability that your error is equal to or larger than 0.01?</p>

</div>
<div id="confidence-intervals" class="section level2">
<h2><span class="header-section-number">15.6</span> Confidence intervals</h2>
<p>Confidence intervals are a very useful concept widely employed by data analysts. A version of these that are commonly seen come from the <code>ggplot</code> geometry <code>geom_smooth</code>. Here is an example using a temperature dataset available in R:</p>
<p><img src="book_files/figure-html/first-confidence-intervals-example-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>In the Machine Learning part we will learn how the curve is formed, but for now consider the shaded area around the curve. This is created using the concept of confidence intervals.</p>
<p>In our earlier competition, you were asked to give an interval. If the interval you submitted includes the <span class="math inline">\(p\)</span>, you get half the money you spent on your “poll” back and pass to the next stage of the competition. One way to pass to the second round is to report a very large interval. For example, the interval <span class="math inline">\([0,1]\)</span> is guaranteed to include <span class="math inline">\(p\)</span>. However, with an interval this big, we have no chance of winning the competition. Similarly, if you are an election forecaster and predict the spread will be between -100% and 100%, you will be ridiculed for stating the obvious. Even a smaller interval, such as saying the spread will be between -10 and 10%, will not be considered serious.</p>
<p>On the other hand, the smaller the interval we report, the smaller our chances are of winning the prize. Likewise, a bold pollster that reports very small intervals and misses the mark most of the time will not be considered a good pollster. We want to be somewhere in between.</p>
<p>We can use the statistical theory we have learned to compute the probability of any given interval including <span class="math inline">\(p\)</span>. If we are asked to create an interval with, say, a 95% chance of including <span class="math inline">\(p\)</span>, we can do that as well. These are called 95% confidence intervals.</p>
<p>When a pollster reports an estimate and a margin of error, they are, in a way, reporting a 95% confidence interval. Let’s show how this works mathematically.</p>
<p>We want to know the probability that the interval <span class="math inline">\([\bar{X} - 2\hat{\mbox{SE}}(\bar{X}), \bar{X} - 2\hat{\mbox{SE}}(\bar{X})]\)</span> contains the true proportion <span class="math inline">\(p\)</span>. First, consider that the start and end of these intervals are random variables: every time we take a sample, they change. To illustrate this, run the Monte Carlo simulation above twice. We use the same parameters as above:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p &lt;-<span class="st"> </span><span class="fl">0.45</span>
N &lt;-<span class="st"> </span><span class="dv">1000</span></code></pre></div>
<p>And notice that the interval here:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">size =</span> N, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">-</span>p, p))
x_hat &lt;-<span class="st"> </span><span class="kw">mean</span>(x)
se_hat &lt;-<span class="st"> </span><span class="kw">sqrt</span>(x_hat <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>x_hat) <span class="op">/</span><span class="st"> </span>N)
<span class="kw">c</span>(x_hat <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se_hat, x_hat <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se_hat)
<span class="co">#&gt; [1] 0.416 0.478</span></code></pre></div>
<p>is different from this one:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">size=</span>N, <span class="dt">replace=</span><span class="ot">TRUE</span>, <span class="dt">prob=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">-</span>p, p))
x_hat &lt;-<span class="st"> </span><span class="kw">mean</span>(x)
se_hat &lt;-<span class="st"> </span><span class="kw">sqrt</span>(x_hat <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>x_hat) <span class="op">/</span><span class="st"> </span>N)
<span class="kw">c</span>(x_hat <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se_hat, x_hat <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se_hat)
<span class="co">#&gt; [1] 0.409 0.471</span></code></pre></div>
<p>Keep sampling and creating intervals and you will see the random variation.</p>
<p>To determine the probability that the interval includes <span class="math inline">\(p\)</span>, we need to compute this: <span class="math display">\[
\mbox{Pr}\left(\bar{X} - 1.96\hat{\mbox{SE}}(\bar{X}) \leq p \leq \bar{X} + 1.96\hat{\mbox{SE}}(\bar{X})\right)
\]</span></p>
<p>By subtracting and dividing the same quantities in all parts of the equation, we get that the above is equivalent to:</p>
<p><span class="math display">\[
\mbox{Pr}\left(-1.96 \leq \frac{\bar{X}- p}{\hat{\mbox{SE}}(\bar{X})} \leq  1.96\right)
\]</span></p>
<p>The term in the middle is an approximately normal random variable with expected value 0 and standard error 1, which we have been denoting with <span class="math inline">\(Z\)</span>, so we have:</p>
<p><span class="math display">\[
\mbox{Pr}\left(-1.96 \leq Z \leq  1.96\right)
\]</span></p>
<p>which we can quickly compute using :</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pnorm</span>(<span class="fl">1.96</span>) <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="op">-</span><span class="fl">1.96</span>)
<span class="co">#&gt; [1] 0.95</span></code></pre></div>
<p>proving that we have a 95% probability.</p>
<p>If we want to have a larger probability, say 99%, we need to multiply by whatever <code>z</code> satisfies the following:</p>
<p><span class="math display">\[
\mbox{Pr}\left(-z \leq Z \leq  z\right) = 0.99
\]</span></p>
<p>Using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z &lt;-<span class="st"> </span><span class="kw">qnorm</span>(<span class="fl">0.995</span>)
z
<span class="co">#&gt; [1] 2.58</span></code></pre></div>
<p>will achieve this because by definition <code>pnorm(qnorm(0.995))</code> is 0.995 and by symmetry <code>pnorm(1-qnorm(0.995))</code> is 1 - 0.995. As a consequence, we have that:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pnorm</span>(z) <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="op">-</span>z)
<span class="co">#&gt; [1] 0.99</span></code></pre></div>
<p>is <code>0.995 - 0.005 = 0.99</code>. We can use this approach for any proportion <span class="math inline">\(p\)</span>: we set <code>z = qnorm(1 - (1 - p)/2)</code> because <span class="math inline">\(1 - (1 - p)/2 + (1 - p)/2 = p\)</span>.</p>
<p>So, for example, for <span class="math inline">\(p=0.95\)</span>, <span class="math inline">\(1 - (1-p)/2 = 0.975\)</span> and we get the 1.96 we have been using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qnorm</span>(<span class="fl">0.975</span>)
<span class="co">#&gt; [1] 1.96</span></code></pre></div>
<div id="a-monte-carlo-simulation-1" class="section level3">
<h3><span class="header-section-number">15.6.1</span> A Monte Carlo simulation</h3>
<p>We can run a Monte Carlo simulation to confirm that, in fact, a 95% confidence interval includes <span class="math inline">\(p\)</span> 95% of the time.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="dv">1000</span>
B &lt;-<span class="st"> </span><span class="dv">10000</span>
inside &lt;-<span class="st"> </span><span class="kw">replicate</span>(B, {
  x &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">size =</span> N, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">-</span>p, p))
  x_hat &lt;-<span class="st"> </span><span class="kw">mean</span>(x)
  se_hat &lt;-<span class="st"> </span><span class="kw">sqrt</span>(x_hat <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>x_hat) <span class="op">/</span><span class="st"> </span>N)
  <span class="kw">between</span>(p, x_hat <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se_hat, x_hat <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se_hat)
})
<span class="kw">mean</span>(inside)
<span class="co">#&gt; [1] 0.948</span></code></pre></div>
<p>The following plot shows the first 100 confidence intervals. In this case, we created the simulation so the black line denotes the parameter we are trying to estimate:</p>
<p><img src="book_files/figure-html/confidence-interval-coverage-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="the-correct-language" class="section level3">
<h3><span class="header-section-number">15.6.2</span> The correct language</h3>
<p>When using the theory we described above, it is important to remember that it is the intervals that are random, not <span class="math inline">\(p\)</span>. In the plot above, we can see the random intervals moving around and <span class="math inline">\(p\)</span>, represented with the vertical line, staying in the same place. The proportion of blue in the urn <span class="math inline">\(p\)</span> is not. So the 95% relates to the probability that this random interval falls on top of <span class="math inline">\(p\)</span>. Saying the <span class="math inline">\(p\)</span> has a 95% chance of being between this and that is technically an incorrect statement because <span class="math inline">\(p\)</span> is not random.</p>
</div>
</div>
<div id="exercises-28" class="section level2">
<h2><span class="header-section-number">15.7</span> Exercises</h2>
<p>For these exercises, we will use actual polls from the 2016 election. You can load the data from the <strong>dslabs</strong> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dslabs)
<span class="kw">data</span>(<span class="st">&quot;polls_us_election_2016&quot;</span>)</code></pre></div>
<p>Specifically, we will use all the national polls that ended within one week before the election.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
polls &lt;-<span class="st"> </span>polls_us_election_<span class="dv">2016</span> <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(enddate <span class="op">&gt;=</span><span class="st"> &quot;2016-10-31&quot;</span> <span class="op">&amp;</span><span class="st"> </span>state <span class="op">==</span><span class="st"> &quot;U.S.&quot;</span>) </code></pre></div>
<p>1. For the first poll, you can obtain the samples size and estimated Clinton percentage with:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span>polls<span class="op">$</span>samplesize[<span class="dv">1</span>]
x_hat &lt;-<span class="st"> </span>polls<span class="op">$</span>rawpoll_clinton[<span class="dv">1</span>]<span class="op">/</span><span class="dv">100</span></code></pre></div>
<p>Assume there are only two candidates and construct a 95% confidence interval for the election night proportion <span class="math inline">\(p\)</span>.</p>
<p>2. Now use <code>dplyr</code> to add a confidence interval as two columns, call them <code>lower</code> and <code>upper</code>, to the object <code>poll</code>. Then use <code>select</code> to show the <code>pollster</code>, <code>enddate</code>, <code>x_hat</code>,<code>lower</code>, <code>upper</code> variables. Hint: define temporary columns <code>x_hat</code> and <code>se_hat</code>.</p>
<p>3. The final tally for the popular vote was Clinton 48.2% and Trump 46.1%. Add a column, call it <code>hit</code>, to the previous table stating if the confidence interval included the true proportion <span class="math inline">\(p=0.482\)</span> or not.</p>
<p>4. For the table you just created, what proportion of confidence intervals included <span class="math inline">\(p\)</span>?</p>
<p>5. If these confidence intervals are constructed correctly, and the theory holds up, what proportion should include <span class="math inline">\(p\)</span>?</p>
<p>6. A much smaller proportion of the polls than expected produce confidence intervals containing <span class="math inline">\(p\)</span>. If you look closely at the table, you will see that most polls that fail to include <span class="math inline">\(p\)</span> are underestimating. The reason for this is undecided voters, individuals polled that do not yet know who they will vote for or do not want to say. Because, historically, undecideds divide evenly between the two main candidates on election day, it is more informative to estimate the spread or the difference between the proportion of two candidates <span class="math inline">\(d\)</span>, which in this election was <span class="math inline">\(0. 482 - 0.461 = 0.021\)</span>. Assume that there are only two parties and that <span class="math inline">\(d = 2p - 1\)</span>, redefine <code>polls</code> as below and re-do exercise 1, but for the difference.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">polls &lt;-<span class="st"> </span>polls_us_election_<span class="dv">2016</span> <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(enddate <span class="op">&gt;=</span><span class="st"> &quot;2016-10-31&quot;</span> <span class="op">&amp;</span><span class="st"> </span>state <span class="op">==</span><span class="st"> &quot;U.S.&quot;</span>)  <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">d_hat =</span> rawpoll_clinton <span class="op">/</span><span class="st"> </span><span class="dv">100</span> <span class="op">-</span><span class="st"> </span>rawpoll_trump <span class="op">/</span><span class="st"> </span><span class="dv">100</span>)</code></pre></div>
<p>7. Now repeat exercise 3, but for the difference.</p>
<p>8. Now repeat exercise 4, but for the difference.</p>
<p>9. Although the proportion of confidence intervals goes up substantially, it is still lower than 0.95. In the next chapter, we learn the reason for this. To motivate this, make a plot of the error, the difference between each poll’s estimate and the actual <span class="math inline">\(d=0.021\)</span>. Stratify by pollster.</p>
<p>10. Redo the plot that you made for exercise 9, but only for pollsters that took five or more polls.</p>
</div>
<div id="power" class="section level2">
<h2><span class="header-section-number">15.8</span> Power</h2>
<p>Pollsters are not successful at providing correct confidence intervals, but rather at predicting who will win. When we took a 25 bead sample size, the confidence interval for the spread:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="dv">25</span>
x_hat &lt;-<span class="st"> </span><span class="fl">0.48</span>
(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x_hat <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">+</span><span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="fl">1.96</span>, <span class="fl">1.96</span>) <span class="op">*</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(x_hat <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>x_hat) <span class="op">/</span><span class="st"> </span>N)
<span class="co">#&gt; [1] -0.432  0.352</span></code></pre></div>
<p>includes 0. If this were a poll and we were forced to make a declaration, we would have to say it was a “toss-up”.</p>
<p>A problem with our poll results is that given the sample size and the value of <span class="math inline">\(p\)</span>, we would have to sacrifice on the probability of an incorrect call to create an interval that does not include 0.</p>
<p>This does not mean that the election is close. It only means that we have a small sample size. In statistical textbooks this is called lack of <em>power</em>. In the context of polls, <em>power</em> is the probability of detecting spreads different from 0.</p>
<p>By increasing our sample size, we lower our standard error and therefore have a much better chance of detecting the direction of the spread.</p>
</div>
<div id="p-values" class="section level2">
<h2><span class="header-section-number">15.9</span> p-values</h2>
<p>p-values are ubiquitous in the scientific literature. They are related to confidence intervals so we introduce the concept here.</p>
<p>Let’s consider the blue and red beads. Suppose that rather than wanting an estimate of the spread or the proportion of blue, I am interested only in the question: are there more blue beads or red beads? I want to know if the spread <span class="math inline">\(2p-1 &gt; 0\)</span>.</p>
<p>Say we take a random sample of <span class="math inline">\(N=100\)</span> and we observe <span class="math inline">\(52\)</span> blue beads, which gives us <span class="math inline">\(2\bar{X}-1=0.04\)</span>. This seems to be pointing to the existence of more blue than red beads since 0.04 is larger than 0. However, as data scientists we need to be skeptical. We know there is chance involved in this process and we could get a 52 even when the actual spread is 0. We call the assumption that the spread is <span class="math inline">\(2p-1=0\)</span> a <em>null hypothesis</em>. The null hypothesis is the skeptic’s hypothesis. We have observed a random variable <span class="math inline">\(2*\bar{X}-1 = 0.04\)</span> and the p-value is the answer to the question: how likely is it to see a value this large, when the null hypothesis is true? So we write:</p>
<p><span class="math display">\[\mbox{Pr}(\mid \bar{X} - 0.5 \mid &gt; 0.02 ) \]</span></p>
<p>assuming the <span class="math inline">\(2p-1=0\)</span> or <span class="math inline">\(p=0.5\)</span>. Under the null hypothesis we know that:</p>
<p><span class="math display">\[
\sqrt{N}\frac{\bar{X} - 0.5}{\sqrt{0.5(1-0.5)}}
\]</span></p>
<p>is standard normal. We therefore can compute the probability above, which is the p-value.</p>
<p><span class="math display">\[\mbox{Pr}\left(\sqrt{N}\frac{\mid \bar{X} - 0.5\mid}{\sqrt{0.5(1-0.5)}} &gt; \sqrt{N} \frac{0.02}{ \sqrt{0.5(1-0.5)}}\right)\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="dv">100</span>
z &lt;-<span class="st"> </span><span class="kw">sqrt</span>(N)<span class="op">*</span><span class="fl">0.02</span><span class="op">/</span><span class="fl">0.5</span>
<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(<span class="kw">pnorm</span>(z) <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="op">-</span>z))
<span class="co">#&gt; [1] 0.689</span></code></pre></div>
<p>In this case, there is actually a large chance of seeing 52 or larger under the null hypothesis.</p>
<p>Keep in mind that there is a close connection between p-values and confidence intervals. If a 95% confidence interval of the spread does not include 0, we know that the p-value must be smaller than 0.05.</p>
<p>To learn more about p-values, you can consult any statistics textbook. However, in general, we prefer reporting confidence intervals over p-values since it gives us an idea of the size of the estimate. If we just report the p-value we provide no information about the significance of the finding in the context of the problem.</p>

</div>
<div id="association-tests" class="section level2">
<h2><span class="header-section-number">15.10</span> Association tests</h2>
<p>The statistical tests we have studied up to now leave out a substantial portion of data types. Specifically, we have not discussed inference for binary, categorical, and ordinal data. To give a very specific example, consider the following case study.</p>
<p>A 2014 PNAS paper<a href="#fn52" class="footnoteRef" id="fnref52"><sup>52</sup></a> analyzed success rates from funding agencies in the Netherlands and concluded that their:</p>
<blockquote>
<p>results reveal gender bias favoring male applicants over female applicants in the prioritization of their “quality of researcher” (but not “quality of proposal”) evaluations and success rates, as well as in the language use in instructional and evaluation materials.</p>
</blockquote>
<p>The main evidence for this conclusion comes down to a comparison of the percentages. Table S1 in the paper includes the information we need. Here are the three columns showing the overall outcomes:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(dslabs)
<span class="kw">data</span>(<span class="st">&quot;research_funding_rates&quot;</span>)
research_funding_rates <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(discipline, applications_total, 
                                  success_rates_total) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">head</span>()
<span class="co">#&gt;           discipline applications_total success_rates_total</span>
<span class="co">#&gt; 1  Chemical sciences                122                26.2</span>
<span class="co">#&gt; 2  Physical sciences                174                20.1</span>
<span class="co">#&gt; 3            Physics                 76                26.3</span>
<span class="co">#&gt; 4         Humanities                396                16.4</span>
<span class="co">#&gt; 5 Technical sciences                251                17.1</span>
<span class="co">#&gt; 6  Interdisciplinary                183                15.8</span></code></pre></div>
<p>We have these values for each gender:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">names</span>(research_funding_rates)
<span class="co">#&gt;  [1] &quot;discipline&quot;          &quot;applications_total&quot;  &quot;applications_men&quot;   </span>
<span class="co">#&gt;  [4] &quot;applications_women&quot;  &quot;awards_total&quot;        &quot;awards_men&quot;         </span>
<span class="co">#&gt;  [7] &quot;awards_women&quot;        &quot;success_rates_total&quot; &quot;success_rates_men&quot;  </span>
<span class="co">#&gt; [10] &quot;success_rates_women&quot;</span></code></pre></div>
<p>We can compute the totals that were successful and the totals that were not as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">totals &lt;-<span class="st"> </span>research_funding_rates <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>discipline) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize_all</span>(sum) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">yes_men =</span> awards_men, 
            <span class="dt">no_men =</span> applications_men <span class="op">-</span><span class="st"> </span>awards_men, 
            <span class="dt">yes_women =</span> awards_women, 
            <span class="dt">no_women =</span> applications_women <span class="op">-</span><span class="st"> </span>awards_women) </code></pre></div>
<p>So we see that a larger percent of men than women received awards:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">totals <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarize</span>(<span class="dt">percent_men =</span> yes_men<span class="op">/</span>(yes_men<span class="op">+</span>no_men),
                     <span class="dt">percent_women =</span> yes_women<span class="op">/</span>(yes_women<span class="op">+</span>no_women))
<span class="co">#&gt;   percent_men percent_women</span>
<span class="co">#&gt; 1       0.177         0.149</span></code></pre></div>
<p>But could this be due just to random variability? Here we learn how to perform inference for this type of data.</p>
<div id="lady-tasting-tea" class="section level3">
<h3><span class="header-section-number">15.10.1</span> Lady Tasting Tea</h3>
<p>R.A. Fisher<a href="#fn53" class="footnoteRef" id="fnref53"><sup>53</sup></a> was one of the first to formalize hypothesis testing. The “Lady Tasting Tea” is one of the most famous examples.</p>
<p>The story is as follows: an acquaintance of Fisher’s claimed that she could tell if milk was added before or after tea was poured. Fisher was skeptical. He designed an experiment to test this claim. He gave her four pairs of cups of tea: one with milk poured first, the other after. The order was randomized. The null hypothesis here is that she is guessing. Fisher derived the distribution for the number of correct picks on the assumption that the choices were random and independent.</p>
<p>As an example, suppose she picked 3 out of 4 correctly. Do we believe she has a special ability? The basic question we ask is: if the tester is actually guessing, what are the chances that she gets 3 or more correct? Just as we have done before, we can compute a probability under the null hypothesis that she is guessing 4 of each. Under this null hypothesis, we can think of this particular example as picking 4 balls out of an urn with 4 blue (correct answer) and 4 red (incorrect answer) balls. Remember, she knows that there are four before tea and four after.</p>
<p>Under the null hypothesis that she is simply guessing, each ball has the same chance of being picked. We can then use combinations to figure out each probability. The probability of picking 3 is <span class="math inline">\({4 \choose 3} {4 \choose 1} / {8 \choose 4} = 16/70\)</span>. The probability of picking all 4 correct is <span class="math inline">\({4 \choose 4} {4 \choose 0}/{8 \choose 4}= 1/70\)</span>. Thus, the chance of observing a 3 or something more extreme, under the null hypothesis, is <span class="math inline">\(\approx 0.24\)</span>. This is the p-value. The procedure that produced this p-value is called <em>Fisher’s exact test</em> and it uses the <em>hypergeometric distribution</em>.</p>
</div>
<div id="two-by-two-tables" class="section level3">
<h3><span class="header-section-number">15.10.2</span> Two-by-two tables</h3>
<p>The data from the experiment is usually summarized by a table like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tab &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">3</span>),<span class="dv">2</span>,<span class="dv">2</span>)
<span class="kw">rownames</span>(tab)&lt;-<span class="kw">c</span>(<span class="st">&quot;Poured Before&quot;</span>,<span class="st">&quot;Poured After&quot;</span>)
<span class="kw">colnames</span>(tab)&lt;-<span class="kw">c</span>(<span class="st">&quot;Guessed before&quot;</span>,<span class="st">&quot;Guessed after&quot;</span>)
tab
<span class="co">#&gt;               Guessed before Guessed after</span>
<span class="co">#&gt; Poured Before              3             1</span>
<span class="co">#&gt; Poured After               1             3</span></code></pre></div>
<p>These are referred to as a two-by-two table. For each of the four combinations one can get with a pair of binary variables, they show the observed counts for each occurrence.</p>
<p>The function <code>fisher.test</code> performs the inference calculations above:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fisher.test</span>(tab, <span class="dt">alternative=</span><span class="st">&quot;greater&quot;</span>)<span class="op">$</span>p.value
<span class="co">#&gt; [1] 0.243</span></code></pre></div>
</div>
<div id="chi-square-test" class="section level3">
<h3><span class="header-section-number">15.10.3</span> Chi-square Test</h3>
<p>Notice that, in a way, our funding rates example is similar to the Lady Tasting Tea. However, in the Lady Tasting Tea example, the number of blue and red beads is experimentally fixed and the number of answers given for each category is also fixed. This is because Fisher made sure there were four cups with milk poured before tea and four cups with milk poured after and the lady knew this, so the answers would also have to include four befores and four afters. If this is the case, the sum of the rows and the sum of the columns are fixed. This defines constraints on the possible ways we can fill the two by two table and also permits us to use the hypergeometric distribution. In general, this is not the case. Nonetheless, there is another approach, the Chi-squared test, which is described below.</p>
<p>Imagine we have 290, 1,345, 177, 1,011 applicants, some are men and some are women and some get funded, whereas others don’t. We saw that the success rates for men and woman were:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">totals <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarize</span>(<span class="dt">percent_men =</span> yes_men<span class="op">/</span>(yes_men<span class="op">+</span>no_men),
                     <span class="dt">percent_women =</span> yes_women<span class="op">/</span>(yes_women<span class="op">+</span>no_women))
<span class="co">#&gt;   percent_men percent_women</span>
<span class="co">#&gt; 1       0.177         0.149</span></code></pre></div>
<p>respectively. Would we see this again if we randomly assign funding at the overall rate:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rate &lt;-<span class="st"> </span>totals <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">percent_total =</span> 
              (yes_men <span class="op">+</span><span class="st"> </span>yes_women)<span class="op">/</span>
<span class="st">              </span>(yes_men <span class="op">+</span><span class="st"> </span>no_men <span class="op">+</span>yes_women <span class="op">+</span><span class="st"> </span>no_women)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">pull</span>(percent_total)
rate
<span class="co">#&gt; [1] 0.165</span></code></pre></div>
<p>The Chi-square test answers this question. The first step is to create the two-by-two data table:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">two_by_two &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">awarded =</span> <span class="kw">c</span>(<span class="st">&quot;no&quot;</span>, <span class="st">&quot;yes&quot;</span>), 
                     <span class="dt">men =</span> <span class="kw">c</span>(totals<span class="op">$</span>no_men, totals<span class="op">$</span>yes_men),
                     <span class="dt">women =</span> <span class="kw">c</span>(totals<span class="op">$</span>no_women, totals<span class="op">$</span>yes_women))
two_by_two
<span class="co">#&gt;   awarded  men women</span>
<span class="co">#&gt; 1      no 1345  1011</span>
<span class="co">#&gt; 2     yes  290   177</span></code></pre></div>
<p>The general idea of the Chi-square test is to compare this two-by-two table to what you expect to see, which would be:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data.frame</span>(<span class="dt">awarded =</span> <span class="kw">c</span>(<span class="st">&quot;no&quot;</span>, <span class="st">&quot;yes&quot;</span>), 
       <span class="dt">men =</span> (totals<span class="op">$</span>no_men <span class="op">+</span><span class="st"> </span>totals<span class="op">$</span>yes_men) <span class="op">*</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>rate, rate),
       <span class="dt">women =</span> (totals<span class="op">$</span>no_women <span class="op">+</span><span class="st"> </span>totals<span class="op">$</span>yes_women) <span class="op">*</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>rate, rate))
<span class="co">#&gt;   awarded  men women</span>
<span class="co">#&gt; 1      no 1365   991</span>
<span class="co">#&gt; 2     yes  270   197</span></code></pre></div>
<p>We can see that more men than expected and fewer women than expected received funding. However, under the null hypothesis these observations are random variables. The Chi-square test tells us how likely it is to see a deviation this large or larger. This test uses an asymptotic result, similar to the CLT, related to the sums of independent binary outcomes. The R function <code>chisq.test</code> takes a two-by-two table and returns the results from the test:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">chisq_test &lt;-<span class="st"> </span>two_by_two <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>awarded) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">chisq.test</span>()</code></pre></div>
<p>We see that the p-value is 0.0509:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">chisq_test<span class="op">$</span>p.value
<span class="co">#&gt; [1] 0.0509</span></code></pre></div>
</div>
<div id="odds-ratio" class="section level3">
<h3><span class="header-section-number">15.10.4</span> The odds ratio</h3>
<p>An informative summary statistic associated with two-by-two tables is the odds ratio. Define the two variables as <span class="math inline">\(X = 1\)</span> if you are a male and 0 otherwise, and <span class="math inline">\(Y=1\)</span> if you are funded and 0 otherwise. The odds of getting funded if you are a man is defined:</p>
<p><span class="math display">\[\mbox{Pr}(Y=1 \mid X=1) / \mbox{Pr}(Y=0 \mid X=1)\]</span></p>
<p>and can be computed like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">odds_men &lt;-<span class="st"> </span><span class="kw">with</span>(two_by_two, (men[<span class="dv">2</span>]<span class="op">/</span><span class="kw">sum</span>(men)) <span class="op">/</span><span class="st"> </span>(men[<span class="dv">1</span>]<span class="op">/</span><span class="kw">sum</span>(men)))
odds_men
<span class="co">#&gt; [1] 0.216</span></code></pre></div>
<p>And the odds of being funded if you are a woman is:</p>
<p><span class="math display">\[\mbox{Pr}(Y=1 \mid X=0) / \mbox{Pr}(Y=0 \mid X=0)\]</span></p>
<p>and can be computed like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">odds_women &lt;-<span class="st"> </span><span class="kw">with</span>(two_by_two, (women[<span class="dv">2</span>]<span class="op">/</span><span class="kw">sum</span>(women)) <span class="op">/</span><span class="st"> </span>(women[<span class="dv">1</span>]<span class="op">/</span><span class="kw">sum</span>(women)))
odds_women
<span class="co">#&gt; [1] 0.175</span></code></pre></div>
<p>The odds ratio is the ratio for these two odds: how many times larger are the odds for men than for women?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">odds_men <span class="op">/</span><span class="st"> </span>odds_women
<span class="co">#&gt; [1] 1.23</span></code></pre></div>
<p>We often see two-by-two tables written out as</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:center;">
Men
</th>
<th style="text-align:center;">
Women
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Awarded
</td>
<td style="text-align:center;">
a
</td>
<td style="text-align:center;">
b
</td>
</tr>
<tr>
<td style="text-align:left;">
Not Awarded
</td>
<td style="text-align:center;">
c
</td>
<td style="text-align:center;">
d
</td>
</tr>
</tbody>
</table>
<p>In this case, the odds ratio is <span class="math inline">\(\frac{a/c}{b/d}\)</span> which is equivalent to <span class="math inline">\((ad) / (bc)\)</span></p>
</div>
<div id="confidence-intervals-for-the-odds-ratio" class="section level3">
<h3><span class="header-section-number">15.10.5</span> Confidence intervals for the odds ratio</h3>
<p>Computing confidence intervals for the odds ratio is not mathematically straightforward. Unlike other statistics, for which we can derive useful approximations of their distributions, the odds ratio is not only a ratio, but a ratio of ratios. Therefore, there is no simple way of using, for example, the CLT.</p>
<p>However, statistical theory tells us that when all four entries of the two-by-two table are large enough, then the log of the odds ratio is approximately normal with standard error</p>
<p><span class="math display">\[
\sqrt{1/a + 1/b + 1/c + 1/d} 
\]</span></p>
<p>This implies that a 95% confidence interval for the log odds ratio can be formed by:</p>
<p><span class="math display">\[
\log\left(\frac{ad}{bc}\right) \pm 1.96 \sqrt{1/a + 1/b + 1/c + 1/d} 
\]</span></p>
<p>By exponentiating these two numbers we can construct a confidence interval of the odds ratio.</p>
<p>Using R we can compute this confidence interval as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">log_or &lt;-<span class="st"> </span><span class="kw">log</span>(odds_men <span class="op">/</span><span class="st"> </span>odds_women)
se &lt;-<span class="st"> </span>two_by_two <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>awarded) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">se =</span> <span class="kw">sqrt</span>(<span class="kw">sum</span>(<span class="dv">1</span><span class="op">/</span>men) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">/</span>women))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">pull</span>(se)
ci &lt;-<span class="st"> </span>log_or <span class="op">+</span><span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>) <span class="op">*</span><span class="st"> </span><span class="kw">qnorm</span>(<span class="fl">0.975</span>) <span class="op">*</span><span class="st"> </span>se</code></pre></div>
<p>If we want to convert it back to the odds ratio scale, we can exponentiate:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>(ci)
<span class="co">#&gt; [1] 1.00 1.51</span></code></pre></div>
<p>Note that 1 is not included in the confidence interval which must mean that the p-value is smaller than 0.05. We can confirm this using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">2</span><span class="op">*</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(log_or, <span class="dv">0</span>, se))
<span class="co">#&gt; [1] 0.0454</span></code></pre></div>
<p>This is a slightly different p-value than that with the Chi-square test. This is because we are using a different asymptotic approximation to the null distribution. To learn more about inference and asymptotic theory for odds ratio, consult the <em>Generalized Linear Models</em> book by McCullagh and Nelder.</p>
</div>
<div id="small-count-correction" class="section level3">
<h3><span class="header-section-number">15.10.6</span> Small count correction</h3>
<p>Note that the log odds ratio is not defined if any of the cells of the two-by-two table is 0. This is because if <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, <span class="math inline">\(c\)</span>, or <span class="math inline">\(d\)</span> is 0, the <span class="math inline">\(\log(\frac{ad}{bc})\)</span> is either the log of 0 or has a 0 in the denominator. For this situation, it is common practice to avoid 0s by adding 0.5 to each cell. This is referred to as the <em>Haldane–Anscombe correction</em> and has been shown, both in practice and theory, to work well.</p>
</div>
<div id="large-samples-small-p-values" class="section level3">
<h3><span class="header-section-number">15.10.7</span> Large samples, small p-values</h3>
<p>As mentioned earlier, reporting only p-values is not an appropriate way to report the results of data analysis. In scientific journals, for example, some studies seem to overemphasize p-values. Some of these studies have large sample sizes and report impressively small p-values. Yet when one looks closely at the results, we realize odds ratios are quite modest: barely bigger than 1. In this case the difference may not be <em>practically significant</em> or <em>scientifically significant</em>.</p>
<p>Note that the relationship between odds ratio and p-value is not one-to-one. It depends on the sample size. So a very small p-value does not necessarily mean a very large odds ratio. Notice what happens to the p-value if we multiply our two-by-two table by 10, which does not change the odds ratio:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">two_by_two <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>awarded) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">men =</span> men<span class="op">*</span><span class="dv">10</span>, <span class="dt">women =</span> women<span class="op">*</span><span class="dv">10</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">chisq.test</span>() <span class="op">%&gt;%</span><span class="st"> </span>.<span class="op">$</span>p.value
<span class="co">#&gt; [1] 2.63e-10</span></code></pre></div>
</div>
</div>
<div id="exercises-29" class="section level2">
<h2><span class="header-section-number">15.11</span> Exercises</h2>
<p>1. A famous athlete has an impressive career, winning 70% of her 500 career matches. However, this athlete gets criticized because in important events, such as the Olympics, she has a losing record of 8 wins and 9 losses. Perform a Chi-square test to determine if this losing record can be simply due to chance as opposed to not performing well under pressure.</p>
<p>2. Why did we use the Chi-square test instead of Fisher’s exact test in the previous exercise?</p>
<ol style="list-style-type: lower-alpha">
<li>It actually does not matter, since they give the exact same p-value.</li>
<li>Fisher’s exact and the Chi-square are different names for the same test.</li>
<li>Because the sum of the rows and columns of the two-by-two table are not fixed so the hypergeometric distribution is not an appropriate assumption for the null hypothesis. For this reason, Fisher’s exact test is rarely applicable with observational data.</li>
<li>Because the Chi-square test runs faster.</li>
</ol>
<p>3. Compute the odds ratio of “losing under pressure” along with a confidence interval.</p>
<p>4. Notice that the p-value is larger than 0.05 but the 95% confidence interval does not include 1. What explains this?</p>
<ol style="list-style-type: lower-alpha">
<li>We made a mistake in our code.</li>
<li>These are not t-tests so the connection between p-value and confidence intervals does not apply.</li>
<li>Different approximations are used for the p-value and the confidence interval calculation. If we had a larger sample size the match would be better.</li>
<li>We should use the Fisher exact test to get confidence intervals.</li>
</ol>
<p>5. Multiply the two-by-two table by 2 and see if the p-value and confidence retrieval are a better match.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="50">
<li id="fn50"><p><a href="http://www.realclearpolitics.com" class="uri">http://www.realclearpolitics.com</a><a href="inference.html#fnref50">↩</a></p></li>
<li id="fn51"><p><a href="http://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html" class="uri">http://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html</a><a href="inference.html#fnref51">↩</a></p></li>
<li id="fn52"><p><a href="http://www.pnas.org/content/112/40/12349.abstract" class="uri">http://www.pnas.org/content/112/40/12349.abstract</a><a href="inference.html#fnref52">↩</a></p></li>
<li id="fn53"><p><a href="https://en.wikipedia.org/wiki/Ronald_Fisher" class="uri">https://en.wikipedia.org/wiki/Ronald_Fisher</a><a href="inference.html#fnref53">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="random-variables.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rafalab/dsbook/edit/master/inference/parameters-estimates.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
